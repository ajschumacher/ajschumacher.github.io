<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <link rel="stylesheet" type="text/css" href="/styles/csshake.min.css">
    <link rel="stylesheet" type="text/css" href="/styles/zenburn.css">
    <link rel="stylesheet" type="text/css" href="/styles/planspace.css">
    <title>TF-IDF is about what matters</title>
  </head>
  <body>
    <article>
<h1>TF-IDF is about what matters</h1>
<p class="date">Sunday May 24, 2015</p>
<p>TF-IDF is <em>term frequency inverse document frequency</em>. But what does that mean?</p>
<p><img alt="cat and dog" src="cat_and_dog.png" /></p>
<!-- Image from Wikimedia Commons: http://commons.wikimedia.org/wiki/File:Cat_and_dog.JPG -->

<p>Consider a collection (or corpus, or set) of four sentences (or documents, or strings) made up of words (or terms, or tokens):</p>
<ol>
<li>the cat and dog sat</li>
<li>the dog and cat sat</li>
<li>the cat sat and sat</li>
<li>the cat killed the dog</li>
</ol>
<p>One thing we can do is transform this corpus to a bag of words representation, which just means we’re keeping track of what words there are but not what order they’re in. We could use binary values to represent whether a word appears or not:</p>
<pre><code class="language-text">    the    cat    and    dog    sat   killed
1     1      1      1      1      1        0
2     1      1      1      1      1        0
3     1      1      1      0      1        0
4     1      1      0      1      0        1</code></pre>

<p>In this representation we've lost word order, but we've also lost <em>term frequency</em>: we can't tell that sentence three has twice as much sitting as sentence two. Let's use the counts:</p>
<pre><code class="language-text">    the    cat    and    dog    sat   killed
1     1      1      1      1      1        0
2     1      1      1      1      1        0
3     1      1      1      0      2        0
4     2      1      0      1      0        1</code></pre>

<p>Now we can see how many times each term appears in each document.</p>
<p>Is it good that “killed” is given the same score as “cat” in sentence four? The word “killed” is much rarer, after all; it is more “special” to sentence four. Let’s count the number of sentences that have each word, and call this the <em>document frequency</em>:</p>
<pre><code class="language-text">    the    cat    and    dog    sat  killed
      4      4      3      3      3       1</code></pre>

<p>The document frequencies are high for words that are boring, so dividing by the document frequencies will give us low scores for boring words and high scores for interesting words:</p>
<pre><code class="language-text">    the    cat    and   dog     sat  killed
1  0.25   0.25   0.33   0.33   0.33       0
2  0.25   0.25   0.33   0.33   0.33       0
3  0.25   0.25   0.33      0   0.67       0
4  0.50   0.25      0   0.33      0    1.00</code></pre>

<p>These scores match the intuition that “killed” is a very important word. In addition to making some intuitive sense, scores like these tend to work well in machine learning tasks.</p>
<p>All we did was this:</p>
<p>\[ \frac{\text{number of times the word is in this document}}{\text{number of documents the word is in}} \]</p>
<p>Or, for short:</p>
<p>\[ \frac{\text{term frequency}}{\text{document frequency}} \]</p>
<p>Dividing by a number is the same as multiplying by its inverse, \( \frac{1}{\text{a number}} \), so we can call this transformation <em>term frequency inverse document frequency</em>.</p>
<p>This is not a very difficult idea. You could have invented it yourself. But it gets so buried in terminology and secondary implementation details that people sometimes talk about it as if it were a deep and mysterious modeling technique. (It's not.)</p>
<p>Here's an <a href="http://www.cs.utexas.edu/~ml/papers/marlin-dissertation-06.pdf">example</a> of how the exposition can lose people. It starts nicely:</p>
<blockquote>
<p>“Tokens that occur frequently in a given string should have higher contribution to similarity than those that occur few times, as should those tokens that are rare among the set of strings under consideration.”</p>
</blockquote>
<p>Who could disagree? Then it jumps to this:</p>
<blockquote>
<p>“The Term Frequency-Inverse Document Frequency (TF-IDF) weighting scheme achieves this by associating a weight \( w_{v_i,s} = \frac{N(v_i,s)}{\text{max}_{v_j \in s} N(v_j,s)} \cdot \text{log} \frac{N}{N(v_i)} \) with every token \( v_i \) from string \( s \), where \( N(v_i, s) \) is the number of times \( v_i \) occurs in \( s \) (term frequency), \( N \) is the number of strings in the overall corpus under consideration, and \( N(v_i) \) is the number of strings in the corpus that include \( v_i \) (document frequency).”</p>
</blockquote>
<p>There's some notation there, and we've also thrown in two normalizations and a log. None of this is a particularly big deal, but it makes TF-IDF seem complicated. There are <a href="http://en.wikipedia.org/wiki/Tf%E2%80%93idf">a lot of ways</a> you could do the particulars of TF-IDF but they all achieve essentially what we did above by just counting and dividing.</p>
<p>It can also be confusing when results don't match what you expect. Here is what you get by default (with <a href="tfidf.py">this script</a>) from <a href="http://scikit-learn.org/">sklearn</a>'s <a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html">TfidfVectorizer</a>:</p>
<pre><code class="language-text">    the    cat    and    dog    sat   killed
1  0.39   0.39   0.48   0.48   0.48        0
2  0.39   0.39   0.48   0.48   0.48        0
3  0.32   0.32   0.40      0   0.79        0
4  0.63   0.31      0   0.38      0     0.60</code></pre>

<p>The formula <code>sklearn</code> version 0.16.1 uses to compute TF-IDF is this:</p>
<p>\[ \text{term frequency} \cdot \left ( \text{log} \left ( \frac{\text{number of documents} + 1}{\text{document frequency} +1} \right ) + 1 \right ) \]</p>
<p>And then <code>sklearn</code> also scales the rows of the results to each have unit length.</p>
<p>So you could be surprised that the results of a TF-IDF implementation don't exactly match the method you were thinking of. As above, there are a lot of small choices in exactly how to calculate TF-IDF, and <code>sklearn</code> chooses one particular implementation.</p>
<hr />
<p>I've seen better classification performance with TF-IDF than with raw counts, but I haven't seen a systematic comparison across various TF-IDF methods; it might be interesting to see such a comparison.</p>
<hr />
<p>Thanks to <a href="https://twitter.com/hallr">Rob Hall</a> for doing <a href="https://github.com/ga-students/DAT_SF_13/tree/master/lectures#session-14-natural-language-processing">work</a> that helped inspire this post!</p>
<!-- mathjax for formulas -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML" async></script>
    </article>
    <footer>
      <hr />
<p><a name="contact"></a><form class="email_updates">
  <input type="email" name="email" placeholder="your@email.address" style="width: 49%" />
  <input type="submit" value="Get monthly updates" style="width: 49%" />
  <input type="hidden" name="_subject" value="planspace.org updates list" />
  <input type="text" name="_honey" style="display:none" />
  <input type="hidden" name="_captcha" value="false" />
</form></p>
<p><a id="back_link2" href="/">This site</a> also has <a href="/rss.xml">RSS</a>. You can connect with <a id="aaron_link2" href="/aaron/">me</a> via <a href="https://twitter.com/planarrowspace">Twitter</a>, <a href="https://www.linkedin.com/in/ajschumacher">LinkedIn</a>, <a href="https://github.com/ajschumacher">GitHub</a>, and <a href="mailto:aaron@planspace.org">email</a>.</p>

      <hr />
    </footer>

<!-- my weird stuff -->
<script src="/scripts/planspace.js"></script>

<!-- syntax highlighting -->
<script src="/scripts/highlight.pack.js"></script>
<script type="text/javascript">hljs.initHighlightingOnLoad();</script>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-EZBQMHT77F"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-EZBQMHT77F');
</script>

<!-- Disqus comments -->
<div id="disqus_thread"></div>
<script type="text/javascript">
  var disqus_shortname = 'planspace';
  (function() {
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>

  </body>
</html>
