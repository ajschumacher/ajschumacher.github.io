<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <link rel="stylesheet" type="text/css" href="/styles/csshake.min.css">
    <link rel="stylesheet" type="text/css" href="/styles/zenburn.css">
    <link rel="stylesheet" type="text/css" href="/styles/planspace.css">
    <title>Weight of Evidence is logistic coefficients</title>
  </head>
  <body>
    <article>
<h1>Weight of Evidence is logistic coefficients</h1>
<p class="date">Friday September 17, 2021</p>
<p>“Weight of Evidence” (WoE) is a <a href="https://www.endocrinescience.org/glossary/weight-of-evidence-woe/" title="Weight of evidence refers to a systematic approach that scientists use to evaluate the totality of scientific evidence to assess if the science supports a particular conclusion.">good idea</a> for decision-making,
but, especially in the financial risk modeling world, it's also a
specific feature processing <a href="https://contrib.scikit-learn.org/category_encoders/woe.html">method</a> based on <a href="http://helios.mm.di.uoa.gr/~rouvas/ssi/sigkdd/sigkdd.vol3.1/barreca.pdf">target statistics</a>.
Weight of Evidence changes a categorical variable into the log odds
coefficients corresponding to a logistic regression with intercept at
the overall rate.</p>
<p>Say you have a categorical predictor, or a continuous predictor that
you're going to bin into categories in order to make it easier to
model nonlinear relationships, and a binary outcome like “defaulted on
loan or not”. Then for each category, the WoE score is:</p>
<p>\[ \text{WoE} =
    \log \left( \frac{\text{count of positives for this category} /
                      \text{count of all positives}}
                     {\text{count of negatives for this category} /
                      \text{count of all negatives}} \right) \]</p>
<pre><code class="language-python">import numpy as np
import pandas as pd
import category_encoders as ce

X = pd.DataFrame({'x': ['a', 'a', 'a', 'a', 'b', 'b', 'b', 'b']})
y =                    [ 1 ,  1 ,  0 ,  0 ,  1 ,  0 ,  0 ,  0 ]

woe = ce.woe.WOEEncoder(regularization=0)
woe.fit(X, y)
woe.transform(pd.DataFrame({'x': ['a', 'b']}))
## (0.5108256237659906, -0.587786664902119)

np.log((2/3)/(2/5)), np.log((1/3)/(3/5))
## (0.5108256237659906, -0.587786664902119)</code></pre>

<p>It's usually written <a href="https://www.listendata.com/2015/03/weight-of-evidence-woe-and-information.html">like</a> <a href="https://docs.tibco.com/data-science/GUID-44739B00-E85F-4CE7-8404-24F9B775ADE8.html">that</a>, sometimes with additive
smoothing (add some small numbers to the counts) to avoid zeros and
reduce variance. To make it even clearer that this is just
<a href="https://en.wikipedia.org/wiki/Logit">log odds</a>, rearrange to:</p>
<p>\[ \text{WoE} =
    \log \left( \frac{\text{count of positives for this category}}
                     {\text{count of negatives for this category}} \right) -
    \log \left( \frac{\text{count of all positives}}
                     {\text{count of all negatives}} \right) \]</p>
<pre><code class="language-python">np.log(2/2) - np.log(3/5), np.log(1/3) - np.log(3/5)
## (0.5108256237659907, -0.5877866649021191)</code></pre>

<p>The WoE values are exactly the coefficients you'd get if you
<a href="https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html">made indicator columns</a> for your categorical variable, added an
intercept column, and did a logistic regression. That design matrix
isn't full rank, so there isn't a unique solution—unless you set the
intercept coefficient to represent the overall log odds (as in the
subtracted term above).</p>
<p>These WoE scores are monotonic with the category-specific percent
positive, for example, so if you're going to use a tree-based model
where spacing doesn't matter, the more immediately interpretable value
might be preferable. If you're doing a simple logistic regression with
a WoE predictor, the coefficient will be one. In combination with
other features, it isn't obvious to me that WoE will always be an
absolutely optimal transform, but it seems like a fine choice for
multiple logistic regression as well. Using WoE instead of a
categorical feature can prevent learning interactions with the
affected categories, etc., which could be a consideration.</p>
<p>Like other transforms based on target statistics, WoE can leak label
information into training data. Even in large datasets, if some
categories appear rarely, this may be a problem. Cranking up the
additive smoothing (<a href="https://contrib.scikit-learn.org/category_encoders/woe.html">"regularization"</a>) a bit might help, or
consider alternatives as in the <a href="https://arxiv.org/abs/1706.09516">CatBoost paper</a> etc.</p>
<p>I think WoE is interesting in part because it's sort of on the
threshold between a simple pre-processing step and what might be
considered model stacking. It's a reminder that even “fancy” models
are just statistics with more steps; the mean is a model too.</p>
<hr />
<p>If you're working on credit scoring, maybe you'll also do feature
selection using “Information Value” (<a href="https://www.lexjansen.com/mwsug/2013/AA/MWSUG-2013-AA14.pdf">IV</a>). I don't know... It
reminds me a little bit of <a href="/2013/02/05/finding-the-mic-of-a-circle/">MIC</a>, almost? That's not quite right.
I'm not so interested in IV right now.</p>
<!-- mathjax for formulas -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML" async></script>
    </article>
    <footer>
      <hr />
<p><a name="contact"></a><form class="email_updates">
  <input type="email" name="email" placeholder="your@email.address" style="width: 49%" />
  <input type="submit" value="Get email updates" style="width: 49%" />
  <input type="hidden" name="_subject" value="planspace.org updates list" />
  <input type="text" name="_honey" style="display:none" />
  <input type="hidden" name="_captcha" value="false" />
</form></p>
<p><a id="back_link2" href="/">This site</a> also has <a href="/rss.xml">RSS</a>. You can connect with <a id="aaron_link2" href="/aaron/">me</a> via <a href="https://twitter.com/planarrowspace">Twitter</a>, <a href="https://www.linkedin.com/in/ajschumacher">LinkedIn</a>, <a href="https://github.com/ajschumacher">GitHub</a>, and <a href="mailto:aaron@planspace.org">email</a>.</p>

      <hr />
    </footer>

<!-- my weird stuff -->
<script src="/scripts/planspace.js"></script>

<!-- syntax highlighting -->
<script src="/scripts/highlight.pack.js"></script>
<script type="text/javascript">hljs.initHighlightingOnLoad();</script>

<!-- Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-44351009-1', 'auto');
  ga('send', 'pageview');
</script>

<!-- Disqus comments -->
<div id="disqus_thread"></div>
<script type="text/javascript">
  var disqus_shortname = 'planspace';
  (function() {
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>

  </body>
</html>
