<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <link rel="stylesheet" type="text/css" href="/styles/csshake.min.css">
    <link rel="stylesheet" type="text/css" href="/styles/zenburn.css">
    <link rel="stylesheet" type="text/css" href="/styles/planspace.css">
    <title>TensorFlow use of Google Technologies</title>
  </head>
  <body>
    <article>
<h1>TensorFlow use of Google Technologies</h1>
<p class="date">Monday March 13, 2017</p>
<p>TensorFlow is a large project. It has a lot of unique features, and as a Google project, it also connects with other Google projects: gflags, Bazel, protobuf, gRPC, gemmlowp, StreamExecutor, GFile, and even XLA. To better understand and use TensorFlow, it helps to know what these pieces are and how they fit together.</p>
<hr />
<h3>gflags</h3>
<p>In TensorFlow code, you may see, for <a href="https://github.com/GoogleCloudPlatform/cloudml-samples/blob/master/mnist/trainable/trainer/task.py">example</a>, <code>tf.app.flags.FLAGS</code>. This is part of a TensorFlow <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/platform/flags.py">implementation</a> for <a href="https://en.wikipedia.org/wiki/Command-line_interface">command-line</a> argument parsing. It happens automatically when using <code>tf.app.run()</code>. Internally it uses Python's standard <a href="https://docs.python.org/3/library/argparse.html">argparse</a>, but the API is inspired by Google's gflags.</p>
<p>Formerly Google Commandline Flags, gflags is a Google system for building standard command-line interfaces. There's a <a href="https://github.com/gflags/gflags">C++ implementation</a>, which has <a href="https://gflags.github.io/gflags/">a documentation page</a> reminiscent of a command-line interface. The <a href="https://github.com/google/python-gflags">Python gflags</a> is documented primarily via a collection of <a href="https://github.com/google/python-gflags/tree/master/examples">examples</a>.</p>
<p>Python users may be more familiar with the standard <a href="https://docs.python.org/3/library/argparse.html">argparse</a> API, or perhaps even the older <a href="https://docs.python.org/2/library/optparse.html">optparse</a>, which both achieve functionality similar to that of gflags.</p>
<p>While TensorFlow doesn't include a full gflags implementation, the TensorFlow version appears in pretty many code examples, where it imparts a little bit of extra Google flavor to argument handling for TensorFlow scripts. You can use it (though there are <a href="http://stackoverflow.com/questions/33932901/whats-the-purpose-of-tf-app-flags-in-tensorflow/33938519#33938519">reasons not to</a>) or opt for a separate and possibly more full-featured package, whether it be the full <a href="https://github.com/google/python-gflags">gflags</a>, <a href="https://docs.python.org/3/library/argparse.html">argparse</a>, or <a href="https://pythonhosted.org/horetu/">something stranger</a>.</p>
<hr />
<h3>Bazel</h3>
<p><a href="https://bazel.build/">Bazel</a> is a build tool like <a href="https://www.gnu.org/software/make/">make</a>. Bazel is <a href="https://github.com/bazelbuild/bazel">open source</a>; <a href="https://en.wikipedia.org/wiki/Bazel_(software)">originally</a> there was Google's internal "Blaze" tool.</p>
<p>You might encounter Bazel if you're <a href="https://www.tensorflow.org/install/install_sources">building TensorFlow from source</a>, for example.</p>
<p><img alt="bazel" src="img/bazel.png" /></p>
<hr />
<h3>protobuf</h3>
<p><a href="https://developers.google.com/protocol-buffers/">Protocol buffers</a> (often protobuf, or just proto) "are Google's [<a href="https://github.com/google/protobuf">open source</a>] language-neutral, platform-neutral, extensible mechanism for serializing structured data – think XML, but smaller, faster, and simpler."</p>
<p>When you look at protocol buffers in their text representations, they look something like <a href="http://www.json.org/">JSON</a>.</p>
<p>You likely won't encounter protocol buffers directly when using TensorFlow. They're used under the hood all over the place: for example, as the format for <a href="https://www.tensorflow.org/get_started/summaries_and_tensorboard">TensorBoard</a> summaries, and to help make data transfer efficient in distributed settings.</p>
<hr />
<h3>gRPC</h3>
<p><a href="http://www.grpc.io/">gRPC</a> is Google's <a href="https://github.com/grpc/grpc">open source</a> framework for <a href="https://en.wikipedia.org/wiki/Remote_procedure_call">remote procedure call (RPC)</a> systems. It uses protocol buffers.</p>
<p>If RPC is unfamiliar, there is some comparison to <a href="https://en.wikipedia.org/wiki/Representational_state_transfer">RESTful</a> communications for web services.</p>
<p>You might encounter gRPC if you build a client for a <a href="https://tensorflow.github.io/serving/">TensorFlow serving</a> server, for example.</p>
<p><img alt="gRPC" src="img/grpc.svg" /></p>
<hr />
<h3>gemmlowp</h3>
<p><a href="https://github.com/google/gemmlowp">gemmlowp</a> is Google's open source low-precision matrix multiplication library.</p>
<p>You won't likely encounter gemmlowp directly, but you'll benefit from it if you're using TensorFlow for <a href="https://github.com/google/gemmlowp/blob/master/doc/low-precision.md">low-precision</a> implementations.</p>
<hr />
<h3>StreamExecutor</h3>
<p><a href="http://www.nvidia.com/">NVIDIA</a> currently dominates the <a href="https://en.wikipedia.org/wiki/Graphics_processing_unit">GPU</a> market, and so their <a href="https://en.wikipedia.org/wiki/CUDA">CUDA</a> language for programming GPUs is the dominant GPU language. But other vendors would like to sell GPUs too, and they would like everybody to standardize on <a href="https://en.wikipedia.org/wiki/OpenCL">OpenCL</a> for programming them. Programmers would rather just program once for whatever platform; they would like to have <a href="https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units">general-purpose computing on graphics processing units (GPGPU)</a>.</p>
<p>StreamExecutor is Google's GPGPU system. They've <a href="http://lists.llvm.org/pipermail/llvm-dev/2016-March/096576.html">talked about</a> open-sourcing it directly as part of the <a href="http://llvm.org/">LLVM project</a>, but as far as I know it still exists in open source only <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/stream_executor">inside TensorFlow</a>.</p>
<p>OpenCL support is not necessarily perfect, but you may be able to try it out if you <a href="https://www.tensorflow.org/install/install_sources">build from source</a>. It's <a href="https://github.com/tensorflow/tensorflow/issues/22">issue #22</a> for the <a href="https://github.com/tensorflow/tensorflow/issues/22">TensorFlow GitHub project</a>.</p>
<p>StreamExecutor is another implementation technology that you aren't likely to encounter directly as a user of TensorFlow.</p>
<hr />
<h3>GFile</h3>
<p>Google has C++ file I/O code that avoids thread locking, and <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/lib/io">this</a> is <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/lib/io/file_io.py">included</a> as <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/platform/gfile.py">part</a> TensorFlow.</p>
<p>This is another implementation technology that should benefit TensorFlow users silently within the system.</p>
<hr />
<h3>XLA</h3>
<p>The <a href="https://www.tensorflow.org/versions/master/experimental/xla/">Accelerated Linear Algebra (XLA)</a> system is described as <a href="https://haosdent.gitbooks.io/tensorflow-document/content/resources/xla_prerelease.html">the TensorFlow compiler framework</a>, but there's more to it than that.</p>
<p>For one thing, the aspects of XLA that appear in open source TensorFlow aren't all the XLA that exists; Google uses their own hardware, <a href="https://en.wikipedia.org/wiki/Tensor_processing_unit">TPUs</a>, and both that hardware and the XLA components to target it are internal to Google.</p>
<p>For another, XLA has been described as <a href="https://autodiff-workshop.github.io/slides/JeffDean.pdf">"designed for reuse"</a>, and other projects could incorporate the technology as well.</p>
<p>XLA is still experimental, and largely deep inside TensorFlow's implementation. Users might <a href="https://gist.github.com/yaroslavvb/53052184e50cdfec35f0a127dd6df843">try</a> turning on XLA to see whether it helps speed up their computations.</p>
<hr />
<p>Thanks to Googlers <a href="https://twitter.com/juliaferraioli">Julia Ferraioli</a>, <a href="https://twitter.com/petewarden">Pete Warden</a>, and <a href="https://twitter.com/martin_wicke">Martin Wicke</a>, for a brief <a href="https://twitter.com/petewarden/status/841062427202527232">twitter exchange</a> that informed this list, and to <a href="https://twitter.com/mrry">Derek Murray</a> for <a href="https://twitter.com/mrry/status/841315298221342720">correcting</a> my misconception about gflags in TensorFlow.</p>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    </article>
    <footer>
      <hr />
      <ul>
        <li id="back_link">
          <a href="/">Plan <span class="rotate180">➔</span> Space</a>
        </li>
        <li>
          <a id="edit_link" href="https://github.com/ajschumacher/ajschumacher.github.io">Edit</a> this page
        </li>
        <li>
          Find <a id="aaron_link" href="/aaron/">Aaron</a> on
          <ul>
            <li>
              <a href="https://twitter.com/planarrowspace">Twitter</a>
            </li>
            <li>
              <a href="https://www.linkedin.com/in/ajschumacher">LinkedIn</a>
            </li>
            <li>
              <a href="https://plus.google.com/112658546306232777448/">Google+</a>
            </li>
            <li>
              <a href="https://github.com/ajschumacher">GitHub</a>
            </li>
            <li>
              <a href="mailto:ajschumacher@gmail.com">email</a>
            </li>
          </ul>
        </li>
        <li>
          Comment below
        </li>
      </ul>
      <hr />
    </footer>

<!-- my weird stuff -->
<script src="/scripts/planspace.js"></script>

<!-- syntax highlighting -->
<script src="/scripts/highlight.pack.js"></script>
<script type="text/javascript">hljs.initHighlightingOnLoad();</script>

<!-- Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-44351009-1', 'auto');
  ga('send', 'pageview');
</script>

<!-- Disqus comments -->
<div id="disqus_thread"></div>
<script type="text/javascript">
  var disqus_shortname = 'planspace';
  (function() {
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>

  </body>
</html>
