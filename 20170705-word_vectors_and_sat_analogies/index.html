<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <link rel="stylesheet" type="text/css" href="/styles/csshake.min.css">
    <link rel="stylesheet" type="text/css" href="/styles/zenburn.css">
    <link rel="stylesheet" type="text/css" href="/styles/planspace.css">
    <title>Word Vectors and SAT Analogies</title>
  </head>
  <body>
    <article>
<h1>Word Vectors and SAT Analogies</h1>
<p class="date">Wednesday July  5, 2017</p>
<p>In 2013, <a href="https://code.google.com/archive/p/word2vec/">word2vec</a> popularized <a href="http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/">word vectors</a> and <em>king - man + woman = queen</em>.</p>
<p><a href="http://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html"><img alt="king queen etc." src="img/king_queen.png" /></a></p>
<p>This reminded me of <a href="https://en.wikipedia.org/wiki/SAT">SAT</a> analogy questions, which <a href="http://blog.prepscholar.com/sat-analogies-and-comparisons-why-removed-what-replaced-them">disappeared from the SAT in 2005</a>, but looked like this:</p>
<pre><code>PALTRY : SIGNIFICANCE ::

A. redundant : discussion
B. austere : landscape
C. opulent : wealth
D. oblique : familiarity
E. banal : originality</code></pre>

<p>The king/queen example is not difficult, and I don't know whether it was tested or discovered. A better evaluation would use a set of challenging pre-determined questions.</p>
<p>There is a Google set of analogy questions, but all the relationships are grammatical, geographical, or by gender. Typical: "fast : fastest :: old : oldest." (<a href="http://download.tensorflow.org/data/questions-words.txt">dataset</a>, <a href="https://arxiv.org/abs/1301.3781">paper</a>, <a href="https://aclweb.org/aclwiki/Google_analogy_test_set_(State_of_the_art)">context</a>)</p>
<p>SAT questions are more interesting. Selecting from fixed answer choices provides a nice guessing baseline (1/5 is 20%) and using a human test means it's easier to get human performance levels (average US college applicant is 57%; human voting is 81.5%).</p>
<p>Michael Littman and Peter Turney have made available <a href="https://aclweb.org/aclwiki/SAT_Analogy_Questions_(State_of_the_art)">a set of 374 SAT analogy questions</a> since 2003. You have to email Turney to get them, and I appreciate that he helped me out.</p>
<p>Littman and Turney <a href="http://cogprints.org/4518/1/NRC-48273.pdf">used</a> a vector-based approach on their dataset back in 2005. They achieved 47% accuracy (state of the art at the time) which is a nice benchmark.</p>
<p>They made vectors for each word pair using web data. To get one value for "banal : originality" they would search <a href="https://en.wikipedia.org/wiki/AltaVista">AltaVista</a> for "banal and not originality" and take the log of the number of hits. With a list of 64 connectives they made vectors with 128 components.</p>
<p>I'm using <a href="https://nlp.stanford.edu/projects/glove/">GloVe</a> and word2vec word vectors that are per-word and based on various text corpora directly. Since the vectors are not specific to particular pairings, they may at a relative disadvantage for the SAT task. To get a vector for a word pair, I just subtract.</p>
<p>Stanford provides a variety of GloVe vectors pre-trained on three different corpora:</p>
<ul>
<li>Twitter (<code>glove.twitter</code>)<ul>
<li>2B tweets, 27B tokens, 1.2M vocab, uncased</li>
<li>as each of 25d, 50d, 100d, &amp; 200d vectors</li>
</ul>
</li>
<li>Wikipedia 2014 + Gigaword 5 (<code>glove.6B</code>)<ul>
<li>6B tokens, 400K vocab, uncased</li>
<li>as each of 50d, 100d, 200d, &amp; 300d vectors</li>
</ul>
</li>
<li>Common Crawl<ul>
<li>uncased (<code>glove.42B</code>) 42B tokens, 1.9M vocab, 300d vectors</li>
<li>cased (<code>glove.840B</code>) 840B tokens, 2.2M vocab, 300d vectors</li>
</ul>
</li>
</ul>
<p>I have one word2vec model: <code>GoogleNews-vectors</code> trained on a Google News dataset, providing 300d vectors.</p>
<p><img src="img/accuracy.png" width="480" /></p>
<ul>
<li>The best word vectors for the SAT analogies task (<code>glove.840B</code>) achieve 49% accuracy. This outperforms the 2005 47% result, but is still within the confidence bounds of 42.2% to 52.5% that they report.</li>
<li>Holding the training set constant and varying the size of the vectors affects performance on the SAT analogies task: bigger vectors work better, though performance may be plateauing around 300d.</li>
<li>Twitter data does markedly worse on the SAT analogies task. This is consistent with Twitter's limitations and reputation for being less than erudite.</li>
</ul>
<p>Trained word vectors provide a fixed vocabulary. When a word was missing, I used a vector of zeros. Most embeddings only missed one to eight words from the 374 questions, but <code>glove.twitter</code> missed 105. I also checked accuracies when excluding questions that had unsupported words for a set of embeddings, and the results were remarkably close, even with the Twitter embeddings. So the accuracies are due to the embeddings and not just gaps in the vocabularies.</p>
<p>It is pretty impressive that word vectors work as well as they do on this task, but nobody should consider word vectors a solution to natural language understanding. Problems with word vectors have been pointed out (<a href="http://www.aclweb.org/anthology/N15-1098">source</a>, <a href="http://www.aclweb.org/anthology/C/C16/C16-1262.pdf">source</a>, <a href="http://anthology.aclweb.org/W16-2503">source</a>, <a href="https://arxiv.org/abs/1705.11168">source</a>).</p>
<p>Still, the idea of word vectors (translating sparse data to a learned dense representation) is super useful, and not just for words. See implementations <a href="https://keras.io/layers/embeddings/">in Keras</a> and <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/layers/embedding_column">in TensorFlow</a>.</p>
<p>These methods are not the best-performing non-human technique for these SAT analogy questions. Littman and Turney <a href="https://aclweb.org/aclwiki/SAT_Analogy_Questions_(State_of_the_art)">report</a> several. Latent Relational Analysis comes in at 56% accuracy, against the average US college applicant at 57%.</p>
<p>This is a case in which "the average human" is not a good bar for measuring AI success. The average human has a remarkably small vocabulary relative to what a computer should easily handle, not to mention that the average human would not be admitted to most colleges you could name.</p>
<hr />
<p>I'm grateful to the <a href="https://www.meetup.com/DC-Deep-Learning-Working-Group/">DC Deep Learning Working Group</a> for helpful discussions, and particularly Shabnam Tafreshi and Dmitrijs Milajevs for sharing references at <a href="https://www.meetup.com/DC-Deep-Learning-Working-Group/events/237114317/">the January 26, 2017 meeting</a>. Thanks also to <a href="https://twitter.com/metasemantic">Travis Hoppe</a> of <a href="http://dc.hackandtell.org/">DC Hack and Tell</a> for always doing cool things with NLP. Thanks to Peter Turney for providing the dataset and commenting on distances.</p>
<p>Notes:</p>
<ul>
<li>An <a href="https://www.technologyreview.com/s/541356/king-man-woman-queen-the-marvelous-mathematics-of-computational-linguistics/">example</a> of word2vec popularization.</li>
<li>My code is available at <a href="https://github.com/ajschumacher/sat_analogies">ajschumacher/sat_analogies</a>.</li>
<li>All but 20 of the 374 questions have five designed answer choices; those 20 have four and "no : choice" as the last option. To maintain comparability I kept those questions in, though it means a human guessing baseline should be slightly over 20%.</li>
<li>For efficiency, I used GloVe values as 2-byte floats, which is lossy. I also tested with 4-byte floats, and the results varied by at most one question, while being much slower to generate.</li>
<li>Cosine distance is more effective than Euclidean distance for this task, and the advantage increases with dimensionality.</li>
</ul>
<p><img src="img/cosine_advantage.png" width="480" /></p>    </article>
    <footer>
      <hr />
<p><form class="email_updates">
  <input type="email" name="email" placeholder="your@email.address" style="width: 49%" />
  <input type="submit" value="Get email updates" style="width: 49%" />
  <input type="hidden" name="_subject" value="planspace.org updates list" />
  <input type="text" name="_honey" style="display:none" />
  <input type="hidden" name="_captcha" value="false" />
</form></p>
<p><a id="back_link2" href="/">This site</a> also has <a href="/rss.xml">RSS</a>. You can connect with <a id="aaron_link2" href="/aaron/">me</a> via <a href="https://twitter.com/planarrowspace">Twitter</a>, <a href="https://www.linkedin.com/in/ajschumacher">LinkedIn</a>, <a href="https://github.com/ajschumacher">GitHub</a>, and <a href="mailto:ajschumacher@gmail.com">email</a>.</p>

      <hr />
    </footer>

<!-- my weird stuff -->
<script src="/scripts/planspace.js"></script>

<!-- syntax highlighting -->
<script src="/scripts/highlight.pack.js"></script>
<script type="text/javascript">hljs.initHighlightingOnLoad();</script>

<!-- Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-44351009-1', 'auto');
  ga('send', 'pageview');
</script>

<!-- Disqus comments -->
<div id="disqus_thread"></div>
<script type="text/javascript">
  var disqus_shortname = 'planspace';
  (function() {
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>

  </body>
</html>
