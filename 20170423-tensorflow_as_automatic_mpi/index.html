<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <link rel="stylesheet" type="text/css" href="/styles/csshake.min.css">
    <link rel="stylesheet" type="text/css" href="/styles/zenburn.css">
    <link rel="stylesheet" type="text/css" href="/styles/planspace.css">
    <title>TensorFlow as Automatic MPI</title>
  </head>
  <body>
    <article>
<h1>TensorFlow as Automatic MPI</h1>
<p class="date">Sunday April 23, 2017</p>
<p>TensorFlow raises the level of abstraction in distributed programs from message-passing to data structures and operations directly on those data structures. The difference is analogous to the difference between programming in <a href="https://en.wikipedia.org/wiki/Assembly_language">assembly</a> and programming in a high-level language. TensorFlow may not have every possible high-performance feature for cluster computing, but what it offers is compelling.</p>
<hr />
<h2>Message-Passing</h2>
<p>Leaving aside shared filesystems or directly accessed shared memory, systems that run across multiple computers have to communicate by sending messages to one another. A low-level approach might use <a href="https://en.wikipedia.org/wiki/Transmission_Control_Protocol">TCP</a> sockets directly. Some convenience could be obtained by using Remote Procedure Calls (<a href="https://en.wikipedia.org/wiki/Remote_procedure_call">RPC</a>).</p>
<p>The Message-Passing Interface (<a href="https://en.wikipedia.org/wiki/Message_Passing_Interface">MPI</a>) was created in the early 1990s to make distributed programming easier within the High Performance Computing (HPC) community. It standardized interfaces like <code>MPI_Send</code> and <code>MPI_Recv</code> to facilitate exchanging data between processes of a distributed system.</p>
<p>The <a href="https://en.wikipedia.org/wiki/Actor_model">actor model</a> also focuses on message-passing, as for example in <a href="https://en.wikipedia.org/wiki/Erlang_(programming_language)">Erlang</a> or with <a href="http://akka.io/">Akka</a> in Java or Scala.</p>
<p>Message-passing gives you fine-grained control over how nodes communicate. You can use message-passing to build up a variety of algorithms and new systems. For example, <a href="http://spark.apache.org/">Spark</a> was built in part with Akka agents for some time, before switching to an RPC-based implementation. TensorFlow also builds its own abstractions using message-passing.</p>
<hr />
<h2>Automatic Message-Passing with TensorFlow</h2>
<p>TensorFlow uses message-passing, but it's largely invisible to TensorFlow users. The TensorFlow API lets you say where data and operations should live, and TensorFlow automatically handles any necessary messaging.</p>
<p>Programming with explicit messages is like the low-level memory shifting necessary when programming in <a href="https://en.wikipedia.org/wiki/Assembly_language">assembly</a>. For example, here is some assembly pseudo-code:</p>
<pre><code>copy value from position `a` to register 1
copy value from position `b` to register 2
add register 1 and 2
copy result to position `c`</code></pre>

<p>In a high-level language, this could be:</p>
<pre><code class="language-python">c = a + b</code></pre>

<p>Similarly, with TensorFlow, you focus on the computation you want performed, and don't worry about how values may need to be moved around to make it happen. A distributed analog to the assembly above might look like this:</p>
<pre><code>send value of `a` from machine A to machine D
send value of `b` from machine B to machine D
add values on machine D
send result to machine C</code></pre>

<p>And with TensorFlow:</p>
<pre><code class="language-python">c = a + b</code></pre>

<p>At least, that's the ideal. TensorFlow may still need explicit direction on where things should be placed (<a href="https://www.tensorflow.org/api_docs/python/tf/device">with tf.device</a>) especially when multiple machines are involved. But that direction is more succinct and declarative than the full imperative detail of message-passing.</p>
<hr />
<h2>Faster Messages and Collective Communications</h2>
<p>TensorFlow's user API is essentially message-less, but TensorFlow still uses messages under the hood. The contents of the messages are serialized protocol buffers sent via <a href="http://www.grpc.io/">gRPC</a>, which uses <a href="https://en.wikipedia.org/wiki/HTTP/2">HTTP/2</a>.</p>
<p>A message-based approach can be made faster by using a faster transport, and by using faster algorithms. TensorFlow has some early support for both, but other frameworks may offer more. You may or may not care, as the simplicity of TensorFlow's approach may or may not outweigh possible performance boosts.</p>
<p>One easy transport speedup could come in hardware with NVIDIA's <a href="http://www.nvidia.com/object/nvlink.html">NVLink</a> interconnect, which is faster than <a href="https://en.wikipedia.org/wiki/PCI_Express">PCIe</a>. As far as I can tell NVLink won't require code changes, but it looks like it will be pretty rare: it may be that only some supercomputers (<a href="https://www.olcf.ornl.gov/summit/">Summit</a>, <a href="https://asc.llnl.gov/coral-info">Sierra</a>) will use NVLink, with IBM's <a href="https://en.wikipedia.org/wiki/IBM_POWER_microprocessors">POWER</a> processors.</p>
<p>Some clusters connect machines with <a href="https://en.wikipedia.org/wiki/InfiniBand">InfiniBand</a> rather than <a href="https://en.wikipedia.org/wiki/Ethernet">ethernet</a>. InfiniBand Remote Direct Memory Access (<a href="https://en.wikipedia.org/wiki/Remote_direct_memory_access">RDMA</a>) is supposed to be pretty fast.</p>
<p>Jun Shi at Yahoo contributed an <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/README.md">implementation</a>, <a href="https://github.com/tensorflow/tensorflow/pull/8943">merged</a> four days ago, that begins to let TensorFlow use RDMA for transport via IB verbs, in addition to gRPC.</p>
<p>MPI implementations often support InfiniBand, and an advantage of using MPI is that you can let that implementation worry about InfiniBand support without writing your own RDMA code. There's an <a href="https://github.com/tensorflow/tensorflow/pull/7710">open pull request</a> to give TensorFlow this kind of MPI integration.</p>
<p>On the algorithm side, there are <a href="https://computing.llnl.gov/tutorials/mpi/#Collective_Communication_Routines">collective communication</a> operations that can be optimized, and MPI implementations tend to be good at these. For example, <a href="http://research.baidu.com/bringing-hpc-techniques-deep-learning/">ring allreduce</a> is more efficient for syncing up model parameters than sending them all to a central parameter server and then sending them all back out. Baidu has a <a href="https://github.com/baidu-research/tensorflow-allreduce">fork</a> of TensorFlow that adds their <a href="https://github.com/baidu-research/baidu-allreduce">implementation</a> of ring allreduce using MPI at <a href="https://github.com/baidu-research/tensorflow-allreduce/tree/master/tensorflow/contrib/mpi"><code>tf.contrib.mpi</code></a>.</p>
<p>Outside the MPI world, the NVIDIA Collective Communications Library (<a href="https://devblogs.nvidia.com/parallelforall/fast-multi-gpu-collectives-nccl/">NCCL</a>) works with multiple GPUs on the same machine. TensorFlow has some support for this via <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/nccl"><code>tf.contrib.nccl</code></a>.</p>
<p>Quite distinct from TensorFlow, the Facebook Incubator project <a href="https://github.com/facebookincubator/gloo">Gloo</a> focuses on collective communication algorithms and supports transport via TCP/IP and InfiniBand without MPI. Gloo is used by <a href="https://caffe2.ai/">Caffe2</a>; the Caffe2 example <a href="https://github.com/caffe2/caffe2/blob/master/caffe2/python/examples/resnet50_trainer.py">resnet50 trainer</a> uses Gloo's ring allreduce.</p>
<!--

Just for fun, here's an Akka implementation of allreduce that I found:
https://github.com/brianmartin/akka-allreduce
I believe that's more of a tree allreduce than a ring allreduce.

-->

<hr />
<h2>Single-Machine Parallelism</h2>
<p>The HPC community has an approach called "MPI everywhere" which means running one single-threaded MPI processes per CPU core. The simplicity of writing single-threaded programs with all parallelism via MPI is attractive, but it's not necessarily the best way to use a machine with a lot of cores and possibly GPUs.</p>
<p>The hybrid approach is to have parallelism both across multiple machines and within each individual machine, usually via threads. HPC folks seem to like to use <a href="https://en.wikipedia.org/wiki/OpenMP">OpenMP</a> for multithreading.</p>
<p>TensorFlow supports this kind of within-process parallelism. TensorFlow uses threads pretty freely internally, and you can add your own additional parallelism with Python's <a href="https://docs.python.org/3/library/threading.html">threading</a> or <a href="https://docs.python.org/3/library/multiprocessing.html">multiprocessing</a> libraries, among many possible options. Just as with MPI though, when programs are run by a cluster manager, resources may be restricted.</p>
<hr />
<h2>Code organization</h2>
<p>It isn't strictly required for either, but it seems most common to write MPI programs and distributed TensorFlow programs with one entry point that quickly specializes based on the particular role in the cluster for that node. As this implies, every invocation of the program has to know something about the cluster and its place in it.</p>
<hr />
<h2>Cluster Topology</h2>
<p>MPI implementations provide a mechanism in starting a group of processes by which every process knows how many there are in its group and its position or <em>rank</em> in the group. The rank 0 process, called the <em>root</em>, is generally special.</p>
<p>TensorFlow leaves the starting of distributed processes to a cluster manager like Kubernetes. It's common to have several task groups like <code>ps</code> and <code>worker</code>, with a given process having a task type and a number within that group.</p>
<p>Gloo is interestingly different in that individual processes may not start with knowledge of the cluster topology, but only with a reference to a central broker which could be a file on a shared filesystem or a Redis server. Each process checks in with and gets information about other members of the cluster via the central broker. Gloo calls this process <em>rendezvous</em>. As with MPI, processes find out about just one group of processes, and in fact Gloo can use MPI for its initial rendezvous setup.</p>
<hr />
<h2>See also</h2>
<ul>
<li><a href="/20170328-tensorflow_as_a_distributed_virtual_machine/">TensorFlow as a Distributed Virtual Machine</a></li>
<li><a href="/20170410-tensorflow_clusters_questions_and_code/">TensorFlow Clusters: Questions and Code</a></li>
<li><a href="/20170411-distributed_mapreduce_with_tensorflow/">Distributed MapReduce with TensorFlow</a></li>
</ul>
<hr />
<p>Thanks to <a href="https://twitter.com/danfujita123">Dan Fujita</a> for suggesting more connections between TensorFlow and MPI, and sharing some MPI code that was helpful. Thanks also to <a href="https://twitter.com/ljdursi">Jonathan Dursi</a>, whose <a href="https://www.dursi.ca/">blog</a> I read with interest. And thanks to <a href="https://github.com/cliffwoolley">Cliff Woolley</a> of NVIDIA for <a href="https://github.com/NVIDIA/nccl/issues/86">clarifying</a> the meaning of NCCL.</p>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    </article>
    <footer>
      <hr />
<p>If you sign up, I'll send you an email update at most monthly with new stuff.</p>
<iframe src="https://planspace.substack.com/embed" width="100%" frameborder="0" scrolling="no"></iframe>
<p><a id="back_link2" href="/">This site</a> also has <a href="/rss.xml">RSS</a>. You can connect with <a id="aaron_link2" href="/aaron/">me</a> via <a href="https://twitter.com/planarrowspace">Twitter</a>, <a href="https://www.linkedin.com/in/ajschumacher">LinkedIn</a>, <a href="https://github.com/ajschumacher">GitHub</a>, and <a href="mailto:aaron@planspace.org">email</a>.</p>

      <hr />
    </footer>

<!-- my weird stuff -->
<script src="/scripts/planspace.js"></script>

<!-- syntax highlighting -->
<script src="/scripts/highlight.pack.js"></script>
<script type="text/javascript">hljs.initHighlightingOnLoad();</script>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-EZBQMHT77F"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-EZBQMHT77F');
</script>

<!-- Disqus comments -->
<div id="disqus_thread"></div>
<script type="text/javascript">
  var disqus_shortname = 'planspace';
  (function() {
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>

  </body>
</html>
