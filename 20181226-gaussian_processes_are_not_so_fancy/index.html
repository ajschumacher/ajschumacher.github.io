<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <link rel="stylesheet" type="text/css" href="/styles/csshake.min.css">
    <link rel="stylesheet" type="text/css" href="/styles/zenburn.css">
    <link rel="stylesheet" type="text/css" href="/styles/planspace.css">
    <title>Gaussian Processes are Not So Fancy</title>
  </head>
  <body>
    <article>
<h1>Gaussian Processes are Not So Fancy</h1>
<p class="date">Wednesday December 26, 2018</p>
<p><a href="https://en.wikipedia.org/wiki/Gaussian_process">Gaussian Processes</a>
have a mystique related to the dense probabilistic terminology that's
already evident in their name. But Gaussian Processes are just models,
and they're much more like k-nearest neighbors and linear regression
than may at first be apparent.</p>
<p><img alt="Predictive mean and range" src="img/predictive_mean_and_range.png" /></p>
<p>Gaussian Processes have
<a href="https://en.wikipedia.org/wiki/Kriging#Applications">applications</a>
ranging from finding gold to
<a href="https://cloud.google.com/blog/products/gcp/hyperparameter-tuning-cloud-machine-learning-engine-using-bayesian-optimization">optimizing hyperparameters</a>
of other models. The focus here is on how Gaussian Processes work,
using an example that's simple enough to show completely from
beginning to end.</p>
<hr />
<h3>A simple training data set</h3>
<p>A model is trained with predictors \( X \) and known labels \( y
\). Here's some data in Python:</p>
<pre><code class="language-python">train_X = [[0.8], [1.2], [3.8], [4.2]]
train_y = [   3,     4,    -2,    -2 ]</code></pre>

<p>Since elements of \( X \) are one-dimensional, all the data can be
shown in a simple figure:</p>
<p><img alt="Training data" src="img/training_data.png" /></p>
<p>The task of a model is to predict \( y \) values for test points \(
x \).</p>
<hr />
<h3>Applying a kernel function</h3>
<p>Like <a href="https://en.wikipedia.org/wiki/Support_vector_machine">SVMs</a>,
Gaussian Processes use kernel functions. A kernel gives a closeness,
or similarity, between two points. This is related to distances, and a
kernel may involve distance. Here's the matrix of Euclidean distances
between points in our training data \( X \):</p>
<pre><code class="language-python">dist_XX = sklearn.metrics.pairwise_distances(train_X)
## array([[0. , 0.4, 3. , 3.4],
##        [0.4, 0. , 2.6, 3. ],
##        [3. , 2.6, 0. , 0.4],
##        [3.4, 3. , 0.4, 0. ]])
# Recall:
# train_X = [[0.8], [1.2], [3.8], [4.2]]</code></pre>

<p>The Gaussian radial basis function (RBF) kernel is commonly used. In
<a href="http://gaussianprocess.org/gpml/chapters/">Gaussian Processes for Machine Learning</a>,
Rasmussen and Williams call it the <em>squared exponential</em> kernel,
probably to avoid confusion with other things that are Gaussian. For
distance \( d \), it's \( e^{-\frac{1}{2}d^2}\):</p>
<pre><code class="language-python">def squared_exponential(distance):
    return np.exp(distance**2 / -2)</code></pre>

<p>It's one when distance is zero, and it goes to zero when distance is
big. This is evident for our data when we make the matrix of kernel
values for the training data \( X \):</p>
<pre><code class="language-python">kern_XX = squared_exponential(dist_XX)
## array([[1.  , 0.92, 0.01, 0.  ],
##        [0.92, 1.  , 0.03, 0.01],
##        [0.01, 0.03, 1.  , 0.92],
##        [0.  , 0.01, 0.92, 1.  ]])
# Recall:
# train_X = [[0.8], [1.2], [3.8], [4.2]]</code></pre>

<p>This matrix \( K(X, X) \) is a core component of Gaussian Processes,
and as the example shows, it reflects a core concern with nearness as
represented via a kernel function.</p>
<hr />
<h3>Using the Gaussian Process prediction equation</h3>
<p>This is Rasmussen and Williams' Equation 2.19 for the predictive
posterior distribution, which I promise isn't as bad as it looks:</p>
<p>\[ \mathbf{f_{\ast}} | X_{\ast}, X, \mathbf{f} \sim \mathcal{N}( K(X_{\ast}, X) K(X,X)^{-1} \mathbf{f}, \\ K(X_{\ast}, X_{\ast}) - K(X_{\ast}, X) K(X,X)^{-1} K(X, X_{\ast}) ) \]</p>
<p>Here \( \mathbf{f_{\ast}} \) is the predicted \( y \) for a test
point in \( X_{\ast} \) based on the training data \( X \) and \(
y \) labels \( \mathbf{f} \). It's predicted to be normally
distributed with mean \( K(X_{\ast}, X) K(X,X)^{-1} \mathbf{f} \)
and the given covariance. The mean can be considered "the" prediction,
though you can also sample.</p>
<hr />
<h3>Behaves like nearest neighbors</h3>
<p>Let's use Equation 2.19 to make a prediction for this \( X_{\ast} \):</p>
<pre><code class="language-python">test_X = [[1]]</code></pre>

<p>We find the kernel similarities between the test point and each point
of training data:</p>
<pre><code class="language-python">kern_xX = squared_exponential(
              sklearn.metrics.pairwise_distances(test_X, train_X))
## array([[0.98019867, 0.98019867, 0.01984109, 0.00597602]])</code></pre>

<p>The test point is close to the first two training points, and far from
the second two.</p>
<p>Let's evaluate just the first two terms of the predictive mean given
in Equation 2.19:</p>
<pre><code class="language-python">kern_xX.dot(np.linalg.inv(kern_XX))
## array([[ 0.50835358,  0.51127124, -0.01378216,  0.01144869]])</code></pre>

<p>It isn't exactly, but this looks a lot like a weighting for a weighted
average. And the prediction roughly fits that interpretation:</p>
<pre><code class="language-python">test_y = kern_xX.dot(np.linalg.inv(kern_XX)).dot(train_y)[0]
## 3.57</code></pre>

<p><img alt="Predictive mean at one point" src="img/predictive_mean_at_one_point.png" /></p>
<p>This behavior is similar to k-nearest neighbors. Nearby points matter;
points that are far away don't. Nearest neighbors can even include a
weighting based on a kernel function.</p>
<p>Also like k-nearest neighbors, you have to use the whole training set
for every Gaussian Process prediction, comparing the test point to
every training point.</p>
<p>With Gaussian Processes, however, you don't have to specify a number
of neighbors \( k \). Every point that is near enough contributes to
the prediction.</p>
<p>The comparison here to a kind of weighted average nearest neighbors is
much more directly valid for
<a href="https://en.wikipedia.org/wiki/Kernel_regression">kernel regression</a>,
which is another technique that also uses kernels. With Gaussian
Processes, there's really a further step of curve-fitting going on.</p>
<hr />
<h3>It's linear regression</h3>
<p>The mean prediction of a Gaussian Process is the same as a linear
regression with a particular choice of coordinates. Let's talk about
how.</p>
<p>Why is \( K(X,X)^{-1} \) involved in the predictive mean?</p>
<p>Consider the kernel matrix as a transformation of the original
training data \( X \) into new variables, where each of the new
variables is kernel-nearness to one of the points of training data.
This is very much like transforming raw data to include interactions
or polynomial terms, as is common in regression. Say the transformed
data is \( Z \).</p>
<p>Linear regression coefficients
<a href="https://en.wikipedia.org/wiki/Ordinary_least_squares#Matrix/vector_formulation">can be solved for</a>
with \( (Z^T Z)^{-1} Z^T \). But with a nice symmetric square matrix
like \( K(X,X) \), this is the same as just taking \( Z^{-1} \)
directly.</p>
<p>So the mean prediction of a Gaussian Process is the same as linear
regression in the coordinates defined by kernel similarities with each
training point. It <em>is</em> linear regression.</p>
<hr />
<h3>With covariance</h3>
<p>With Gaussian Processes, kernel functions are also called covariance
functions. Things that are kernel-near vary together—they have similar
values—and this enforces smoothness.</p>
<p>Here's the covariance part of Equation 2.19 again. It has two terms:</p>
<p>\[ K(X_{\ast}, X_{\ast}) - K(X_{\ast}, X) K(X,X)^{-1} K(X, X_{\ast}) \]</p>
<p>The positive first term shows that test point(s) vary together when
they're close to one another. The negative second term (which is also
a linear regression solution, like the mean) captures how much
variance is eliminated due to being close to observed points.</p>
<!-- How to read the negative term as linear regression: The data set
X is, for each training point, kernel nearness to each other training
point. The training labels y are kernel nearness from test data to
training data. Then the test point has its nearness to the training
data, and we get out a consolidated estimate of nearness of this test
point to the training data. It can be thought of like a local weighted
average, just like for the mean. -->

<hr />
<h3>Predicting for multiple points</h3>
<p>If you get the predictive mean for many points, you can draw out a
curve:</p>
<p><img alt="Predictive mean at many points" src="img/predictive_mean_at_many_points.png" /></p>
<p>You may instead want to use the predictive mean and variance at some
test point to <em>sample</em> an outcome \( y \). If you then take that
sampled value as a new point of training data (adding it to your
training set) future predictions will be consistent with that first
one, and so on.</p>
<p>You can equivalently make multiple consistent predictions
simultaneously by putting multiple test points in \( X_{\ast} \) and
sampling from a multivariate Gaussian.</p>
<p>Just like the mean alone, values sampled this way will draw out a
smooth curve. But unlike the mean alone, each random draw will be a
different curve: Gaussian Processes are random over a space of
functions. An approachable
<a href="http://katbailey.github.io/post/gaussian-processes-for-dummies/">post by Bailey</a>
focuses on this.</p>
<p>It can be fun to sample curves, but often the mean and variance alone
are useful.</p>
<p><img alt="Predictive mean and range" src="img/predictive_mean_and_range.png" /></p>
<hr />
<h3>The Bayesian prior away from data</h3>
<p>Gaussian Processes have Bayesian priors. For the example here, the
assumption is that the function is always zero with covariance one,
until we see training data showing otherwise.</p>
<p>In Equation 2.19, you can think of the mean as zero plus contributions
from the training data; that's where the zero comes from. Covariance
comes from the kernel function, which gets as big as one. To change
the prior on the covariance, change the kernel function.</p>
<p><img alt="Predictive mean and range (wider view)" src="img/predictive_mean_and_range_wider_view.png" /></p>
<p>The prior is visible when the bounds of the plot are expanded, which
illustrates that Gaussian Processes often focus on local interpolation
more than extrapolation.</p>
<hr />
<h3>Length scale matters</h3>
<p>The kernel specifies the scale of the variance, and in the case of the
squared exponential kernel, there's also a length scale parameter that
has significant effects.</p>
<pre><code class="language-python">def squared_exponential(distance, length_scale=1):
    return np.exp((distance / length_scale)**2 / -2)</code></pre>

<p>For our example, a length scale of one works reasonably well. For a
smaller length scale, the function is allowed to change faster, and
the prior asserts itself more quickly:</p>
<p><img alt="Predictive mean with length scale = 0.05" src="img/predictive_mean_with_small_length_scale.png" /></p>
<p>For a Gaussian Process to capture a trend, its kernel needs to support
it. In the case of the squared exponential kernel, this means a long
enough length scale.</p>
<hr />
<h3>When hyperparameters are your parameters</h3>
<p>As presented here, a Gaussian Process will always exactly fit its
training data. This is often considered a sign of overfitting, and
regardless it's clearly unsustainable if training data ever has two
different \( y \) values for identical \( x \) values.</p>
<p>Noise can be added to address these issues, and the scale of the noise
is another hyperparameter, joining the overall scale and length scale
of the covariance function.</p>
<p>To get really interesting behavior may require composing kernel
functions, altering what "near" means for the model and adding even
more hyperparameters, as in
<a href="https://scikit-learn.org/stable/modules/gaussian_process.html#gpr-on-mauna-loa-co2-data">this example</a>.</p>
<p>Gaussian Process implementations like
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html">sci-kit's</a>
try to automatically fit these hyperparameters, which may remove some
of the need to know what they should be in advance. But optimization
can't do all the work of designing an appropriate kernel for a
problem, or eliminate the difficulty of distances in high-dimensional
spaces.</p>
<p>There are also some approaches for improving computational efficiency.</p>
<hr />
<h3>Not useless, but not magical</h3>
<p>Even with enhancements, the fundamental nature of Gaussian Processes
is as presented here: local smooth curve fitting built on linear
regression.</p>
<p>A Gaussian Process might be useful for you. But please don't assume
that it is sophisticated just because the language around it often is,
or that its results are automatically true just because they have
error bars.</p>
<hr />
<p>The code and plots from this post are all in
<a href="https://github.com/ajschumacher/ajschumacher.github.io/blob/master/20181226-gaussian_processes_are_not_so_fancy/gaussian_processes.ipynb">a Jupyter notebook</a>.</p>
<p>Thanks to Erica Blom, Marco Pariguana, Sylvia Blom, Travis Hoppe, and Ajay Deonarine for reading drafts of this post and providing feedback. Thanks also to everybody <a href="https://www.reddit.com/r/MachineLearning/comments/ac6er3/d_gaussian_processes_are_not_so_fancy/">on Reddit</a>, <a href="https://news.ycombinator.com/item?id=18814776">on HN</a>, <a href="https://www.datatau.com/item?id=28391">on DataTau</a>, <a href="https://twitter.com/search?f=tweets&amp;vertical=default&amp;q=https%3A%2F%2Fplanspace.org%2F20181226-gaussian_processes_are_not_so_fancy%2F&amp;src=typd">on Twitter</a>, <a href="https://pinboard.in/url:077f95c750f2b064e882bafb0d7899b7a3eaf48a/">on Pinboard</a>, and elsewhere!</p>
<!-- mathjax for formulas -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML" async></script>
    </article>
    <footer>
      <hr />
<p>If you sign up, I'll send you an email update at most monthly with new stuff.</p>
<iframe src="https://planspace.substack.com/embed" width="100%" frameborder="0" scrolling="no"></iframe>
<p><a id="back_link2" href="/">This site</a> also has <a href="/rss.xml">RSS</a>. You can connect with <a id="aaron_link2" href="/aaron/">me</a> via <a href="https://twitter.com/planarrowspace">Twitter</a>, <a href="https://www.linkedin.com/in/ajschumacher">LinkedIn</a>, <a href="https://github.com/ajschumacher">GitHub</a>, and <a href="mailto:aaron@planspace.org">email</a>.</p>

      <hr />
    </footer>

<!-- my weird stuff -->
<script src="/scripts/planspace.js"></script>

<!-- syntax highlighting -->
<script src="/scripts/highlight.pack.js"></script>
<script type="text/javascript">hljs.initHighlightingOnLoad();</script>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-EZBQMHT77F"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-EZBQMHT77F');
</script>

<!-- Disqus comments -->
<div id="disqus_thread"></div>
<script type="text/javascript">
  var disqus_shortname = 'planspace';
  (function() {
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>

  </body>
</html>
