<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <link rel="stylesheet" type="text/css" href="/styles/csshake.min.css">
    <link rel="stylesheet" type="text/css" href="/styles/zenburn.css">
    <link rel="stylesheet" type="text/css" href="/styles/planspace.css">
    <title>Linear Algebra Done Right, by Axler</title>
  </head>
  <body>
    <article>
<h1>Linear Algebra Done Right, by Axler</h1>
<p class="date">Sunday June 16, 2024</p>
<p>Axler's beautiful <a href="https://linear.axler.net/">textbook</a>, now in Open Access for the fourth
edition, is sort of an expansion of his 1995
<a href="https://www.axler.net/DwD.html" title="Down with Determinants!">Down with Determinants!</a> and is full of his characteristic
humanity, humor, joy, and good sense. I love the book's title most for
the implication that there <em>are</em> different ways that linear algebra
can be done—and Axler's book is convincing evidence that developing
the subject without prioritizing determinants can be done well.</p>
<p>The equation on the cover is the closed form solution for the \( n
\)th Fibonacci number, derived using linear algebra in an exercise on
page 174, just one of the book's treasures.</p>
<p><img src="cover.png" height=640 /></p>
<hr />
<blockquote>
<p>Do not memorize the formula for the product of two complex
numbers—you can always rederive it by recalling that \( i^2 = -1
\) and then using the usual rules of arithmetic (as given by 1.3).
(page 2)</p>
</blockquote>
<p>I love seeing advice like this in textbooks; sort of a didactic
principle of emphasizing what's fundamental and what's a
consequence... On the other hand you don't want to re-derive
everything every time, but there's some optimization of what gets a
spot in memory...</p>
<hr />
<blockquote>
<p>When we think of an element of \( \mathbb{R}^2 \) as an arrow, we
refer to it as a vector.</p>
<p>Whenever we use pictures in \( \mathbb{R}^2 \) or use the somewhat
vague language of points and vectors, remember that these are just
aids to our understanding, not substitutes for the actual
mathematics that we will develop. (page 8)</p>
</blockquote>
<p>Also good pedagogy here, I think: making a distinction between the
thing itself and a mental model or metaphor for it.</p>
<hr />
<blockquote>
<p>Mathematical models of the economy can have thousands of variables,
say \( x_1, ..., x_{5000} \), which means that we must work in \(
\mathbb{R}^{5000} \). Such a space cannot be dealt with
geometrically. However, the algebraic approach works well. Thus our
subject is called <strong>linear algebra</strong>. (page 8)</p>
</blockquote>
<p>I appreciate the attempt to justify the name, but I feel like this one
isn't completely satisfying... Good aspect: Gets at the centrality of
multi-dimensionality. Less good aspect: Doesn't really justify the
"linear" part.</p>
<hr />
<blockquote>
<p>We could define a multiplication in \( \mathbb{F}^n \) in a
similar fashion, starting with two elements of \( \mathbb{F}^n \)
and getting another element of \( \mathbb{F}^n \) by multiplying
corresponding coordinates. Experience shows that this definition is
not useful for our purposes. Another type of multiplication, called
scalar multiplication, will be central to our subject. (page 9)</p>
</blockquote>
<p>I feel like the appeal to experience is not a satisfying justification
for matrix multiplication. I like the <a href="/20210915-flow_metaphor_for_matrix_multiplication/" title="The flow metaphor for matrix multiplication">flow metaphor</a>, and I think
even better can be done than my exposition there...</p>
<hr />
<blockquote>
<p>In general, a vector space is an abstract entity whose elements
might be lists, functions, or weird objects. (page 14)</p>
</blockquote>
<p>Fun example of being both mathematically rigorous and friendly in
communication.</p>
<hr />
<blockquote>
<p>I at once gave up my former occupations, set down natural history
and all its progeny as a deformed and abortive creation, and
entertained the greatest disdain for a would-be science which could
never even step within the threshold of real knowledge. In this mood
I betook myself to the mathematics and the branches of study
appertaining to that science as being built upon secure foundations,
and so worthy of my consideration.</p>
<p>—Frankenstein, Mary Wollstonecraft Shelley (page 50)</p>
</blockquote>
<p>Axler has inserted some fun quotes at the ends of sections here and
there; this is one I liked.</p>
<hr />
<blockquote>
<p>We will frequently use the powerful fundamental theorem of linear
maps, which states that the dimension of the domain of a linear map
equals the dimension of the subspace that gets sent to 0 plus the
dimension of the range. (page 51)</p>
</blockquote>
<hr />
<blockquote>
<p>Thus the linear functions of high school algebra are not the same as
linear maps in the context of linear algebra. (page 56)</p>
</blockquote>
<p>He's using zero-to-zero to show this here. I think this is a really
helpful thing to emphasize immediately.</p>
<hr />
<blockquote>
<p>Consider linear maps \( T: U \to V \) and \(S: V \to W \). The
composition \( ST \) is a linear map from \( U \) to \( W \).
Does \( M(ST) \) equal \( M(S)M(T) \)? This question does not
yet make sense because we have not defined the product of two
matrices. We will choose a definition of matrix multiplication that
forces this question to have a positive answer. (page 72)</p>
</blockquote>
<p>Here is at least some justification of matrix multiplication. I think
it comes a little late, since matrices were introduced already, and
really they sort of co-design...</p>
<hr />
<blockquote>
<p>Note that we define the product of two matrices only when the number
of columns of the first matrix equals the number of rows of the
second matrix. (73)</p>
</blockquote>
<p>Couldn't this be made to feel less arbitrary?</p>
<hr />
<blockquote>
<p>The fundamental theorem of algebra is an existence theorem. Its
proof does not lead to a method for finding zeros. The quadratic
formula gives the zeros explicitly for polynomials of degree 2.
Similar but more complicated formulas exist for polynomials of
degree 3 and 4. No such formulas exist for polynomials of degree 5
and above. (page 124)</p>
</blockquote>
<p>Axler does a lot to connect these algebra ideas and linear algebra,
which is pretty cool.</p>
<p>It's the <a href="https://en.wikipedia.org/wiki/Abel%E2%80%93Ruffini_theorem" title="Abel–Ruffini theorem">Abel–Ruffini theorem</a> that shows there aren't formulas for
degree five and up. Is this mentioned when teaching the quadratic
formula? Would be cool to at least show the crazy equations for degree
three and four...</p>
<p>A later example:</p>
<blockquote>
<p>This exercise shows that every monic polynomial is the minimal
polynomial of some operator. Hence a formula or an algorithm that
could produce exact eigenvalues for each operator on each \(
\mathbb{F}^n \) could then produce exact zeros for each polynomial
[by 5.27(a)]. Thus there is no such formula or algorithm. However,
efficient numeric methods exist for obtaining very good
approximations for the eigenvalues of an operator. (page 152)</p>
</blockquote>
<hr />
<blockquote>
<p>Now we begin our investigation of operators, which are linear maps
from a vector space to itself. Their study constitutes the most
important part of linear algebra. (page 132)</p>
</blockquote>
<hr />
<blockquote>
<p>The word eigenvalue is half-German, half-English. The German prefix
eigen means “own” in the sense of characterizing an intrinsic
property. (page 134)</p>
</blockquote>
<p>I like this kind of explanation.</p>
<hr />
<blockquote>
<p>The main reason that a richer theory exists for operators (which map
a vector space into itself) than for more general linear maps is
that operators can be raised to powers. (page 137)</p>
</blockquote>
<hr />
<blockquote>
<p>A central goal of linear algebra is to show that given an operator
\( T \) on a finite-dimensional vector space \( V \), there
exists a basis of \( V \) with respect to which \( T \) has a
reasonably simple matrix. (page 154)</p>
</blockquote>
<hr />
<blockquote>
<p>You may recall from a previous course that every matrix of numbers
can be changed to a matrix in what is called row echelon form. If
one begins with a square matrix, the matrix in row echelon form will
be an upper-triangular matrix. Do not confuse this upper-triangular
matrix with the upper-triangular matrix of an operator with respect
to some basis whose existence is proclaimed by 5.47 (if \(
\mathbb{F} = \mathbb{C} \))—there is no connection between these
upper-triangular matrices. (page 160)</p>
</blockquote>
<hr />
<blockquote>
<p>The Euclidean algorithm for polynomials (look it up) can quickly
determine the greatest common divisor of two polynomials, without
requiring any information about the zeros of the polynomials. (page
173)</p>
</blockquote>
<p>I'm just amused by the "look it up".</p>
<hr />
<blockquote>
<p>Thus the pseudoinverse provides what is called a best fit to the
equation above. (page 222)</p>
</blockquote>
<p>I thought a bit about how linear regression is solved. The thing
that's usually written down, \( (X^T X)^{-1} X^T y \), is really
crummy computationally if \( X \) isn't well-behaved, because of the
inverse...</p>
<p>It seems like nobody actually implements regression this way. In R,
<code>lm</code> is done via QR decomposition, anyway.</p>
<p>The pseudoinverse seems to work too, but doesn't always give the same
solution, when multiple solutions are possible. I suspect it isn't
used in practice because SVD is more work than QR. There may be more
reasons.</p>
<hr />
<blockquote>
<p>Recall that an operator on \( V \) is called diagonalizable if the
operator has a diagonal matrix with respect to some basis of \( V
\). Recall also that this happens if and only if there is a basis
of \( V \) consisting of eigenvectors of the operator (see 5.55).
(page 243)</p>
</blockquote>
<hr />
<blockquote>
<p>7.54 eigenvalues of unitary operators have absolute value 1 (page 262)</p>
</blockquote>
<p>Makes me want to see some quantum examples...</p>
<hr />
<blockquote>
<p>8.50 trace of matrix of operator does not depend on basis (page 327)</p>
</blockquote>
<p>How cool is this? Could it come earlier?</p>
<hr />
<blockquote>
<p>Our definition of the determinant leads to the following magical
proof that the determinant is multiplicative. (page 357)</p>
</blockquote>
<p>We're having fun!</p>
<hr />
<blockquote>
<p>Vandermonde matrices have important applications in polynomial
interpolation, the discrete Fourier transform, and other areas of
mathematics. (page 366)</p>
</blockquote>
<p>Makes me want to read an applied book.</p>
<hr />
<blockquote>
<p>I find that in my own elementary lectures, I have, for pedagogical
reasons, pushed determinants more and more into the background. Too
often I have had the experience that, while the students acquired
facility with the formulas, which are so useful in abbreviating long
expressions, they often failed to gain familiarity with their
meaning, and skill in manipulation prevented the student from going
into all the details of the subject and so gaining a mastery.</p>
<p>—Elementary Mathematics from an Advanced Standpoint: Geometry, Felix
Klein (page 369)</p>
</blockquote>
<!-- mathjax for formulas -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML" async></script>
    </article>
    <footer>
      <hr />
<p>If you sign up, I'll send you an email update at most monthly with new stuff.</p>
<iframe src="https://planspace.substack.com/embed" width="100%" frameborder="0" scrolling="no"></iframe>
<p><a id="back_link2" href="/">This site</a> also has <a href="/rss.xml">RSS</a>. You can connect with <a id="aaron_link2" href="/aaron/">me</a> via <a href="https://twitter.com/planarrowspace">Twitter</a>, <a href="https://www.linkedin.com/in/ajschumacher">LinkedIn</a>, <a href="https://github.com/ajschumacher">GitHub</a>, and <a href="mailto:aaron@planspace.org">email</a>.</p>

      <hr />
    </footer>

<!-- my weird stuff -->
<script src="/scripts/planspace.js"></script>

<!-- syntax highlighting -->
<script src="/scripts/highlight.pack.js"></script>
<script type="text/javascript">hljs.initHighlightingOnLoad();</script>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-EZBQMHT77F"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-EZBQMHT77F');
</script>

<!-- Disqus comments -->
<div id="disqus_thread"></div>
<script type="text/javascript">
  var disqus_shortname = 'planspace';
  (function() {
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>

  </body>
</html>
