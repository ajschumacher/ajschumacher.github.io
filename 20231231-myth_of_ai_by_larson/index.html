<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <link rel="stylesheet" type="text/css" href="/styles/csshake.min.css">
    <link rel="stylesheet" type="text/css" href="/styles/zenburn.css">
    <link rel="stylesheet" type="text/css" href="/styles/planspace.css">
    <title>The myth of artificial intelligence, by Larson</title>
  </head>
  <body>
    <article>
<h1>The myth of artificial intelligence, by Larson</h1>
<p class="date">Sunday December 31, 2023</p>
<p>The relevant point in <a href="https://en.wikipedia.org/wiki/Erik_J._Larson" title="Wikipedia: Erik J. Larson">Larson</a>'s <a href="https://www.hup.harvard.edu/books/9780674278660" title="The Myth of Artificial Intelligence: Why Computers Can’t Think the Way We Do">book</a> is that advances in AI
shouldn't undermine human intellectual flourishing. We shouldn't hand
over all our thinking to machines. He explores the example of the
<a href="https://en.wikipedia.org/wiki/Human_Brain_Project" title="Wikipedia: Human Brain Project">Human Brain Project</a>, arguing convincingly that its "big data"
computational focus was not effective. While I think it isn't quite so
black and white (AI can be a useful tool) I also support human
learning, not just machine learning.</p>
<p>The book doesn't deliver on its subtitle, "Why computers can't think
the way we do." It doesn't even really argue for "can't," but only for
"don't, so far." Like <a href="https://en.wikipedia.org/wiki/Gary_Marcus" title="Wikipedia: Gary Marcus">Marcus</a>, Larson is arguing that we need
something more than deep learning and other extant approaches.</p>
<blockquote>
<p>The myth of artificial intelligence is that its arrival is
inevitable, and only a matter of time—that we have already embarked
on the path that will lead to human-level AI, and then
superintelligence. (page 1)</p>
</blockquote>
<p>Larson's main argument is that current machine learning doesn't do
<a href="https://en.wikipedia.org/wiki/Abductive_reasoning" title="Wikipedia: Abductive reasoning">abduction</a>, and it should. This is close to Marcus arguing for
algebraic reasoning. It is even close to me, <a href="/20170429-from_behaviorist_to_constructivist_ai/" title="From Behaviorist to Constructivist AI">arguing</a> for going
from Behaviorist to Constructivist AI.</p>
<p>This book came out before ChatGPT, which is relevant in at least two
ways.</p>
<p>First, it's more possible now to take seriously the idea that neural
generative models might really be, eventually, enough for AGI; this is
roughly what Sutskever seems to <a href="https://www.youtube.com/watch?v=YEUclZdj_Sc" title="Youtube: Why next-token prediction is enough for AGI - Ilya Sutskever (OpenAI Chief Scientist)">say</a>. The human brain is a big mess
that somehow does smart-seeming things. A generative transformer is a
big mess that somehow does smart-seeming things. Maybe they can
achieve equivalent results? This is not a proof, but it might be a
plausibility argument.</p>
<p>Second, Larson spends a good deal of time on examples to show that
models weren't solving some problems very well, when he published.
Many of these examples haven't aged well for his argument, as models
now do better.</p>
<blockquote>
<p>For instance (and ironically), Google Translate as of October 2020
still gets Bar-Hillel's 1960s example wrong. Bar-Hillel asked how to
program a machine to translate <em>The box is in the pen</em> correctly.
Here, <em>pen</em> is ambiguous. (page 202)</p>
</blockquote>
<p>In December 2023, Google Translate still fails.</p>
<p><img alt="Google Translate" src="google_translate.png" /></p>
<p>But ChatGPT <a href="https://chat.openai.com/share/b4dfff5f-413b-466c-af57-c0c4d64e2e7e">succeeds</a>, as far as I can tell.</p>
<p><img alt="ChatGPT translating" src="gpt_translate.png" /></p>
<p>ChatGPT similarly has no problem with challenges like "Can a crocodile
run a steeplechase?" (page 196) (It even titles the <a href="https://chat.openai.com/share/97595ce1-03c3-4c99-bc52-942752615dbb">chat</a>,
"Crocodile Steeplechase: Not Feasible.") And where Larson wrote "Their
[AI's] performance on Winograd schemas is not much better than random
guessing," (page 197) his 2016 reference score of <a href="https://cs.nyu.edu/~davise/papers/WinogradSchemas/WS.html">58%</a> has been
improved as of 2022 to <a href="https://paperswithcode.com/dataset/wsc">90%</a> (or higher).</p>
<p>Whether or not AGI will require explicit architectural support for
abduction (or symbolic reasoning, etc.) the performance limits of
current approaches are not entirely clear.</p>
<p>Really I wish Larson was a better spokesperson for ideas I agree with.
Supporting human learning is a good idea, I think (though Larson
unfortunately takes even this in strange hero-oriented directions).
Larson doesn't really address at all the point that LeCun sometimes
foregrounds, that current AI is hugely data-inefficient (it requires
tons of data) which could actually point to future directions for AI.
I can't endorse this book.</p>
<p><img alt="cover" src="cover.jpg" /></p>
<hr />
<blockquote>
<p>Data science (the application of AI to “big data”) is at best a
prosthetic for human ingenuity, which if used correctly can help us
deal with our modern “data deluge.” (page 4)</p>
</blockquote>
<p>Somewhat routinely, Larson defines things strangely. Data science is
not the application of AI to “big data,” and foundational
misalignments like this make it hard for Larson to land arguments.</p>
<hr />
<blockquote>
<p>The Turing test is actually very difficult—no computer has ever
passed it. (page 10)</p>
</blockquote>
<p>There are plenty of articles claiming that this or that passed the
Turing test, but it seems that
<a href="https://isturingtestpassed.github.io/">isturingtestpassed.github.io</a>
is correct: the answer is still no.</p>
<hr />
<blockquote>
<p>Turing's great genius, and his great error, was in thinking that
human intelligence reduces to problem-solving. (page 23)</p>
</blockquote>
<p>There's a bunch of unclear language here. How is problem-solving
defined? What is excluded from that definition? What even is
intuition?</p>
<hr />
<blockquote>
<p>Turing and colleagues called it the “weight of evidence,” borrowing
a term coined by the American scientist and logician C. S. Peirce
(who is prominently featured in Part Two of this book). (page 24)</p>
</blockquote>
<p>The "it" here has no clear referent, but as it happens I've been
interested in <a href="/20210917-weight_of_evidence_is_logistic_coefficients/" title="Weight of Evidence is logistic coefficients">weight of evidence</a> before, and it seems that this is
the same thing! How neat, to know it traces to Turing. One interesting
work of interpretation is Gillies' short <a href="https://www.jstor.org/stable/688010">paper</a>, "The Turing-Good
Weight of Evidence Function and Popper's Measure of the Severity of a
Test."</p>
<hr />
<blockquote>
<p>Japan had invested millions in its high-profile
<a href="https://en.wikipedia.org/wiki/Fifth_Generation_Computer_Systems" title="Fifth Generation Computer Systems">Fifth Generation project</a> aimed at achieving success in robotics,
and Japan too had failed, rather spectacularly. (page 55)</p>
</blockquote>
<p>At a quick search, Larson seems to be the only person saying the Fifth
Generation project was principally interested in robotics.</p>
<hr />
<blockquote>
<p>Here, it's best to be clear: equating a mind with a computer is not
scientific, it's philosophical. (page 69)</p>
</blockquote>
<hr />
<blockquote>
<p>Dan Gardner's excellent book <em>Future Babble</em> documents the success
rate of predictions in realms from history and geopolitics to the
sciences. He found that theorists—experts with big visions of the
future based on a particular theory they endorse—tend to make worse
predictions than pragmatic people, who see the world as complicated
and lacking a clear fit with any single theory. (page 69)</p>
</blockquote>
<hr />
<blockquote>
<p>Werner Heisenberg discovered his uncertainty principle by working
out the consequences of the new physics of quanta. The principle
states that it is impossible to isolate the position and the
momentum of a subatomic particle simultaneously. This places
fundamental limits on our ability to predict the individual
movements of particles at the subatomic realm (because “seeing” the
position of a particle requires impinging it with a photon, which
also has the effect of knocking it off course). (page 72)</p>
</blockquote>
<p>But it isn't "because" of that; this is a common misconception. Even
<a href="https://en.wikipedia.org/wiki/Uncertainty_principle" title="Uncertainty principle">wikipedia</a> does better:</p>
<blockquote>
<p>Historically, the uncertainty principle has been confused with a
related effect in physics, called the observer effect, which notes
that measurements of certain systems cannot be made without
affecting the system, that is, without changing something in a
system. Heisenberg used such an observer effect at the quantum level
(see below) as a physical "explanation" of quantum uncertainty. It
has since become clearer, however, that the uncertainty principle is
inherent in the properties of all wave-like systems, and that it
arises in quantum mechanics simply due to the matter wave nature of
all quantum objects.</p>
</blockquote>
<hr />
<blockquote>
<p>Statistical AI ends up with a long-tail problem, where common
patterns (in the fat head of a distribution curve) are easy, but
rare ones (in the long tail) are hard. (page 128)</p>
</blockquote>
<p>This is true, and something of a missed opportunity to better tie into
the overall argument.</p>
<hr />
<blockquote>
<p>Calling such systems <em>learners</em> is ironic, because the meaning of
the word <em>learn</em> for humans essentially involves escaping narrow
performances to gain more general understanding of things in the
world. (page 141)</p>
</blockquote>
<p>There's a good point in here somewhere on function approximation vs.
model-building; on interpolation vs. extrapolation.</p>
<hr />
<blockquote>
<p>Big data is an inevitable consequence of Moore's law: as computers
become more powerful, statistical techniques like machine learning
become better, and new business models emerge—all from data and its
analysis. (page 144)</p>
</blockquote>
<p>I pulled this out just as an example of the kind of muddy reasoning
and poor use of language found throughout the book. This is the kind
of sort-of-but-are-you-sure-you-want-to-say-it-that-way output that
I'd expect from a language model.</p>
<hr />
<blockquote>
<p>Much inference in the real world is defeasible, that is, proven
wrong or incomplete by subsequent observation or learning (say, by
reading a book). (page 163)</p>
</blockquote>
<hr />
<blockquote>
<p>Work on so-called <a href="https://en.wikipedia.org/wiki/Non-monotonic_logic" title="Non-monotonic logic">non-monotonic</a> reasoning peaked in the 1980s
and 1990s, but has since been largely abandoned, in large part
because the extensions of deduction to make it more flexible for
language understanding work only on “toy” examples that aren't
useful in the real world. (page 167)</p>
</blockquote>
<hr />
<blockquote>
<p>Many of the same researchers who worked on extending deduction in AI
to make it defeasible also worked out deductive-based approaches to
abduction in the 1980s and 1990s, notably with
<a href="https://en.wikipedia.org/wiki/Abductive_logic_programming" title="Abductive logic programming">abductive logic programming</a> (ALP). (page 168)</p>
</blockquote>
<hr />
<p>Page 172 has a breakdown like this:</p>
<ul>
<li>Deduction: A→B; A ∴ B</li>
<li>Induction: A; B ∴ A→B</li>
<li>Abduction: A→B; B ∴ A</li>
</ul>
<p>Nice to see compactly this way, I think...</p>
<hr />
<blockquote>
<p>The strategy of exposing supervised learning systems to foreseeable
exceptions, as is done with ongoing work on driverless cars, is a
Sisyphean undertaking, because exceptions by their very nature
cannot be completely foreseen. (pages 173-174)</p>
</blockquote>
<p>(See the selection above from page 128.)</p>
<hr />
<blockquote>
<p>a once-prominent field in AI known as
<a href="https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning" title="Knowledge representation and reasoning">knowledge representation and reasoning</a> (KR&amp;R) (page 175)</p>
</blockquote>
<hr />
<blockquote>
<p>technically, entailment is stronger than material implication (page
176)</p>
</blockquote>
<p>Poking around online, there are lots of people making claims about the
difference here, but it doesn't seem like everyone agrees... It seems
roughly like entailment is more causal, while implication can be
correlative.</p>
<hr />
<blockquote>
<p>To abduce we must solve the selection problem among competing causes
or factors, and to solve this problem, we must somehow grasp what is
relevant in some situation or other. The problem is that no one has
a clue how to do this. (page 183)</p>
</blockquote>
<hr />
<blockquote>
<p>noetic, or knowledge-based. (page 185)</p>
</blockquote>
<p>This does not seem to be the sense in which the word is typically used.</p>
<hr />
<blockquote>
<p>What people mean is almost never a literal function of what they
word-for-word say. (page 206)</p>
</blockquote>
<p>Yes... Though the world-knowledge bits of the book are the ones that
most fail in the face of LLMs.</p>
<hr />
<blockquote>
<p><a href="https://en.wikipedia.org/wiki/Cooperative_principle#Grice's_maxims">Grice's Maxims</a> (page 215)</p>
</blockquote>
<p>Quantity, Quality, Relation, Manner</p>
<hr />
<blockquote>
<p><a href="https://en.wikipedia.org/wiki/Text_Retrieval_Conference">Text REtrieval Conferences</a> (page 221)</p>
</blockquote>
<hr />
<blockquote>
<p>Writing about the experience
[of working on early computing machines] in 1887 in an oddly
prescient paper titled "Logical Machines," in the <em>American Journal
of Psychology,</em> Peirce begins characteristically with cautionary
comment. "In the 'Voyage to Laputa' there is a description of a
machine for evolving science automatically," he writes. "The
intention is to ridicule the Organon of Aristotle and the Organon of
Bacon by showing the absurdity of supposing any 'instrument' can do
the work of the mind." Peirce, the skeptic, no doubt appreciated the
wisdom of Swift's imagination. (page 232)</p>
</blockquote>
<p>This is <a href="https://www.gutenberg.org/files/829/829-h/829-h.htm#part03">Gulliver's Travels</a>:</p>
<blockquote>
<p>Every one knew how laborious the usual method is of attaining to
arts and sciences; whereas, by his contrivance, the most ignorant
person, at a reasonable charge, and with a little bodily labour,
might write books in philosophy, poetry, politics, laws,
mathematics, and theology, without the least assistance from genius
or study.</p>
</blockquote>
<p>Is there a better description of ChatGPT than as a contrivance by
which "the most ignorant person, at a reasonable charge, and with a
little bodily labour, might write books in philosophy, poetry,
politics, laws, mathematics, and theology, without the least
assistance from genius or study"?</p>
<blockquote>
<p>He assured me “that this invention had employed all his thoughts
from his youth; that he had emptied the whole vocabulary into his
frame, and made the strictest computation of the general proportion
there is in books between the numbers of particles, nouns, and
verbs, and other parts of speech.”</p>
</blockquote>
<p>A very early statistical NLP! And with a charming illustration:</p>
<p><img alt="the frame" src="theframe.jpg" /></p>
<hr />
<blockquote>
<p>For scientists don't believe in vagaries like the "evolution of
science" except as frosting, as backdrop. They really believe in
scientific genius. They really are all possessed by Prometheus, by
what innovators can dream and achieve. (page 239)</p>
</blockquote>
<p>This is quite the generalization... Maybe you're not a real scientist
if you don't go in for a Great Man theory of science? Maybe the
<a href="https://en.wikipedia.org/wiki/Ortega_hypothesis">Ortega hypothesis</a> is for the weak?</p>
<hr />
<blockquote>
<p>principles linking neurons together into circuits and larger
functional units (mesa circuits) (page 250)</p>
</blockquote>
<p>I don't find a lot of support for "mesa circuits" as a term people
use...</p>
<hr />
<blockquote>
<p>we are actively attempting to cover up a key deficiency—a lack of
flourishing human culture—with rhetoric about the inevitable rise of
machines. (page 277)</p>
</blockquote>
<hr />
<blockquote>
<p>The problem of inference, like the problem of consciousness, is
entrenched at the center of ongoing grand mysteries, and is really
presupposed in our understanding of everything else. We should not
be surprised that the undiscovered mind resists technological
answers. It's possible that, as Horgan worried, we're out of ideas.
If so, the myth represents our final, unrecoverable turn away from
human possibility—a darkly comforting fairy tale, a pretense that
out of our ashes something else, something great and alive, must
surely and inevitably arise.</p>
<p>If we're <em>not</em> out of ideas, then we must do the hard and deliberate
work of reinvesting in a culture of invention and human flourishing.
For we will need our own general intelligence to find paths to the
future, and a future better than the past. (pages 280-281)</p>
</blockquote>
<p>This is interesting material.</p>
<p>I think a lot of "AI isn't as good as people" reduces to how we feel
about consciousness. You can't prove consciousness by behaviors or any
other observable. So do we extend our belief in consciousness to
machines?</p>
<p>On whether we're out of ideas, I like the arguments of <a href="/20220410-beginning_of_infinity_by_deutsch/" title="The Beginning of Infinity, by Deutsch">Deutsch</a> and
<a href="https://en.wikipedia.org/wiki/Frank_Wilczek" title="Wikipedia: Frank Wilczek">Wilczek</a>, which suggest that we're not.</p>    </article>
    <footer>
      <hr />
<p>If you sign up, I'll send you an email update at most monthly with new stuff.</p>
<iframe src="https://planspace.substack.com/embed" width="100%" frameborder="0" scrolling="no"></iframe>
<p><a id="back_link2" href="/">This site</a> also has <a href="/rss.xml">RSS</a>. You can connect with <a id="aaron_link2" href="/aaron/">me</a> via <a href="https://twitter.com/planarrowspace">Twitter</a>, <a href="https://www.linkedin.com/in/ajschumacher">LinkedIn</a>, <a href="https://github.com/ajschumacher">GitHub</a>, and <a href="mailto:aaron@planspace.org">email</a>.</p>

      <hr />
    </footer>

<!-- my weird stuff -->
<script src="/scripts/planspace.js"></script>

<!-- syntax highlighting -->
<script src="/scripts/highlight.pack.js"></script>
<script type="text/javascript">hljs.initHighlightingOnLoad();</script>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-EZBQMHT77F"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-EZBQMHT77F');
</script>

<!-- Disqus comments -->
<div id="disqus_thread"></div>
<script type="text/javascript">
  var disqus_shortname = 'planspace';
  (function() {
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>

  </body>
</html>
