<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <link rel="stylesheet" type="text/css" href="/styles/csshake.min.css">
    <link rel="stylesheet" type="text/css" href="/styles/zenburn.css">
    <link rel="stylesheet" type="text/css" href="/styles/planspace.css">
    <title>Sparse Tensors and TFRecords</title>
  </head>
  <body>
    <article>
<h1>Sparse Tensors and TFRecords</h1>
<p class="date">Thursday April 27, 2017</p>
<p>When a matrix, array, or tensor has lots of values that are zero, it can be called <em>sparse</em>. You might want to represent the zeros implicitly with a <em>sparse representation</em>. TensorFlow has support for this, and the support extends to its TFRecords <code>Example</code> format.</p>
<p>Here is a sparse one-dimensional tensor:</p>
<pre><code class="language-python">[0, 7, 0, 0, 8, 0, 0, 0, 0]</code></pre>

<p>The tensor is sparse, in that it has a lot of zeros, but the representation is dense, in that all those zeros are represented explicitly.</p>
<p>A sparse representation of the same tensor will focus only on the non-zero values.</p>
<pre><code class="language-python">values = [7, 8]</code></pre>

<p>We have to also remember where those values occur, by their indices:</p>
<pre><code class="language-python">indices = [1, 5]</code></pre>

<p>The one-dimensional <code>indices</code> form will work with some methods, for this one-dimensional example, but in general indices have multiple dimensions, so it will be more consistent (and work everywhere) to represent <code>indices</code> like this:</p>
<pre><code class="language-python">indices = [[1], [5]]</code></pre>

<p>With <code>values</code> and <code>indices</code>, we don't have quite enough information yet. How many zeros are there to the right of the last value? We have to represent the dense shape of the tensor.</p>
<pre><code class="language-python">dense_shape = [9]</code></pre>

<p>These three things together, <code>values</code>, <code>indices</code>, and <code>dense_shape</code>, are a sparse representation of the tensor.</p>
<p>TensorFlow accepts lists of values and NumPy arrays to define dense tensors, and it returns NumPy arrays when dense tensors are evaluated. But what to do with sparse tensors? SciPy has <a href="https://docs.scipy.org/doc/scipy/reference/sparse.html">several</a> sparse matrix representations, but not a good match for TensorFlow's general sparse tensor form. So for sparse tensors, instead of reusing an existing Python class, TensorFlow provides <a href="https://www.tensorflow.org/api_docs/python/tf/SparseTensorValue"><code>tf.SparseTensorValue</code></a>. These are values that exist outside the TensorFlow graph, so they can be made without a <code>tf.Session</code>, for example.</p>
<pre><code class="language-python">tf.SparseTensorValue(values=values, indices=indices, dense_shape=dense_shape)
## SparseTensorValue(indices=[[1], [5]], values=[7, 8], dense_shape=[9])</code></pre>

<p>Using <a href="https://www.tensorflow.org/api_docs/python/tf/SparseTensor"><code>tf.SparseTensor</code></a> puts that in the TensorFlow graph.</p>
<pre><code class="language-python">tf.SparseTensor(values=values, indices=indices, dense_shape=dense_shape)
## &lt;tensorflow.python.framework.sparse_tensor.SparseTensor at 0x11a4e0c10></code></pre>

<p>That <code>tf.SparseTensor</code> will be constant, since we specified all the pieces of it, and if you run it in a session, you'll get back the equivalent <code>tf.SparseTensorValue</code>.</p>
<p>TensorFlow has <a href="https://www.tensorflow.org/api_guides/python/sparse_ops">operations</a> specifically for working with sparse tensors, such as <a href="https://www.tensorflow.org/api_docs/python/tf/sparse_matmul"><code>tf.sparse_matmul</code></a>. And you can change a sparse matrix to a dense one with <a href="https://www.tensorflow.org/api_docs/python/tf/sparse_tensor_to_dense"><code>tf.sparse_tensor_to_dense</code></a>. These operations live in the graph, so they have to be run to see a result.</p>
<pre><code class="language-python">sparse = tf.SparseTensor(values=values, indices=indices, dense_shape=dense_shape)
dense = tf.sparse_tensor_to_dense(sparse)
session.run(dense)
## array([0, 7, 0, 0, 0, 8, 0, 0, 0], dtype=int32)</code></pre>

<p>Going from dense to sparse seems a little less <a href="http://stackoverflow.com/questions/39838234/sparse-matrix-from-a-dense-one-tensorflow">straightforward</a> at the moment, so let's continue assuming we already have the components of our sparse representation.</p>
<p>Going to more dimensions is quite natural. Here's a two-dimensional tensor with three non-zero values:</p>
<pre><code class="language-python">[[0, 0, 0, 0, 0, 7],
 [0, 5, 0, 0, 0, 0],
 [0, 0, 0, 0, 9, 0],
 [0, 0, 0, 0, 0, 0]]</code></pre>

<p>This can be represented in sparse form as:</p>
<pre><code class="language-python">indices = [[0, 5],
           [1, 1],
           [2, 4]]

values = [7, 5, 9]

dense_shape = [4, 6]

tf.SparseTensorValue(values=values, indices=indices, dense_shape=dense_shape)
## SparseTensorValue(indices=[[0, 5], [1, 1], [2, 4]], values=[7, 5, 9], dense_shape=[4, 6])</code></pre>

<p>Now, to represent this in a TFRecords <code>Example</code> requires a little bit of transformation. <a href="/20170323-tfrecords_for_humans/">TFRecords</a> only support lists of integers, floats, and bytestrings. The values are easily represented in one <code>Feature</code>, but to represent the <code>indices</code>, each dimension will need its own <code>Feature</code> in the <code>Example</code>. The <code>dense_shape</code> isn't represented at all; that's left to be specified at parsing.</p>
<pre><code class="language-python">my_example = tf.train.Example(features=tf.train.Features(feature={
    'index_0': tf.train.Feature(int64_list=tf.train.Int64List(value=[0, 1, 2])),
    'index_1': tf.train.Feature(int64_list=tf.train.Int64List(value=[5, 1, 4])),
    'values': tf.train.Feature(int64_list=tf.train.Int64List(value=[7, 5, 9]))
}))
my_example_str = my_example.SerializeToString()</code></pre>

<p>This TFRecord sparse representation can then be <a href="/20170426-parsing_tfrecords_inside_the_tensorflow_graph/">parsed inside the graph</a> as a <a href="https://www.tensorflow.org/api_docs/python/tf/SparseFeature"><code>tf.SparseFeature</code></a>.</p>
<pre><code class="language-python">my_example_features = {'sparse': tf.SparseFeature(index_key=['index_0', 'index_1'],
                                                  value_key='values',
                                                  dtype=tf.int64,
                                                  size=[4, 6])}
serialized = tf.placeholder(tf.string)
parsed = tf.parse_single_example(serialized, features=my_example_features)
session.run(parsed, feed_dict={serialized: my_example_str})
## {'sparse': SparseTensorValue(indices=array([[0, 5], [1, 1], [2, 4]]),
##                              values=array([7, 5, 9]),
##                              dense_shape=array([4, 6]))}</code></pre>

<p>Support for multi-dimensional sparse features seems to be new in TensorFlow 1.1, and TensorFlow gives this warning when you use <code>SparseFeature</code>:</p>
<pre><code>WARNING:tensorflow:SparseFeature is a complicated feature config
                   and should only be used after careful consideration
                   of VarLenFeature.</code></pre>

<p><code>VarLenFeature</code> doesn't support real sparsity or multi-dimensionality though; it only supports "ragged edges" as in the case when one example has three elements and the next has seven, for example.</p>
<p>It is a little awkward to put together a sparse representation for TFRecords, but it does give you a lot of flexibility. To put a point on it, I don't know what you can do with a <code>SequenceExample</code> that you can't do with a regular <code>Example</code> using all of <code>FixedLenFeature</code>, <code>VarLenFeature</code>, and <code>SparseFeature</code>.</p>    </article>
    <footer>
      <hr />
      <ul>
        <li id="back_link">
          <a href="/">Plan <span class="rotate180">âž”</span> Space</a>
        </li>
        <li>
          <a id="edit_link" href="https://github.com/ajschumacher/ajschumacher.github.io">Edit</a> this page
        </li>
        <li>
          Find <a id="aaron_link" href="/aaron/">Aaron</a> on
          <ul>
            <li>
              <a href="https://twitter.com/planarrowspace">Twitter</a>
            </li>
            <li>
              <a href="https://www.linkedin.com/in/ajschumacher">LinkedIn</a>
            </li>
            <li>
              <a href="https://plus.google.com/112658546306232777448/">Google+</a>
            </li>
            <li>
              <a href="https://github.com/ajschumacher">GitHub</a>
            </li>
            <li>
              <a href="mailto:ajschumacher@gmail.com">email</a>
            </li>
          </ul>
        </li>
        <li>
          Comment below
        </li>
      </ul>
      <hr />
    </footer>

<!-- my weird stuff -->
<script src="/scripts/planspace.js"></script>

<!-- syntax highlighting -->
<script src="/scripts/highlight.pack.js"></script>
<script type="text/javascript">hljs.initHighlightingOnLoad();</script>

<!-- Google Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-44351009-1', 'auto');
  ga('send', 'pageview');
</script>

<!-- Disqus comments -->
<div id="disqus_thread"></div>
<script type="text/javascript">
  var disqus_shortname = 'planspace';
  (function() {
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>

  </body>
</html>
