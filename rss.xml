<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>plan ➔ space</title>
    <link>http://planspace.org/</link>
    <description>plan space from outer nine</description>
    <language>en-us</language>
    <atom:link href="http://planspace.org/rss.xml" rel="self" type="application/rss+xml" />
<item>
<title>Still Life, by Louise Penny</title>
<description><![CDATA[

<p>Put on your Sony ‘Discman’ and grab your palm pilot, because it's
<a href="https://en.wikipedia.org/wiki/Louise_Penny" title="Wikpedia: Louise Penny">Louise Penny</a>'s first book featuring
<a href="https://en.wikipedia.org/wiki/Chief_Inspector_Armand_Gamache" title="Chief Inspector Armand Gamache">Chief Inspector Armand Gamache</a> (read "Galonche") from 2006!</p>
<blockquote>
<p>‘Life is change. If you aren’t growing and evolving you’re standing
still, and the rest of the world is surging ahead. Most of these
people are very immature. They lead “still” lives, waiting.’ (Myrna,
page 140)</p>
</blockquote>
<p>I would be very pleased if I wrote a book as good as this. It's
charming fun with a bit of morality play blended in.</p>
<blockquote>
<p>‘We choose our thoughts. We choose our perceptions. We choose our
attitudes. We may not think so. We may not believe it, but we do. I
absolutely know we do. I’ve seen enough evidence, time after time,
tragedy after tragedy. Triumph after triumph. It’s about choice.’
(Gamache, page 80)</p>
</blockquote>
<p>Stoic? Positive psychology? Right wing?</p>
<p>There's little sympathy for men's-rights-style whining:</p>
<blockquote>
<p>"We’re not respected just by virtue of being English. It’s not the
same thing. Do you have any idea how much our lives have changed in
the last twenty years? All the rights we’ve lost?" (Ben Hadley, page
50)</p>
</blockquote>
<p>I enjoyed the art world dalliance, which reminded me somewhat of the
end of Vonnegut's Bluebeard. And Nichol! Who's never been Nichol? So
awful! So fun to hate, and/or pity.</p>
<p>I collected things of varying degrees of Canadianness that I looked
up. (Do compound bows have built-in triggers in Montreal?)</p>
<ul>
<li>as hard as the <a href="https://en.wikipedia.org/wiki/Canadian_Shield">Canadian Shield</a>: a large area of exposed
   Precambrian rock</li>
<li><a href="https://carnegiemnh.org/b-is-for-beaver-sticks/">beaver stick</a>: just what it sounds like</li>
<li><a href="https://en.wikipedia.org/wiki/Montreal_Canadiens">Canadiens</a>: the Montreal hockey team</li>
<li>cured Canadian bacon: but they mean regular bacon</li>
<li><em>dépanneur</em>: A convenience store, especially one that sells
   alcohol (in Quebec)</li>
<li><a href="https://en.wikipedia.org/wiki/Eaton%27s_catalogue">Eaton's catalogue</a>: one of the first mail-order catalogs
   distributed by a Canadian retail store</li>
<li>He’s a Golden Mile boy: refers to a rich <a href="https://en.wikipedia.org/wiki/Golden_Square_Mile" title="Golden Square Mile">area</a> of Montreal</li>
<li>Inspector Clouseau: The Chief Inspector from The Pink Panther</li>
<li>Inuk island: apparently Inuk is interchangeable with Inuit? Is this
   a specific place?</li>
<li>"Lee Valley catalogue": "Lee Valley Tools Ltd. is a Canadian
   business specializing in tools and gifts for woodworking and
   gardening."</li>
<li>Lune Moon: a funny Canadian <a href="https://www.flickr.com/photos/hazboy/30114619898" title="Lune Moons">snack cake</a></li>
<li>‘May I have his co-ordinates, please?’ (page 61) was asking for a
   person's location. Is this a common phrasing?</li>
<li>Méteo Media: "a Canadian French-language weather information
   specialty channel and web site"</li>
<li>tabarouette, tabarnouche: "Mild version of french-canadian curse
   word tabarnak."</li>
<li>Tabernacle: This is a curse, somehow?</li>
<li>tête carrée: "square head" / blockhead</li>
<li>three extra large ‘All Dressed’ from Pizza Pizza: "Montreal
   <a href="https://good-pizza-great-pizza.fandom.com/wiki/List_of_orders#All-Dressed">slang</a> for the works"</li>
<li>tins of soft drinks: Is this what they say in Quebec?</li>
<li>toques: normal winter hats, but in Canada (read "tooks")</li>
<li>to twig: To realise something; to catch on; to recognize someone or
   something (British?)</li>
<li>village cozy: A <a href="https://en.wikipedia.org/wiki/Cozy_mystery" title="Wikipedia: Cozy mystery">mystery genre</a> referenced by a character. Still
   Life is pretty close to a village cozy.</li>
<li>‘yes yes’ cookies: Apparently these are like the Girl Scouts'
   "Somoas"</li>
</ul>
<p><img alt="cover" src="cover.jpg"></p>    
    ]]></description>
<link>http://planspace.org/20220212-still_life_by_louise_penny/</link>
<guid>http://planspace.org/20220212-still_life_by_louise_penny/</guid>
<pubDate>Sat, 12 Feb 2022 12:00:00 -0500</pubDate>
</item>
<item>
<title>7 Habits weekly cards</title>
<description><![CDATA[

<p><a href="https://smile.amazon.com/Habits-Highly-Effective-People-Anniversary/dp/1642500267" title="The 7 Habits of Highly Effective People: 30th Anniversary Card Deck">It</a>'s a card a week for a year, classic <a href="https://en.wikipedia.org/wiki/The_7_Habits_of_Highly_Effective_People" title="The 7 Habits of Highly Effective People">7 Habits</a> material, with
reflections and tasks for the week. I did it mostly in 2021, and I
enjoyed it. It inspired me to write a <a href="/20210226-vision_and_mission/" title="Vision and Mission">vision and mission</a>, for
example. More than the content, I like the practice of having a weekly
reflection.</p>
<p>The calendar is somewhat arbitrary, but it can be used for multiple
scales of reflection and planning. I have a <a href="/20211030-my_daily_routine/" title="My daily routine">daily practice</a>, and a
weekly practice is a nice additional layer. Maybe I should think about
a monthly equivalent as well. It's pretty common to use the
<a href="/20211228-habits_for_2022_ratio/" title="Habits for 2022: “ratio”">new year</a> for a yearly reflection and focusing too. I haven't done
five year plans, but could...</p>
<p><img alt="box" src="box.jpg"></p>    
    ]]></description>
<link>http://planspace.org/20220206-7_habits_weekly_cards/</link>
<guid>http://planspace.org/20220206-7_habits_weekly_cards/</guid>
<pubDate>Sun, 06 Feb 2022 12:00:00 -0500</pubDate>
</item>
<item>
<title>System Design Interview, by Xu</title>
<description><![CDATA[

<p>I hadn't done system design interviews before last summer when I
interviewed at Google, Uber, Facebook, Robinhood, and a few others.
Alex Xu's <a href="https://www.amazon.com/System-Design-Interview-Insiders-Guide-ebook/dp/B08B3FWYBX" title="System Design Interview – An Insider's Guide">insider's guide</a> became one of my main prep materials for
this interview type. I think it helped.</p>
<p>I was preparing for system design interviews for machine learning
engineering positions, and sort of related design questions on ML for
data science positions. Xu's book is on more general software
engineering, but I thought it was still interesting and valuable. MLE
interviews can have "regular" SWE system design components too. For
system design content more specific to machine learning, I thought
<a href="https://www.educative.io/">Educative</a> had some decent materials.</p>
<p>I think system design interviews are often trying to evaluate not just
abstract or academic knowledge, but experience, thinking, and
communication skills. Reading a book like this helps jumpstart and
review your knowledge, and is definitely valuable, but is probably not
sufficient for a truly outstanding performance. So you could start
here, but don't stop!</p>
<p><img alt="cover" src="cover.jpg"></p>    
    ]]></description>
<link>http://planspace.org/20220109-system_design_interview_by_xu/</link>
<guid>http://planspace.org/20220109-system_design_interview_by_xu/</guid>
<pubDate>Sun, 09 Jan 2022 12:00:00 -0500</pubDate>
</item>
<item>
<title>Resume dos and don'ts</title>
<description><![CDATA[

<p>I've thought about my resume, I've read <a href="/20220107-tech_resume_inside_out_by_orosz/" title="The Tech Resume Inside Out, by Orosz">a book about resumes</a>, and
I've reviewed hundreds of candidate resumes. Your resume is the first
work product you share with an employer. Make sure it's good. Here's a
collection of resume advice, together with some appalling errors you
wouldn't think people would make—until you see them happen.</p>
<h3>Do this</h3>
<ul>
<li>Consider your audience and what you want them to think about you.</li>
<li>Focus on what you've achieved. Provide <em>evidence</em>, not just claims.<ul>
<li>Use this pattern:
   <a href="https://www.inc.com/bill-murphy-jr/google-recruiters-say-these-5-resume-tips-including-x-y-z-formula-will-improve-your-odds-of-getting-hired-at-google.html" title="Google Recruiters Say Using the 'X-Y-Z Formula' on Your Resume Will Improve Your Odds of Getting Hired at Google">Accomplished X, as measured by Y, by doing Z.</a></li>
</ul>
</li>
<li><a href="/20200715-communicate_immediately/" title="Communicate Immediately">Communicate immediately.</a> Be direct. Be clear.</li>
<li>Use bullet-points, short sentences, rarely anything longer than one
   line.</li>
<li>Make the document look perfect. Format must be PDF.<ul>
<li>Page layout matters. 1.5 pages is worse than 1 or 2.</li>
<li>If at all possible, use LaTeX.</li>
</ul>
</li>
</ul>
<h3>Don't do this</h3>
<ul>
<li>Do not include these elements (as listed by <a href="https://www.cnbc.com/2021/07/20/remove-these-things-from-your-resume-asap-says-ceo-who-has-read-thousands-of-resumes.html" title="Remove these 7 things from your resume ‘ASAP,’ says CEO who has read more than 1,000 resumes this year">Peter Yang</a>):<ul>
<li>Irrelevant hobbies and interests</li>
<li>Too many soft skills</li>
<li>Your professional headshot</li>
<li>Personal pronouns (As in: “Managed a team,” not “I managed a
   team”)</li>
<li>The wrong kind of email (As in: AOL or Hotmail, etc.)</li>
<li>Your mailing address (if you’re applying out-of-state)</li>
<li>Job positions older than 10 to 15 years</li>
</ul>
</li>
<li><a href="https://algodaily.com/lessons/the-perfect-software-engineering-resume-is-like-a-google-search-result/step-eight-8" title="from: Writing The Perfect Software Engineering Resume Is Like a Google Search Result">No Need For a Global Summary (Don't Have a Summary/Objective Section)</a></li>
<li>Include no errors in content and formatting. (See next section for
   examples.)</li>
</ul>
<h3>Resume horror</h3>
<p>Your expertise should be clear from what you've done, not from some
rating scale that you use on yourself. Please no star ratings or “An
asterisk denotes *Expert level” etc.</p>
<p>Don't say “40 Hours/Week” for every job you've worked. Full time
employment is assumed. I'm thinking about resumes for salaried
positions, but even if it were hourly, I don't think I'd want to see
“40 Hours/Week” repeated over and over.</p>
<p>Links and email addresses should not have old-school underlined blue
styling. Control the formatting to make it simple and beautiful.</p>
<p>If you have links in your resume, <em>make sure you know where they go</em>.
I've seen resumes where, presumably because they used somebody else's
resume as a starting point, the link text was updated but the link
still took me to <em>somebody else's</em> LinkedIn and Twitter.</p>
<p>Pay special attention with things you're claiming expertise in: It's
hard to believe you if you say you know “CentOs”—because it's CentOS.</p>
<p>Please get the names of your prior employers correct. There is no “Red
American Cross”—it's “American Red Cross.”</p>
<p>If you are “quick to learn new tools and technics” why have you not
learned how to spell “techniques?”</p>
<p>It's easier to not use color at all than to use it well. Particularly
bad: “Wor” in blue, regular weight, with “k Experience” in bold black.
Why do that?</p>
<p>Almost nobody should include GPA(s) in their resume. Possible
exception: It's perfect, and you've never had a job. Particularly bad:
Including a mediocre GPA for one degree, with GPA then conspicuously
absent for one or more other degrees.</p>
<p>Notice how this actual section from a real resume is
self-contradictory:</p>
<blockquote>
<p>Communication – Persuasive communicator, comfortable challenging the
status quo when appropriate, whether to long-standing processes or
to conventional thinking to drive greater efficiencies and outcomes.
Able to adjust communications to a diverse audience to ensure
understanding and clarity. Recognizes the importance of providing
concise, complete visualizations of complex data.</p>
</blockquote>
<p><a href="https://en.wikipedia.org/wiki/Show,_don't_tell" title="Wikipedia: Show, don't tell">Show, don't tell.</a> Words without evidence are only demonstrating
that you can waste time.</p>
<p>Don't use two spaces between sentences. We're using computers.</p>    
    ]]></description>
<link>http://planspace.org/20220108-resume_dos_and_donts/</link>
<guid>http://planspace.org/20220108-resume_dos_and_donts/</guid>
<pubDate>Sat, 08 Jan 2022 12:00:00 -0500</pubDate>
</item>
<item>
<title>The Tech Resume Inside Out, by Orosz</title>
<description><![CDATA[

<p>I read <a href="https://thetechresume.com/" title="The Tech Resume Inside Out: What a Good Developer Resume Looks Like">this</a> when I was re-doing my resume last spring, and I think
it helped. Here are some changes to my resume that I made based on its
advice:</p>
<ul>
<li>With double the experience, I cut the length in half to get to one
   page.</li>
<li>I dropped my full address, keeping just city and state.</li>
<li>I put job titles before and more prominently than employer names.</li>
<li>Instead of paragraphs describing jobs, 16 bullet-points, all single
   lines.</li>
<li>Lots of active voice, immediate quantitative detail, and specifics.</li>
<li>Instead of a separate section, programming languages etc. mentioned
   in context.</li>
<li>I dropped months, so dates of employment only include years.</li>
</ul>
<p>I didn't make a separate resume for <em>every</em> position I applied for,
but I did distribute at least six different versions for different
kinds of roles.</p>
<p>Do resumes matter? I'm not sure. You probably still need to have one
though. And it may be that the <em>less</em> they matter, the better the
signal you provide by <em>still</em> having a good one. People who are great
do the little things well.</p>
<p>If nothing else, thinking about your resume can focus your work: How
would what I'm doing today look on my resume? If you're doing
something that would seem pointless on a resume, maybe you should do
something else.</p>
<p><img alt="cover" src="cover.png"></p>    
    ]]></description>
<link>http://planspace.org/20220107-tech_resume_inside_out_by_orosz/</link>
<guid>http://planspace.org/20220107-tech_resume_inside_out_by_orosz/</guid>
<pubDate>Fri, 07 Jan 2022 12:00:00 -0500</pubDate>
</item>
<item>
<title>Radium Girls, by Moore</title>
<description><![CDATA[

<p>I like that <a href="https://www.kate-moore.com/" title="Kate Moore">Moore</a> writes with purpose; all her books expose
injustice against and give voice to women (with the possible exception
of the Felix the Railway Cat series; I'm not sure about those).
<a href="https://www.theradiumgirls.com/" title="The Radium Girls">The Radium Girls</a> stays readable over nearly 500 pages, conveying
history that feels relevant to current issues of business, law,
rights, and risks.</p>
<p><strong>Capitalism can be stupid and mean</strong>: Several businesspeople seem
Trump-like in their disregard for ethics in the pursuit of profit.
It's hard to take a libertarian point of view with externalities like
these. And radium dials at least look cool, but many radium businesses
sold absolute snake oil. I noticed today that a company will sell you
<a href="https://www.nytimes.com/2022/01/01/style/gummies-edibles-marijuana-diet.html" title="Counterculture to Counterintuitive: Cannabis to Help You Diet? An edibles company says one of its products will lead not to the “the munchies” but to weight loss. There’s just one problem: the science.">marijuana gummies for weight loss</a>. Maybe not much has changed.</p>
<p><strong>Ignorance and geography</strong>: It was much harder, it seemed, to find a
competent doctor or lawyer in small-town Illinois, compared to the New
York metropolitan area. It seems there was more religiosity in the
middle west as well. It also struck me as strange that terminal
patients were generally not told that they were terminal. Is that
still the case? Is ignorance really better in such cases?</p>
<p><strong>Weaponizing “experts”</strong>: As with radium, so with cigarettes and
opioids and climate change and on and on... The pursuit of knowledge
pushed behind debate-club he-said/she-said and outright lies.</p>
<blockquote>
<p>"But if you looked a little closer at all those positive
publications, there was a common denominator: the researchers, on
the whole, worked for radium firms. As radium was such a rare and
mysterious element, its commercial exploiters in fact controlled, to
an almost monopolizing extent, its image and most of the knowledge
about it. Many firms had their own radium-themed journals, which
were distributed free to doctors, all full of optimistic research.
The firms that profited from radium medicine were the primary
producers and publishers of the positive literature." (page 49)</p>
</blockquote>
<p><strong>Dangers with delayed effects</strong>: Radium took years, usually, to have
negative effects on the radium dial painters. What risks do we
currently not know or underestimate?</p>
<ul>
<li>Climate change? Microplastics?</li>
<li>Social media? Entertainment?</li>
<li>Unemployment? Overemployment?</li>
<li>Migration? Lack of migration?</li>
<li>Trends in education?</li>
<li>Kombucha?</li>
</ul>
<p><strong>Gender and class</strong>: Moore highlights inequities in societal
attention and care. For example, "It seemed wealthy consumers were
much more worthy of protection than working-class girls..." (page 273)
It reminds me of the 21st-century popularization of <a href="https://en.wikipedia.org/wiki/Alice_Neel" title="Wikipedia: Alice Neel">Alice Neel</a>,
who similarly focused on working people and "the female gaze." How far
is there still to go?</p>
<p><strong>The legal system</strong>: The tales of slow and capricious legal
processes, out-of-court settlements, rhetoric over reality, the
influence of wealth, and general nonsense... The 1920s and 30s sound
very much like the present.</p>
<hr>
<p><img alt="cover" src="cover.jpg"></p>    
    ]]></description>
<link>http://planspace.org/20220102-radium_girls_by_moore/</link>
<guid>http://planspace.org/20220102-radium_girls_by_moore/</guid>
<pubDate>Sun, 02 Jan 2022 12:00:00 -0500</pubDate>
</item>
<item>
<title>Try the GSS WORDSUM questions!</title>
<description><![CDATA[

<p>The <a href="https://en.wikipedia.org/wiki/General_Social_Survey" title="Wikipedia: General Social Survey">General Social Survey</a> includes a <a href="https://en.wikipedia.org/wiki/Wordsum" title="Wikipedia: Wordsum">WORDSUM</a> vocabulary test
component that is often used as a proxy measure for <a href="https://en.wikipedia.org/wiki/Intelligence_quotient" title="Wikipedia: Intelligence quotient">IQ</a>. Want to see
how well you can do? Try these 20 questions!</p>
<hr>
<style>
input[type="radio"], label { cursor: pointer; }
input[type="radio"]:checked+label { font-weight: bold; }
.marked_correct { font-style: italic }
</style>

<table width="100%"><tr><td width="60%">

<p>For each initial word, find the other word which means the same or
most nearly the same.</p>

<p>In the example at right, “animal” is selected because it has the
closest meaning to “beast.”</p>

</td><td style="padding-left: 10%;">

<b>beast</b>
<br>
<input type="radio" name="ex" id="ex_1" disabled>
<label for="ex_1">afraid</label>
<br>
<input type="radio" name="ex" id="ex_2" disabled>
<label for="ex_2">words</label>
<br>
<input type="radio" name="ex" id="ex_3" disabled>
<label for="ex_3">large</label>
<br>
<input type="radio" name="ex" id="ex_4" checked>
<label for="ex_4">animal</label>
<br>
<input type="radio" name="ex" id="ex_5" disabled>
<label for="ex_5">separate</label>

</td></tr></table>

<hr>
<p><b>space</b>
<br>
<input type="radio" name="01" id="01_1">
<label for="01_1">school</label>
<br>
<input type="radio" name="01" id="01_2">
<label for="01_2">noon</label>
<br>
<input type="radio" name="01" id="01_3">
<label for="01_3">captain</label>
<br>
<input type="radio" name="01" id="01_4" class="correct">
<label for="01_4" class="correct_label">room</label>
<br>
<input type="radio" name="01" id="01_5">
<label for="01_5">board</label></p>
<hr>
<p><b>lift</b>
<br>
<input type="radio" name="02" id="02_1">
<label for="02_1">sort out</label>
<br>
<input type="radio" name="02" id="02_2" class="correct">
<label for="02_2" class="correct_label">raise</label>
<br>
<input type="radio" name="02" id="02_3">
<label for="02_3">value</label>
<br>
<input type="radio" name="02" id="02_4">
<label for="02_4">enjoy</label>
<br>
<input type="radio" name="02" id="02_5">
<label for="02_5">fancy</label></p>
<hr>
<p><b>concern</b>
<br>
<input type="radio" name="03" id="03_1">
<label for="03_1">see clearly</label>
<br>
<input type="radio" name="03" id="03_2">
<label for="03_2">engage</label>
<br>
<input type="radio" name="03" id="03_3">
<label for="03_3">furnish</label>
<br>
<input type="radio" name="03" id="03_4">
<label for="03_4">disturb</label>
<br>
<input type="radio" name="03" id="03_5" class="correct">
<label for="03_5" class="correct_label">have to do with</label></p>
<hr>
<p><b>broaden</b>
<br>
<input type="radio" name="04" id="04_1">
<label for="04_1">efface</label>
<br>
<input type="radio" name="04" id="04_2">
<label for="04_2">make level</label>
<br>
<input type="radio" name="04" id="04_3">
<label for="04_3">elapse</label>
<br>
<input type="radio" name="04" id="04_4">
<label for="04_4">embroider</label>
<br>
<input type="radio" name="04" id="04_5" class="correct">
<label for="04_5" class="correct_label">widen</label></p>
<hr>
<p><b>blunt</b>
<br>
<input type="radio" name="05" id="05_1" class="correct">
<label for="05_1" class="correct_label">dull</label>
<br>
<input type="radio" name="05" id="05_2">
<label for="05_2">drowsy</label>
<br>
<input type="radio" name="05" id="05_3">
<label for="05_3">deaf</label>
<br>
<input type="radio" name="05" id="05_4">
<label for="05_4">doubtful</label>
<br>
<input type="radio" name="05" id="05_5">
<label for="05_5">ugly</label></p>
<hr>
<p><b>accustom</b>
<br>
<input type="radio" name="06" id="06_1">
<label for="06_1">disappoint</label>
<br>
<input type="radio" name="06" id="06_2">
<label for="06_2">customary</label>
<br>
<input type="radio" name="06" id="06_3">
<label for="06_3">encounter</label>
<br>
<input type="radio" name="06" id="06_4" class="correct">
<label for="06_4" class="correct_label">get used</label>
<br>
<input type="radio" name="06" id="06_5">
<label for="06_5">business</label></p>
<hr>
<p><b>chirrup</b>
<br>
<input type="radio" name="07" id="07_1">
<label for="07_1">aspen</label>
<br>
<input type="radio" name="07" id="07_2">
<label for="07_2">joyful</label>
<br>
<input type="radio" name="07" id="07_3">
<label for="07_3">capsize</label>
<br>
<input type="radio" name="07" id="07_4" class="correct">
<label for="07_4" class="correct_label">chirp</label>
<br>
<input type="radio" name="07" id="07_5">
<label for="07_5">incite</label></p>
<hr>
<p><b>edible</b>
<br>
<input type="radio" name="08" id="08_1">
<label for="08_1">auspicious</label>
<br>
<input type="radio" name="08" id="08_2">
<label for="08_2">eligible</label>
<br>
<input type="radio" name="08" id="08_3" class="correct">
<label for="08_3" class="correct_label">fit to eat</label>
<br>
<input type="radio" name="08" id="08_4">
<label for="08_4">sagacious</label>
<br>
<input type="radio" name="08" id="08_5">
<label for="08_5">able to speak</label></p>
<hr>
<p><b>pact</b>
<br>
<input type="radio" name="09" id="09_1">
<label for="09_1">puissance</label>
<br>
<input type="radio" name="09" id="09_2">
<label for="09_2">remonstrance</label>
<br>
<input type="radio" name="09" id="09_3" class="correct">
<label for="09_3" class="correct_label">agreement</label>
<br>
<input type="radio" name="09" id="09_4">
<label for="09_4">skillet</label>
<br>
<input type="radio" name="09" id="09_5">
<label for="09_5">pressure</label></p>
<hr>
<p><b>solicitor</b>
<br>
<input type="radio" name="10" id="10_1" class="correct">
<label for="10_1" class="correct_label">lawyer</label>
<br>
<input type="radio" name="10" id="10_2">
<label for="10_2">chieftain</label>
<br>
<input type="radio" name="10" id="10_3">
<label for="10_3">watchman</label>
<br>
<input type="radio" name="10" id="10_4">
<label for="10_4">maggot</label>
<br>
<input type="radio" name="10" id="10_5">
<label for="10_5">constable</label></p>
<hr>
<p><b>allusion</b>
<br>
<input type="radio" name="11" id="11_1">
<label for="11_1">aria</label>
<br>
<input type="radio" name="11" id="11_2">
<label for="11_2">illusion</label>
<br>
<input type="radio" name="11" id="11_3">
<label for="11_3">eulogy</label>
<br>
<input type="radio" name="11" id="11_4">
<label for="11_4">dream</label>
<br>
<input type="radio" name="11" id="11_5" class="correct">
<label for="11_5" class="correct_label">reference</label></p>
<hr>
<p><b>caprice</b>
<br>
<input type="radio" name="12" id="12_1">
<label for="12_1">value</label>
<br>
<input type="radio" name="12" id="12_2">
<label for="12_2">a star</label>
<br>
<input type="radio" name="12" id="12_3">
<label for="12_3">grimace</label>
<br>
<input type="radio" name="12" id="12_4" class="correct">
<label for="12_4" class="correct_label">whim</label>
<br>
<input type="radio" name="12" id="12_5">
<label for="12_5">inducement</label></p>
<hr>
<p><b>animosity</b>
<br>
<input type="radio" name="13" id="13_1" class="correct">
<label for="13_1" class="correct_label">hatred</label>
<br>
<input type="radio" name="13" id="13_2">
<label for="13_2">animation</label>
<br>
<input type="radio" name="13" id="13_3">
<label for="13_3">disobedience</label>
<br>
<input type="radio" name="13" id="13_4">
<label for="13_4">diversity</label>
<br>
<input type="radio" name="13" id="13_5">
<label for="13_5">friendship</label></p>
<hr>
<p><b>emanate</b>
<br>
<input type="radio" name="14" id="14_1">
<label for="14_1">populate</label>
<br>
<input type="radio" name="14" id="14_2">
<label for="14_2">free</label>
<br>
<input type="radio" name="14" id="14_3">
<label for="14_3">prominent</label>
<br>
<input type="radio" name="14" id="14_4">
<label for="14_4">rival</label>
<br>
<input type="radio" name="14" id="14_5" class="correct">
<label for="14_5" class="correct_label">come</label></p>
<hr>
<p><b>madrigal</b>
<br>
<input type="radio" name="15" id="15_1" class="correct">
<label for="15_1" class="correct_label">song</label>
<br>
<input type="radio" name="15" id="15_2">
<label for="15_2">mountebank</label>
<br>
<input type="radio" name="15" id="15_3">
<label for="15_3">lunatic</label>
<br>
<input type="radio" name="15" id="15_4">
<label for="15_4">ribald</label>
<br>
<input type="radio" name="15" id="15_5">
<label for="15_5">sycophant</label></p>
<hr>
<p><b>cloistered</b>
<br>
<input type="radio" name="16" id="16_1">
<label for="16_1">miniature</label>
<br>
<input type="radio" name="16" id="16_2">
<label for="16_2">bunched</label>
<br>
<input type="radio" name="16" id="16_3">
<label for="16_3">arched</label>
<br>
<input type="radio" name="16" id="16_4">
<label for="16_4">malady</label>
<br>
<input type="radio" name="16" id="16_5" class="correct">
<label for="16_5" class="correct_label">secluded</label></p>
<hr>
<p><b>encomium</b>
<br>
<input type="radio" name="17" id="17_1">
<label for="17_1">repetition</label>
<br>
<input type="radio" name="17" id="17_2">
<label for="17_2">friend</label>
<br>
<input type="radio" name="17" id="17_3" class="correct">
<label for="17_3" class="correct_label">panegyric</label>
<br>
<input type="radio" name="17" id="17_4">
<label for="17_4">abrasion</label>
<br>
<input type="radio" name="17" id="17_5">
<label for="17_5">expulsion</label></p>
<hr>
<p><b>pristine</b>
<br>
<input type="radio" name="18" id="18_1">
<label for="18_1">flashing</label>
<br>
<input type="radio" name="18" id="18_2">
<label for="18_2">earlier</label>
<br>
<input type="radio" name="18" id="18_3" class="correct">
<label for="18_3" class="correct_label">primeval</label>
<br>
<input type="radio" name="18" id="18_4">
<label for="18_4">bound</label>
<br>
<input type="radio" name="18" id="18_5">
<label for="18_5">green</label></p>
<hr>
<p><b>tactility</b>
<br>
<input type="radio" name="19" id="19_1" class="correct">
<label for="19_1" class="correct_label">tangibility</label>
<br>
<input type="radio" name="19" id="19_2">
<label for="19_2">grace</label>
<br>
<input type="radio" name="19" id="19_3">
<label for="19_3">subtlety</label>
<br>
<input type="radio" name="19" id="19_4">
<label for="19_4">extensibility</label>
<br>
<input type="radio" name="19" id="19_5">
<label for="19_5">manageableness</label></p>
<hr>
<p><b>sedulous</b>
<br>
<input type="radio" name="20" id="20_1">
<label for="20_1">muddied</label>
<br>
<input type="radio" name="20" id="20_2">
<label for="20_2">sluggish</label>
<br>
<input type="radio" name="20" id="20_3">
<label for="20_3">stupid</label>
<br>
<input type="radio" name="20" id="20_4" class="correct">
<label for="20_4" class="correct_label">assiduous</label>
<br>
<input type="radio" name="20" id="20_5">
<label for="20_5">corrupting</label></p>
<hr>
<div id="show_scoring">

<input type="submit" value="Show scoring and italicize correct answers" onclick="
    document.getElementById('scoring').style.visibility = 'visible';
    document.getElementById('show_scoring').style.visibility = 'hidden';
    document.querySelectorAll('.correct_label').forEach(
      element =&gt; element.classList.add('marked_correct'))">

</div>

<div id="scoring" style="visibility: hidden;">

<h3><span id="wordsum">0</span> of 10 points (<span id="count">0</span> correct of 20 questions)</h3>

<input type="submit" value="Hide scoring and un-italicize correct answers" onclick="
    document.getElementById('scoring').style.visibility = 'hidden';
    document.getElementById('show_scoring').style.visibility = 'visible';
    document.querySelectorAll('.correct_label').forEach(
      element =&gt; element.classList.remove('marked_correct'))">

</div>

<hr>
<p>Caveats:</p>
<ul>
<li>I'm pretty sure I've identified the right answers, but I haven't
   seen an answer key (just the relevant test booklets).</li>
<li>The actual <a href="https://en.wikipedia.org/wiki/Wordsum" title="Wikipedia: Wordsum">WORDSUM</a> has 10 questions, not 20. I know that those
   ten questions are among these 20, but I don't know <em>which</em>. For
   more detail, read on.</li>
</ul>
<p>Here's an <a href="https://gss.norc.org/Documents/reports/methodological-reports/MR111%20The%20Psychometric%20Properties%20of%20the%20GSS%20Wordsum%20Vocabulary%20Test.pdf" title="The Psychometric Properties of the GSS Wordsum Vocabulary Test">excerpt</a> on the history of <a href="https://en.wikipedia.org/wiki/Wordsum" title="Wikipedia: Wordsum">WORDSUM</a>:</p>
<blockquote>
<p>In the early 1920s, Edward L. Thorndike developed a lengthy
vocabulary test as part of the I.E.R. Intelligence Scale CAVD to
measure, in his words, “verbal intelligence.” As in the modern-day
Wordsum test, each question asked respondents to identify the word
or phrase in a set of five whose meaning was closest to a target
word. Robert L. Thorndike (1942) later extracted two subsets of the
original test, each containing twenty items of varying difficulty.
For each subset, two target words were selected at each of ten
difficulty levels. The ten items in Wordsum (labeled with letters
A though J) were selected from the first of these two subsets.</p>
</blockquote>
<p>It's not perfectly accurate to say Thorndike developed a lengthy
vocabulary test. The I.E.R Intelligence Scale CAVD, <a href="https://copyright.cornell.edu/publicdomain" title="Copyright at Cornell Libraries: Copyright Term and the Public Domain">copyright</a> 1925
and 1926, is 17 levels (A through Q) and each level has ten vocabulary
questions in multiple choice format, among other question types.</p>
<p>Thorndike 1942 is <a href="https://psycnet.apa.org/record/1942-04205-001" title="Thorndike, R. L. (1942). Two screening tests of verbal intelligence. Journal of Applied Psychology, 26(2), 128–135.">Two screening tests of verbal intelligence</a>,
which describes how two 20-word tests “containing two words from each
of the levels of CAVD from H through Q” (the more difficult end of the
spectrum; levels A through E use pictures) were constructed.</p>
<p><img alt="two forms" src="two_forms.png"></p>
<p>The first subset then is <em>Form A</em>, and those are the questions
included above. I don't know <em>which</em> ten words from Form A's twenty
are in WORDSUM, however. As <a href="https://gssdataexplorer.norc.org/documents/444/display" title="GSS Codebook Appendix D">GSS Codebook Appendix D</a> explains:</p>
<blockquote>
<p>To minimize the admittedly small possibility that some form of
publicity would affect the public's knowledge of the words included
in the test, they are not reported here.</p>
</blockquote>
<p>I think it's fascinating that a vocab test from the 1920s is part of
the GSS, which continues to be administered in the 2020s. I think it's
fascinating that WORDSUM has been used for evidence that
<a href="https://www.semanticscholar.org/paper/Intelligence-makes-people-think-like-economists%3A-Caplan-Miller/ef579356c5c461914e3c0ff896c7949554be1774" title="Intelligence makes people think like economists: Evidence from the General Social Survey">Intelligence makes people think like economists</a>, among many other
things.</p>
<p>I think the benefit of knowing what the WORDSUM questions are far
outweighs any risk “that some form of publicity would affect the
public's knowledge of the words included in the test.” Frankly, I'm
not sure WORDSUM deserves an encomium.</p>

    ]]></description>
<link>http://planspace.org/20220101-try_the_gss_wordsum_questions/</link>
<guid>http://planspace.org/20220101-try_the_gss_wordsum_questions/</guid>
<pubDate>Sat, 01 Jan 2022 12:00:00 -0500</pubDate>
</item>
<item>
<title>Predicting the future from 2022</title>
<description><![CDATA[

<p>For the third year in a row (<a href="/20200112-predicting_the_future_from_2020/" title="Predicting the Future from 2020">2020</a>, <a href="/20210102-predicting_the_future_from_2021/" title="Predicting the future from 2021">2021</a>) I consider what might
happen. I continue to mostly imagine current trends continue, and
throw in some extra stuff for fun.</p>
<h3>By 2023 (1 year from now)</h3>
<ul>
<li>Omicron is the last major Covid variant, but supply chain and
   workforce issues continue to be hard all year.</li>
<li>Democrats become minorities in the House and Senate in 2022 US elections.</li>
<li><a href="https://en.wikipedia.org/wiki/Elizabeth_II" title="Wikipedia: Elizabeth II">Queen Elizabeth</a> is dead. (Recommended by Tonia.)</li>
<li>Inflation carries on around 5%.</li>
<li>S&amp;P 500 closes above 5,000.</li>
</ul>
<h3>By 2027 (5 years from now)</h3>
<ul>
<li>Democratic candidate wins presidency in 2024 US election.</li>
<li>Inflation around 2%.</li>
<li>Global population exceeds 8 billion for the first time in 2023
   (<a href="https://population.un.org/wpp/Download/Standard/Population/" title="Total Population - Both Sexes (XLSX, 2.4 MB)">predicted</a> by the UN).</li>
<li>Total population between 8,221,264 and 8,333,078 (low/med
   <a href="https://population.un.org/wpp/Download/Standard/Population/" title="Total Population - Both Sexes (XLSX, 2.4 MB)">predicted</a> by the UN).</li>
<li>S&amp;P 500 closes above 7,000.</li>
</ul>
<h3>By 2032 (10 years from now)</h3>
<ul>
<li>Texas has non-zero state income tax and plenty of climate and
   infrastructure things to spend on.</li>
<li><a href="https://en.wikipedia.org/wiki/Zendaya" title="Wikipedia: Zendaya">Zendaya</a> wins an Oscar.</li>
<li>At least one European nation has something identifiable as <a href="https://en.wikipedia.org/wiki/Universal_basic_income" title="Wikipedia: Universal Basic Income">UBI</a>.</li>
<li>Total population between 8,445,932 and 8,687,228 (low/med
   <a href="https://population.un.org/wpp/Download/Standard/Population/" title="Total Population - Both Sexes (XLSX, 2.4 MB)">predicted</a> by the UN).</li>
<li>S&amp;P 500 closes above 9,000.</li>
</ul>
<h3>By 2042 (20 years from now)</h3>
<ul>
<li>The US effectively has <a href="https://en.wikipedia.org/wiki/Universal_basic_income" title="Wikipedia: Universal Basic Income">UBI</a>, but it isn't called that. Maybe a
   patchwork of policies.</li>
<li>At least one international advertising campaign intended to
   encourage immigration to the US by ameliorating the reputation of
   unemployed Americans.</li>
<li>Total population between 8,769,695 and 9,315,508 (low/med
   <a href="https://population.un.org/wpp/Download/Standard/Population/" title="Total Population - Both Sexes (XLSX, 2.4 MB)">predicted</a> by the UN).</li>
<li>S&amp;P 500 closes above 15,000.</li>
</ul>
<hr>
<h3>Trends</h3>
<ul>
<li>I think global population growth will be <em>slightly</em> slower than the
   medium variant <a href="https://population.un.org/wpp/Download/Standard/Population/" title="Total Population - Both Sexes (XLSX, 2.4 MB)">predicted</a> by the UN. Covid might contribute a
   little, but I think this is mostly from efforts like Bill Gates's,
   possibly Belt and Road, and general technology and modernization.</li>
<li>Climate change continues. More fires, more floods, more droughts,
   more storms. Climate action continues to be nominal.</li>
<li>It keeps being easy/easier to fake things.<ul>
<li>Lots of cheap/low-quality stuff.</li>
<li>The value of direct human relationships/trust stays very
   high/grows. Elite college education keeps getting more
   valuable.</li>
</ul>
</li>
<li>Inequality continues to grow.<ul>
<li>Extreme poverty continues to decline; the bottom of the range
   is higher than previously. In some places, there may be <a href="https://en.wikipedia.org/wiki/Universal_basic_income" title="Wikipedia: Universal Basic Income">UBI</a>
   eventually.</li>
<li>But the gap between the bottom of the range and the top grows.</li>
</ul>
</li>
<li>More legalization of drugs.</li>
<li>There will continue to be more real problems with not having enough
   people to work than there are with having too many people put out
   of work by technology.</li>
</ul>
<hr>
<h3>Wilder speculations</h3>
<ul>
<li>“The middle class is precarious.” To what extent is <em>downward</em>
   mobility present? acceptable? desirable? To what extent is/should
   mobility be like a ratchet: only upward? The idle rich are not
   generally admired as virtuous, I think, but people also really
   dislike it when their children are worse off than they are/were (or
   if they're those children). What will be the long-term trend? What
   level of wealth should anyone have to “earn”? (Inspired by Emilie.)</li>
<li>Infant (and child) mortality is dramatically <a href="/20200806-life_expectancy_is_historically_misleading/" title="Life expectancy is historically misleading">down</a> and will
   likely continue to get lower. The first-order effect is obviously
   good. What are the second-order effects? Does the decrease in
   infant mortality contribute to any population health effects?</li>
</ul>
<hr>
<h3>Reviewing past predictions</h3>
<p>I was a little crazy in <a href="/20210102-predicting_the_future_from_2021/" title="Predicting the future from 2021">2021</a>. Depending on the level of generosity
in interpretation, I was somewhere from 1/4 to 4/4 on one-year
predictions. None of my longer-term predictions have materialized.
Still no aliens.</p>
<p>From <a href="/20200112-predicting_the_future_from_2020/" title="Predicting the Future from 2020">2020</a>: Hong Kong <a href="https://www.ksl.com/article/50319384/how-democracy-was-dismantled-in-hong-kong-in-2021" title="How democracy was dismantled in Hong Kong in 2021">seems to be</a> pretty well controlled by the
mainland now.</p>    
    ]]></description>
<link>http://planspace.org/20220101-predicting_the_future_from_2022/</link>
<guid>http://planspace.org/20220101-predicting_the_future_from_2022/</guid>
<pubDate>Sat, 01 Jan 2022 12:00:00 -0500</pubDate>
</item>
<item>
<title>A Calendar of Wisdom, by Tolstoy</title>
<description><![CDATA[

<p>In 2021, I followed <a href="https://en.wikipedia.org/wiki/Leo_Tolstoy" title="Wikipedia: Leo Tolstoy">Tolstoy</a>'s <a href="https://en.wikipedia.org/wiki/A_Calendar_of_Wisdom" title="A Calendar of Wisdom">Calendar of Wisdom</a>. It's a
page-a-day calendar, circa 1910, with a little more gravitas. I like
the concept but I don't always agree with Tolstoy.</p>
<blockquote>
<p>"Guiding your thoughts is one of the keys to self-perfection."
(August 9)</p>
</blockquote>
<p>Tolstoy is quite religious, pacifist, vegetarian, and
back-to-the-earth. He mixes quotes and paraphrases with his own
commentary and aphorisms. I can't quite recommend it all because I
find some things objectionable, but I like the idea of having
something to reflect on daily. Some of my most and least favorite
selections are below.</p>
<p><img alt="cover" src="cover.jpg"></p>
<hr>
<h3>Not bad, Tolstoy</h3>
<blockquote>
<p>"To accept the dignity of another person is an axiom." (April 16)</p>
<p>"<em>Effort is the necessary condition of moral perfection.</em>" (July 23)</p>
<p>"When a person tries to apply his intellect to the question “Why do
I exist in this world?” he becomes dizzy. The human intellect cannot
find the answers to such questions." (July 29)</p>
<p>"Think good thoughts, and your thoughts will be turned into good
actions. Everything begins in thought. Guiding your thoughts is one
of the keys to self-perfection." (August 9)</p>
<p>"<em>Real goodness is not something that can be acquired in an instant,
but only through constant effort, because real goodness lies in
constantly striving for perfection.</em>" (September 4)</p>
<p>"You should abstain from arguments. They are very illogical ways to
convince people. Opinions are like nails: the stronger you hit them,
the deeper inside they go." (November 4, quoting Decimus Junius
Juvenalis)</p>
<p>"The more urgently you want to speak, the more likely it is that you
will say something foolish." (November 4)</p>
</blockquote>
<hr>
<h3>No thanks, Tolstoy</h3>
<blockquote>
<p>"<em>A marriage is a special obligation between two people, of opposite
sexes, to have children only with each other. To break this pact is
a lie, a deception, and a crime.</em>" (March 11)</p>
<p>"We can improve this world only by distributing the true faith among
the world's people." (March 17)</p>
<p>"You should never feel depressed.</p>
<p>A man should always feel happy; if he is unhappy, it means he is
guilty." (June 29)</p>
<p>"People know little, because they try to understand those things
which are not open to them for understanding: God, eternity, spirit;
or those which are not worth thinking about: how water becomes
frozen, or a new theory of numbers, or how viruses can transmit
illnesses." (July 27)</p>
<p>"Only religion destroys egoism and selfishness, so that one starts
to live life not only for himself. Only religion destroys the fear
of death, only religion gives us the meaning of life, only religion
creates equality among people, only religion sets a person free from
outer pressures." (August 18)</p>
<p>"It is dangerous to disseminate the idea that our life is purely the
product of material forces and that it depends entirely on these
forces." (August 22)</p>
<p>"<em>Faith is the foundation on which all else rests; it is the root of
all knowledge.</em>" (August 28)</p>
<p>"Though the mission of a woman's life is the same as that of a man's
life and the service to God is fulfilled by the same means, namely
love, for the majority of women the method of this service is more
specific than for men. This is the birth and upbringing of new
workers for the Lord throughout life." (December 1)</p>
<p>"There is nothing more natural for a woman than self-sacrifice."
(December 1)</p>
</blockquote>    
    ]]></description>
<link>http://planspace.org/20211231-calendar_of_wisdom_by_tolstoy/</link>
<guid>http://planspace.org/20211231-calendar_of_wisdom_by_tolstoy/</guid>
<pubDate>Fri, 31 Dec 2021 12:00:00 -0500</pubDate>
</item>
<item>
<title>Getting var(X/Y) can be hard</title>
<description><![CDATA[

<p>There generally isn't an analytic solution for the variance of the
ratio of two random variables (or probability distributions).
Denominators around zero are a problem, and with variance large
relative to mean, the approximation using the delta method may be
worse than you expect.</p>
<p>If you're dividing by something that can be close to zero, the results
can get big, which affects variance. Dividing two standard normal
distributions gives you a <a href="https://en.wikipedia.org/wiki/Cauchy_distribution" title="Wikipediia: Cauchy distribution">Cauchy distribution</a>, for example, and
the variance there is undefined. So if you're near zero, watch out!
Variance may not make sense, and can at least be hard to estimate.</p>
<p>The approximation of the variance of a ratio using the
<a href="https://en.wikipedia.org/wiki/Delta_method" title="Delta method">delta method</a> is:</p>
<p>\[ \text{var} \left( \frac{X}{Y} \right) \approx \frac{1}{\overline{Y}^2} \text{var} \left( X \right) + \frac{ \overline{X}^2 }{ \overline{Y}^4 } \text{var} \left( Y \right) - 2 \frac{ \overline{X} }{ \overline{Y}^3 } \text{cov} \left( X, Y \right) \]</p>
<p>See <a href="https://www.stat.cmu.edu/~hseltman/files/ratio.pdf" title="Approximations for Mean and Variance of a Ratio">Seltman</a> for a derivation. Also presented in <a href="/20211229-trustworthy_online_controlled_experiments/" title="Trustworthy Online Controlled Experiments">Kohavi et al.</a>
and <a href="http://alexdeng.github.io/public/files/WSDM2017draft.pdf" title="Trustworthy analysis of online A/B tests: Pitfalls, challenges and solutions">Deng et al.</a></p>
<p>This is an approximation based on <a href="https://en.wikipedia.org/wiki/Taylor%27s_theorem" title="Wikipedia: Taylor's theorem">Taylor series expansion</a>, and it
isn't always super close, even for seemingly simple examples like this
one in <a href="https://www.r-project.org/" title="The R Project for Statistical Computing">R</a>, with uniform distributions:</p>
<!-- set.seed(42) -->

<pre><code class="language-r">approx_var_ratio &lt;- function(x, y) {
  (1 / mean(y)^2) * var(x) +
    (mean(x)^2 / mean(y)^4) * var(y) -
    2 * (mean(x) / mean(y)^3) * cov(x, y)
}

x = runif(1000, min=14, max=26)
y = runif(1000, min= 4, max=16)

approx_var_ratio(x, y)
## 0.6286227
var(x/y)
## 1.189362</code></pre>

<p>That's no fluke of sampling. The delta-based formula is consistently
under 60% of the empirical value, for these parameters.</p>
<p>It isn't just a weirdness of the uniform distribution either. The
approximation is agnostic to distribution—which is another hint that
it can't be perfectly right, at least not always. Here's an example
with Gaussians, truncated so there's no risk of outliers near zero:</p>
<!--
install.packages("truncnorm")
library(truncnorm)

set.seed(42)
-->

<pre><code class="language-r">x = rtruncnorm(1000, a=14, mean=20, b=26, sd=3)
y = rtruncnorm(1000, a= 4, mean=10, b=16, sd=3)

approx_var_ratio(x, y)
## 0.3323209
var(x/y)
## 0.5294384</code></pre>

<p>That's still a substantial underestimate.</p>
<p>The approximation gets better when means are bigger relative to
variances. For example:</p>
<!-- set.seed(42) -->

<pre><code class="language-r">x = rnorm(1000, mean=200, sd=3)
y = rnorm(1000, mean=100, sd=3)

approx_var_ratio(x, y)
## 0.004369166
var(x/y)
## 0.004381438</code></pre>

<p>Now it's quite close.</p>
<p>A distribution with appreciable mass near zero has a large variance
relative to its mean, but you don't have to be near zero for the
relative spread of the distribution to affect the quality of the
approximation. Especially clear of zero, I think Monte Carlo
estimation can be better than using the formula based on the delta
method.</p>
<p>In conclusion, approximations are approximations. Do you really <em>need</em>
a ratio?</p>
<p>Stay safe out there!</p>
<!-- mathjax for formulas -->

    ]]></description>
<link>http://planspace.org/20211230-getting_var_ratio_can_be_hard/</link>
<guid>http://planspace.org/20211230-getting_var_ratio_can_be_hard/</guid>
<pubDate>Thu, 30 Dec 2021 12:00:00 -0500</pubDate>
</item>
<item>
<title>Trustworthy Online Controlled Experiments</title>
<description><![CDATA[

<p>The <a href="https://exp-platform.com/hippo/" title="HiPPO FAQ">HiPPO</a> on the cover represents the “Highest Paid Person's
Opinion” and the <a href="https://experimentguide.com/" title="Trustworthy Online Controlled Experiments: A Practical Guide to A/B Testing">book</a> is about making decisions in industry based
on experiments instead. It includes <a href="https://en.wikipedia.org/wiki/Twyman%27s_law">Twyman's Law</a> and much more and
is <em>the</em> text to read for online A/B tests. It does have some of the
character of class notes from a busy prof, with details left in other
papers, and the occasional misattribution (quote page 153) and error
(“due to the increase in power that comes from the increase in power”
page 170). Indispensable, but some assembly required.</p>
<p><img alt="cover" src="cover.jpg"></p>
<hr>
<blockquote>
<p>"In statistics, this [Overall Evaluation Criterion (OEC)] is often
called the <em>Response</em> or <em>Dependent</em> variable (Mason, Gunst and Hess
1989, Box, Hunter and Hunter 2005); other synonyms are <em>Outcome</em>,
<em>Evaluation</em> and <em>Fitness Function</em> (Quarto-vonTivadar 2006).
Experiments can have multiple objectives and analysis can use a
balanced scorecard approach (Kaplan and Norton 1996), although
selecting a single metric, possibly as a weighted combination of
such objectives is highly desired and recommended (Roy 2001, 50,
405-429)." (page 7)</p>
</blockquote>
<p>The Roy <a href="https://www.wiley.com/en-us/Design+of+Experiments+Using+The+Taguchi+Approach%3A+16+Steps+to+Product+and+Process+Improvement-p-9780471361015">citation</a> is "Design of Experiments Using The Taguchi
Approach: 16 Steps to Product and Process Improvement". "Step 16" is
"Case studies", so I think Roy has confused "step" with "chapter".</p>
<hr>
<blockquote>
<p>"However, Google's tweaks to the color scheme
[<a href="https://www.nytimes.com/2009/03/01/business/01marissa.html">the 41 blues test</a>] ended up being substantially positive on
user engagement (note that Google does not report on the results of
individual changes) and led to a strong partnership between design
and experimentation moving forward." (page 16)</p>
</blockquote>
<p>This is an interesting take... They say (while saying they can't
support it with evidence) that <a href="https://www.nytimes.com/2009/03/01/business/01marissa.html">the 41 blues test</a> was good in
multiple ways, while the popular lore is mostly about how at least one
designer cited that experimentation as the <a href="http://www.zeldman.com/2009/03/20/41-shades-of-blue/">reason</a> they quit
Google. Hmm.</p>
<hr>
<blockquote>
<p>"One useful concept to keep in mind is <a href="https://tcagley.wordpress.com/2016/01/30/how-to-measure-anything-chapter-7-quantifying-the-value-of-information/">EVI</a>: Expected Value of
Information from Douglas Hubbard (2014), which captures how
additional information can help you in decision making." (page 24)</p>
</blockquote>
<hr>
<blockquote>
<p>"If we use <em>purchase indicator</em> (i.e., did the user purchase yes/no,
without regard to the purchase amount) instead of using
<em>revenue-per-user</em> as our OEC, the standard error will be smaller,
meaning that we will not need to expose the experiment to as many
users to achieve the same sensitivity." (page 32)</p>
</blockquote>
<p>This strikes me as an oversimplification... The measurements aren't on
the same scale, for one, so what does it mean to have a smaller
standard error, exactly? The two are testing different things... You
could imagine a world where the experimental condition convinces 100%
of users to make a $1 purchase, but stops the 5% of users who were
previously making $100 purchases. That's not good. I bet the authors
meant something more precise, and I wish they would have said what.</p>
<hr>
<blockquote>
<p>"In the analysis of controlled experiments, it is common to apply
the Stable Unit Treatment Value Assumption (SUTVA) (Imbens and Rubin
2015), which states that experiment units (e.g., users) do not
interfere with one another." (page 43)</p>
</blockquote>
<hr>
<blockquote>
<p>"Sample Ratio Mismatch (SRM)"</p>
</blockquote>
<hr>
<blockquote>
<p>"Zhao et al. (2016) describe how Treatment assignment was done at
Yahoo! using the <a href="https://en.wikipedia.org/wiki/Fowler%E2%80%93Noll%E2%80%93Vo_hash_function">Fowler-Noll-Vo hash function</a>, which sufficed
for single-layer randomization, but which failed to properly
distribute users in multiple concurrent experiments when the system
was generalized to overlapping experiments. Cryptographic hash
functions like MD5 are good (Kohavi et al. 2009) but slow; a
non-cryptographic function used at Microsoft is Jenkins SpookyHash
(<a href="https://www.burtleburtle.net/bob/hash/spooky.html">www.burtleburtle.net/bob/hash/spooky.html</a>)." (page 47)</p>
</blockquote>
<hr>
<blockquote>
<p>"For Bing, over 50% of US traffic is from bots, and that number is
higher than 90% in China and Russia." (page 48)</p>
</blockquote>
<hr>
<blockquote>
<p>"<em>Goal metrics</em>, also called <em>success metrics</em> or <em>true north
metrics</em>, reflect what the organization ultimately cares about."</p>
<p>"<em>Driver metrics</em>, also called <em>sign post metrics</em>, <em>surrogate
metrics</em>, <em>indirect</em> or <em>predictive metrics</em>, tend to be
shorter-term, faster-moving, and more-sensitive metrics than goal
metrics." (page 91)</p>
<p>"<em>Guardrail metrics</em> guard against violated assumptions and come in
two types: metrics that protect the business and metrics that assess
the trustworthiness and internal validity of experiment results."
(page 92)</p>
</blockquote>
<hr>
<blockquote>
<p>"Between 1945 and 1960, the federal Canadian government paid 70
cents a day per orphan to orphanages, and psychiatric hospitals
received $2.25 per day, per patient. Allegedly, up to 20,000
orphaned children were falsely certified as mentally ill so the
Catholic Church could get $2.25 per day, per patient (Wikipedia
contributors, Data dredging 2019)." (page 101)</p>
</blockquote>
<hr>
<blockquote>
<p>"... many unconstrained metrics are gameable. A metric that measures
ad revenue <em>constrained</em> to space on the page or to a measure of
quality is a much better metric to ensure a high-quality user
experience." (page 101)</p>
</blockquote>
<hr>
<blockquote>
<p>"Generally, we recommend using metrics that measure user value and
actions." (page 101)</p>
</blockquote>
<hr>
<blockquote>
<p>"Combining Key Metrics into an OEC"</p>
<p>"Given the common situation where you have multiple goal and driver
metrics, what do you do? Do you need to choose just one metric, or
do you keep more than one? Do you combine them all into single
combination metric?"</p>
<p>"While some books advocate focusing on just one metric (<em>Lean
Analytics</em> (Croll and Yoskovitz 2013) suggest the One Metric that
Matters (OMTM) and <em>The 4 Disciplines of Execution</em> (McChesney,
Covey and Huling 2012) suggest focusing on Wildly Important Goal
(WIG)), we find that motivating but an oversimplification. Except
for trivial scenarios, there is usually no single metric that
captures what a business is optimizing for. Kaplan and Norton (1996)
give a good example: imagine entering a modern jet airplane. Is
there a single metric that you should put on the pilot's dashboard?
Airspeed? Altitude? Remaining fuel? You know the pilot must have
access to these metrics and more. When you have an online business,
you will have several key goal and driver metrics, typically
measuring user engagement (e.g., active days, sessions-per-user,
clicks- per-user) and monetary value (e.g., revenue-per-user). There
is usually no simple single metric to optimize for."</p>
<p>"In practice, many organizations examine multiple key metrics, and
have a mental model of the tradeoffs they are willing to accept when
they see any particular combination. For example, they may have a
good idea about how much they are willing to lose (churn) users if
the remaining users increase their engagement and revenue to more
than compensate. Other organizations that prioritize growth may not
be willing to accept a similar tradeoff."</p>
<p>"Oftentimes, there is a mental model of the tradeoffs, and devising
a single metric — an OEC — that is a weighted combination of the
such objectives (Roy 2001, 50, 405-429) may be the more desired
solution. And like metrics overall, ensuring that the metrics and
the combination are not gameable is critical (see <em>Sidebar:
Gameability</em> in Chapter 6). For example, basketball scoreboards
don't keep track of shots beyond the two- and three-point lines,
only the combined score for each team, which is the OEC. FICO credit
scores combine multiple metrics into a single score ranging from 300
to 850. The ability to have a single summary score is typical in
sports and critical for business. A single metric makes the exact
definition of success clear and has a similar value to agreeing on
metrics in the first place: it aligns people in an organization
about the tradeoffs. Moreover, by having the discussion and making
the tradeoffs explicit, there is more consistency in decision making
and people can better understand the limitations of the combination
to determine when the OEC itself needs to evolve. This approach
empowers teams to make decisions without having to escalate to
management and provides an opportunity for automated searches
(parameter sweeps)."</p>
<p>"If you have multiple metrics, one possibility proposed by Roy
(2001) is to normalize each metric to a predefined range, say 0-1,
and assign each a weight. Your OEC is the weighted sum of the
normalized metrics." (pages 104-105)</p>
</blockquote>
<hr>
<blockquote>
<p>"GoodUI.org summarizes many UI patterns that win [A/B tests]
repeatedly." (page 113)</p>
</blockquote>
<hr>
<blockquote>
<p>"Experiment randomization can also act as a great instrumental
variable." (page 114)</p>
</blockquote>
<p>Hmm! I guess this would be the case if you found the experiment had
some effect on <em>X</em>, and then you were interested in further effects of
<em>X</em> on other things. See: <a href="/20210430-a_simple_instrumental_variable/">A simple Instrumental Variable</a>.</p>
<hr>
<p><a href="https://www.nber.org/papers/w17345">The Effect of Providing Peer Information on Retirement Savings Decisions</a> (ref page 119; abstract shown here)</p>
<blockquote>
<p>We conducted a field experiment in a 401(k) plan to measure the
effect of disseminating information about peer behavior on savings.
Low-saving employees received simplified plan enrollment or
contribution increase forms. A randomized subset of forms stated the
fraction of age-matched coworkers participating in the plan or
age-matched participants contributing at least 6% of pay to the
plan. We document an oppositional reaction: the presence of peer
information decreased the savings of non-participants who were
ineligible for 401(k) automatic enrollment, and higher observed peer
savings rates also decreased savings. Discouragement from upward
social comparisons seems to drive this reaction.</p>
</blockquote>
<p>Hmm! Usually peer effects are supposed to be so great...</p>
<hr>
<blockquote>
<p>"For example, Bing and Google's scaled-out human evaluation programs
are fast enough to use alongside the online controlled experiment
results to determine whether to launch the change." (page 131)</p>
</blockquote>
<hr>
<blockquote>
<p>"What customers say in a focus group setting or a survey may not
match their true preferences. A well-known example of this
phenomenon occurred when Philips Electronics ran a focus group to
gain insight into teenagers' preferences for boom box features. The
focus group attendees expressed a strong preference for yellow boom
boxes during the focus group, characterizing black boom boxes as
“conservative.” Yet when attendees exited the room and were given
the chance to take home a boom box as a reward for their
participation, most chose black (Cross and Dixit 2005)." (page 132)</p>
</blockquote>
<hr>
<blockquote>
<p>"Note that sophisticated modeling may be necessary to infer the
impact, with an online example of ITS [Interrupted Time Series]
being Bayesian Structural Time Series analysis (Charles and Melvin
2004)." (page 140)</p>
</blockquote>
<hr>
<p>"Interleaved Experiments" (page 141) are when you have two ranking
methods and you interleave their results (removing duplicates) and see
which ones get more clicks. Seems neat.</p>
<hr>
<blockquote>
<p>"More active users are simply more likely to do a broad range of
activities. Using activity as a factor is typically important."
(page 148)</p>
</blockquote>
<hr>
<p>In character for a book on RCTs, they point out
<a href="https://experimentguide.com/refuted_observational_studies/">Refuted Causal Claims from Observational Studies</a>. (pages 147-149)</p>
<hr>
<blockquote>
<p>"Indeed, the most difficult part of instrumentation is getting
engineers to instrument in the first place." (page 165)</p>
</blockquote>
<hr>
<blockquote>
<p>"The real measure of success is the number of experiments that can
be crowded into 24 hours." (quoting Thomas A. Edison, page 171)</p>
</blockquote>
<hr>
<blockquote>
<p>"The visualization tool is not just for per-experiment results but
is also useful for pivotting to <strong>per-metric results</strong> across
experiments. While innovation tends to be decentralized and
evaluated through experimentation, the global health of key metrics
is usually closely monitored by stakeholders." (page 181)</p>
</blockquote>
<hr>
<blockquote>
<p>"Assuming Treatment and Control are of equal size, the total number
of samples you need to achieve 80% power can be derived from the
power formula above, and is approximately as shown in Equation 17.8
(van Belle 2008):</p>
</blockquote>
<p>\[ n \approx \frac{16 \sigma^2 }{ \delta^2 } \]</p>
<blockquote>
<p>where, \( \sigma^2 \) is the sample variance, and \( \delta \)
is the difference between Treatment and Control." (page 189)</p>
</blockquote>
<hr>
<blockquote>
<p>"How can we ensure that Type I and Type II errors are still
reasonably controlled under multiple testing? There are many well
studied approaches; however, most approaches are either simple but
too conservative, or complex and hence less accessible. For example,
the popular Bonferroni correction, which uses a consistent but much
smaller p-value threshold (0.05 divided by the number of tests),
falls into the former category. The Benjamini-Hochberg procedure
(Hochberg and Benjamini 1995) uses varying p-value thresholds for
different tests and it falls into the latter category." (page 191)</p>
</blockquote>
<p>Benjamini–Hochberg (<a href="https://en.wikipedia.org/wiki/False_discovery_rate#BH_procedure" title="Benjamini–Hochberg procedure">wiki</a>, <a href="https://www.statisticshowto.com/benjamini-hochberg-procedure/" title="How to Run the Benjamini–Hochberg procedure">how-to</a>) doesn't seem so bad, either.
Sort of the flavor of a QQ plot, almost?</p>
<hr>
<p>Page 192 (section on "Fisher's Meta analysis") has a bunch on how to
combine p-values from multiple experiments.</p>
<hr>
<p>Pages 194-195 discuss ratio metrics and figuring out the variance of a
ratio using the <a href="https://en.wikipedia.org/wiki/Delta_method" title="Delta method">delta method</a>, as referenced in <a href="http://alexdeng.github.io/public/files/WSDM2017draft.pdf" title="Trustworthy analysis of online A/B tests: Pitfalls, challenges and solutions">Deng et al.</a>
§4.2. See also <a href="https://www.stat.cmu.edu/~hseltman/files/ratio.pdf">Seltman's note</a> deriving the result. It is a little
bit of a weird formula, but the book makes it seem rather fancier than
it really is, I think, and their motivation doesn't seem to be
strictly relevant.</p>
<hr>
<p>On page 197 they mention <a href="https://exp-platform.com/Documents/2013-02-CUPED-ImprovingSensitivityOfControlledExperiments.pdf" title="Improving the Sensitivity of Online Controlled Experiments by Utilizing Pre-Experiment Data (CUPED: Controlled-experiment Using Pre-Experiment Data)">CUPED</a>.</p>
<hr>
<blockquote>
<p>"While you can always resort to bootstrap for conducting the
statistical test by finding the tail probabilities, it gets
expensive computationally as data size grows. On the other hand, if
the statistic follows a normal distribution asymptotically, you can
estimate variance cheaply. For example, the asymptotic variance for
quantile metrics is a function of the density (Lehmann and Romano
2005). By estimating density, you can estimate variance." (page 199)</p>
</blockquote>
<p>The citation is <a href="https://link.springer.com/book/10.1007/0-387-27605-X" title="Testing Statistical Hypotheses">Testing Statistical Hypotheses</a>.</p>
<hr>
<blockquote>
<p>"When conducting t-tests to compute p-values, the distribution of
p-values from repeated trials [of A/A tests] should be close to a
uniform distribution." (page 200)</p>
</blockquote>
<hr>
<blockquote>
<p>"Bing uses continuous A/A testing to identify a carry-over effect
(or residual effect), where previous experiments would impact
subsequent experiments run on the same users." (page 201)</p>
</blockquote>
<hr>
<blockquote>
<p>"We highly recommend running continuous A/A tests in parallel with
other experiments to uncover problems, including distribution
mismatches and platform anomalies." (page 201)</p>
</blockquote>
<hr>
<p>This is wild:</p>
<blockquote>
<p>"The book <em>A/B Testing: The Most Powerful Way to Turn Clicks into
Customers</em> (Siroker and Koomen 2013) suggests an incorrect procedure
for ending experiments: “Once the test reaches statistical
significance, you'll have your answer,” and “When the test has
reached a statistically significance conclusion ...” (Kohavi 2014).
The statistics commonly used assume that a single test will be made
at the end of the experiment and “peeking” violates that assumption,
leading to many more false positives than expected using classical
hypothesis testing.</p>
<p>Early versions of Optimizely encouraged peeking and thus early
stopping, leading to many false successes. When some experimenters
started to run A/A tests, they realized this, leading to articles
such as “How Optimizely (Almost) Got Me Fired” (Borden 2014). To
their credit, Optimizely worked with experts in the field, such as
Ramesh Johari, Leo Pekelis, and David Walsh, and updated their
evaluations, dubbing it “Optimizely's New Stats Engine” (Pekelis
2015, Pekelis, Walsh and Johari 2015). They address A/A testing in
their glossary (Optimizely 2018a)." (page 203)</p>
</blockquote>
<p>Their whole job!</p>
<hr>
<blockquote>
<p>"Always run a series of A/A tests before utlizing an A/B testing
system. Ideally, simulate a thousand A/A tests and plot the
distribution of p-values. If the distribution is far from uniform,
you have a problem. Do not trust your A/B testing system before
resolving the issue." (page 205)</p>
</blockquote>
<hr>
<p><a href="https://en.wikipedia.org/wiki/Rubin_causal_model">Rubin causal model</a>,
page 226</p>
<!-- mathjax for formulas -->

    ]]></description>
<link>http://planspace.org/20211229-trustworthy_online_controlled_experiments/</link>
<guid>http://planspace.org/20211229-trustworthy_online_controlled_experiments/</guid>
<pubDate>Wed, 29 Dec 2021 12:00:00 -0500</pubDate>
</item>
<item>
<title>Habits for 2022: “ratio”</title>
<description><![CDATA[

<p>In 2021, I worked on improving my <a href="/20211030-my_daily_routine/" title="My daily routine">daily routine</a>. I need to keep
working on it, but as 2022 approaches, I was thinking about other
changes to make. In the CGP Grey “<a href="https://www.youtube.com/watch?v=NVGuFdX5guE&amp;ab_channel=CGPGrey" title="CGP Grey: Your Theme">theme</a>” style, I might say
“ratio” as in “ratio of quality to junk.” Here are some practices I
think can support this:</p>
<ul>
<li><strong>Only check the news on Sundays.</strong> I check the news (and my phone
   generally) too much, and it doesn't really make my life better to
   check the news several times a day. Sunday is the day for news.</li>
<li><strong>Keep a book ready in Kindle app.</strong> When I <em>do</em> pick up my phone,
   there should be something worthwhile there. Read a book, or put the
   phone down.</li>
<li><strong>Skim more.</strong> I overcommit to reading things the slow way, which
   takes a lot of time and prevents me from opening books I don't want
   to commit to. I hate the phrase “speed reading” because I don't
   think it's real reading, but skimming in the sense of “survey” in
   <a href="https://en.wikipedia.org/wiki/SQ3R" title="survey, question, read, recite and review">SQ3R</a> is a distinct thing that's worthwhile. I recently did a
   quick <a href="/20211213-practical_data_science_with_python_by_george/" title="Practical Data Science with Python, by George">book review</a> based on skimming and I thought it was fair
   and I learned a few things too. Skimming could get me connected
   with my backlog of nonfiction.</li>
</ul>    
    ]]></description>
<link>http://planspace.org/20211228-habits_for_2022_ratio/</link>
<guid>http://planspace.org/20211228-habits_for_2022_ratio/</guid>
<pubDate>Tue, 28 Dec 2021 12:00:00 -0500</pubDate>
</item>
<item>
<title>naldaramjui.com is closed</title>
<description><![CDATA[

<p>Sorry! After over ten years, I've turned off my old site for
practicing <a href="https://en.wikipedia.org/wiki/Test_of_Proficiency_in_Korean" title="Test of Proficiency in Korean">TOPIK</a> test questions.</p>
<ul>
<li>The <a href="http://www.topik.go.kr/">official TOPIK site</a> is much better than it used to be. It
   has practice questions <a href="https://www.topik.go.kr/TWSTDY/TWSTDY0010.do">every day</a>. The interface seems to be
   still largely in Korean even if you switch the language selector at
   the top, but it's not bad! You can find a lot of resources there.</li>
<li>The site ran on Google App Engine, using their oldest Python setup.
   It had been pretty stable, but in the last year or so it stopped
   handling cookies right, breaking the answer-evaluating
   functionality, and it didn't seem worth re-working to restore.</li>
<li>Some bots were abusing the simple sign-up form to sign up people
   who didn't want to be signed up, which was not cool.</li>
</ul>
<p>If you want to see what other kinds of things I'm up to, check out the
<a href="/">blog</a>! 화이팅! 😄</p>
<hr>
<p><img alt="His name is 무기." src="fs.jpg"></p>    
    ]]></description>
<link>http://planspace.org/20211227-naldaramjui_is_closed/</link>
<guid>http://planspace.org/20211227-naldaramjui_is_closed/</guid>
<pubDate>Mon, 27 Dec 2021 12:00:00 -0500</pubDate>
</item>
<item>
<title>The Genetic Lottery, by Harden</title>
<description><![CDATA[

<p>Harden has something for everyone to dislike, telling the left that
environment isn't everything and telling the right that
self-determination isn't everything. I think her recommendation to use
genetic controls in statistical analysis is interesting but hard.</p>
<blockquote>
<p>"The biggest contribution of genetics to the social sciences is to
give researchers an additional set of tools to do basic research by
measuring and statistically controlling for a variable—DNA—that has
previously been very difficult to measure and statistically control
for." (page 192)</p>
</blockquote>
<p><a href="https://press.princeton.edu/books/hardcover/9780691190808/the-genetic-lottery" title="The Genetic Lottery: Why DNA Matters for Social Equality">The Genetic Lottery</a> has received criticism. Here's
<a href="https://www.lareviewofbooks.org/article/why-dna-is-no-key-to-social-equality-on-kathryn-paige-hardens-the-genetic-lottery" title="Why DNA Is No Key to Social Equality: On Kathryn Paige Harden’s “The Genetic Lottery”">Henn et al.</a>:</p>
<blockquote>
<p>"Ultimately, [Harden's] focus on genetics as a fundamental cause of
social inequality reduces her version of social justice to
benevolent paternalism."</p>
</blockquote>
<p>I think Harden does a fair job of being clear that genetics can be a
cause, but certainly not the only cause, and not a cause that can't be
redressed. On “benevolent paternalism,” I think Henn et al. intend the
phrase to have negative connotation, but couldn't any attempt at
social justice (or social safety nets) be referred to in this way?</p>
<p>As in the “equality vs. equity” <a href="https://interactioninstitute.org/illustrating-equality-vs-equity/">cartoon</a> on page 162, I take it as
a given that everyone should be able to see over the fence—the hard
question is how high the fence that we're trying to get everybody over
is. What's the level that should be guaranteed, and what are
exceptions for extreme cases?</p>
<p>Bird has <a href="https://massivesci.com/articles/genetic-lottery-review-paige-harden-kevin-bird/" title="The Genetic Lottery is a bust for both genetics and policy">issues</a> with the presentation of the science, and I agree
I would have liked more detail and precision in the presentation, but
Harden is also trying to reach a broad audience and cover a lot of
material. I think Bird is incorrect in his accusation that Harden
doesn't engage with the history of eugenics.</p>
<p>Harden's point that genetic confounds can affect optimal policy
recommendations seems meaningful to me. If parents with lots of books
in their home have kids who learn to read, does that imply giving
everyone a stack of books is all we need to do?</p>
<p>I think Harden is right that if the well-intentioned don't engage with
genetics, their impact is muted by confounding while the
ill-intentioned advertise “forbidden knowledge” to the benefit of
none. But I don't expect a major shift in Harden's lifetime.</p>
<p><img alt="cover" src="cover.jpg"></p>
<hr>
<blockquote>
<p>"Building a commitment to egalitarianism on our genetic uniformity
is building a house on sand." (page 19)</p>
</blockquote>
<hr>
<blockquote>
<p>"A study of what is correlated with succeeding in an education
system doesn't tell you whether that system is good, or fair, or
just." (page 60)</p>
</blockquote>
<hr>
<p>Quoting the organizers of the <a href="https://www.fragilefamilieschallenge.org/">Fragile Families Challenge</a>:</p>
<blockquote>
<p>“If one measures our degree of understanding by our ability to
predict, then results ... suggest that our understanding of child
development and the life course is actually quite poor.” (page 70)</p>
</blockquote>
<hr>
<blockquote>
<p>"I think we must dismantle the false distinction between
“inequalities that society is responsible for addressing” and
“inequalities that are caused by differences in biology.”" (page 91)</p>
</blockquote>
<hr>
<p><a href="https://www.nber.org/papers/w22595">Understanding and Misunderstanding Randomized Controlled Trials</a></p>
<hr>
<p>Interesting unit: <a href="https://en.wikipedia.org/wiki/Centimorgan">centimorgan</a></p>
<hr>
<p><a href="https://www.nature.com/articles/nrg2322">Heritability in the genomics era — concepts and misconceptions</a></p>
<blockquote>
<p>"half of the additive genetic variance is between families and half
is within families"</p>
</blockquote>
<hr>
<p><a href="https://www.nature.com/articles/456018a">Personal genomes: The case of the missing heritability</a></p>
<hr>
<p>Dang, I would like to see some worked examples for how heritabilities
are calculated...</p>
<hr>
<p><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2762790/">Individual Differences in Executive Functions Are Almost Entirely Genetic in Origin</a></p>
<hr>
<blockquote>
<p>"As Dostoevsky reminded us, “It takes something more than
intelligence to act intelligently.”" (page 141, referring to <em>Crime
and Punishment</em>)</p>
</blockquote>
<hr>
<p><a href="https://www.pnas.org/content/115/31/E7275">Genetic analysis of social-class mobility in five longitudinal studies</a></p>
<p>In its conclusion, a sentiment shared by Harding:</p>
<blockquote>
<p>"A long-term goal of our sociogenomic research is to use genetics to
reveal novel environmental intervention approaches to mitigating
socioeconomic disadvantage."</p>
</blockquote>
<hr>
<p><a href="https://pubmed.ncbi.nlm.nih.gov/27337136/">Genetically-mediated associations between measures of childhood character and academic achievement</a></p>
<p>In Figure 7.3 of the book, a list based on that ref:</p>
<ul>
<li>grit<ul>
<li>passion and perseverance for long-term goals</li>
</ul>
</li>
<li>growth mindset<ul>
<li>belief that intelligence is malleable</li>
</ul>
</li>
<li>intellectual curiosity<ul>
<li>desire to think about difficult or new problems</li>
</ul>
</li>
<li>mastery orientation<ul>
<li>motivation to learn for the sake of learning</li>
</ul>
</li>
<li>self-concept<ul>
<li>belief that one is smart and capable of learning</li>
</ul>
</li>
<li>test motivation<ul>
<li>trying hard on tests</li>
</ul>
</li>
</ul>
<blockquote>
<p>"The SNPs correlated with non-cognitive skills were correlated with
<em>higher</em> risk for several mental disorders, including schizophrenia,
bipolar disorder, anorexia nervosa, and obsessive-compulsive
disorder. This result warns us against viewing the genetic variants
that are associated with going further in current systems of formal
education as being inherently “good” things. A single genetic
variant might make it a tiny bit more likely that someone will go
further in school, but that same variant might also elevate their
risk of developing schizophrenia or another serious mental
disorder." (page 144)</p>
</blockquote>
<hr>
<blockquote>
<p>"Unfortunately, the mistaken idea that genetic influences are an
impermeable barrier to social change is also widely endorsed not
just by those who are trying to naturalize inequality, but also by
their ideological and political opponents." (page 155)</p>
</blockquote>
<hr>
<p><a href="https://pubmed.ncbi.nlm.nih.gov/27359131/">Strong genetic overlap between executive functions and intelligence</a></p>
<hr>
<blockquote>
<p>"I could quote the Bible verse from Thessalonians that was quoted to
me as a child: “The one who is unwilling to work shall not eat.”"
(page 212)</p>
</blockquote>
<hr>
<blockquote>
<p>"There is no measure of so-called “merit” that is somehow free of
genetic influence or untethered from biology." (page 247)</p>
</blockquote>    
    ]]></description>
<link>http://planspace.org/20211224-genetic_lottery_harden/</link>
<guid>http://planspace.org/20211224-genetic_lottery_harden/</guid>
<pubDate>Fri, 24 Dec 2021 12:00:00 -0500</pubDate>
</item>
<item>
<title>Correlation can help approximate variance of a difference</title>
<description><![CDATA[

<p>Even if random variables \( \textbf{A} \) and \( \textbf{δ} \) are
uncorrelated, realized data \( A \) and \( \delta \) will happen
to have some covariance. If \( B = A + \delta \), then \(
\text{var}(B) = \text{var}(A) + \text{var}(\delta) + 2\text{cov}(A,
\delta) \), and estimating any one variance by the others will be off
by the \( 2\text{cov}(A, \delta) \) term. For \( \text{var}(\delta)
\), however, the estimate \( \text{var}(B)(1 - \text{corr}(A, B)^2)
\) will usually be closer to correct, at the cost of being slightly
biased. (But don't forget that doing \( \text{var}( \delta ) \)
directly is a great first choice.)</p>
<h3>Three methods for \( \text{var}( \delta ) \), when \( B = A + \delta \)</h3>
<p>The case of \( B = A + \delta \) (so that \( \delta = B - A \)) is
central to the paired t-test, for example. The variance of \( A \)
and \( B \) could each be large, but the variance of \( \delta \)
can still be small, making it easier to reject the null for \(
\textbf{δ} \), for example.</p>
<p><em>Be careful: We're trying to estimate a variance, so we're interested
in the variance of the estimate of the variance, which can be
confusing. Hopefully the language is clear enough here.</em></p>
<h4>1) Variance of Differences</h4>
<p>The obvious thing to do is to subtract and calculate the variance of
the differences: \( \text{var}( \delta ) = \text{var}( B - A ) \).
This is a good idea and what you should do. It's unbiased and has low
variance (for the estimate of the true variance of \( \textbf{δ}
\)).</p>
<p>Why not do it like this? Honestly I'm not sure. Maybe you don't have
complete data, and you're looking for a \( \delta_1 - \delta_2 \)
where the means of \( A_1 \) and \( A_2 \) are assumed equal so
you're using \( B_1 - B_2 \) but you want the (smaller) variance of
\( \delta_1 - \delta_2 \)? So you'll estimate some things with
complete cases even though the main effect is estimated with all \( B
\) data? Why not still use this method with the complete cases? Maybe
you're not using simple subtraction, but some more complex regression
with multiple variables? In that case, why not use variance of the
residuals directly? Do you just want a more complicated method?</p>
<h4>2) Difference of Variances</h4>
<p>By the <a href="/20201030-the_variance_sum_law_is_interesting/">variance sum law</a>, \( \text{var}( \textbf{B} ) =
\text{var}( \textbf{A} ) + \text{var}( \textbf{δ} ) \), if \(
\textbf{A} \) and \( \textbf{δ} \) are uncorrelated, so \(
\text{var}( \textbf{δ} ) = \text{var}( \textbf{B} ) - \text{var}(
\textbf{A} ) \). Real data is not generally perfectly uncorrelated,
however: \( \text{var}(\delta) = \text{var}(B) - \text{var}(A) -
2\text{cov}(A, \delta) \). The covariance term is zero in
expectation, so estimating \( \text{var}(\delta) = \text{var}(B) -
\text{var}(A) \) is unbiased. But the \( 2\text{cov}(A, \delta) \)
is a kind of noise term, and adds variance to the estimate of \(
\text{var}( \textbf{δ} )\).</p>
<h4>3) Using Correlation</h4>
<p>It isn't obvious, but using \( \text{var}(B)(1 - \text{corr}(A, B)^2)
\) to estimate \( \text{var}( \textbf{δ} ) \) is much like using
the Difference of Variances, but with a generally smaller (but always
positive) error coming from \( \text{cov}(A, \delta) \). Proceeding
in small steps:</p>
<p>\[ \text{var}(\delta) = \text{var}(B) - \text{var}(A) - 2\text{cov}(A, \delta) \]</p>
<p>\[ \text{var}(\delta) = \text{var}(B) - \left( \text{var}(A) + 2\text{cov}(A, \delta) \right) \]</p>
<p>\[ \text{var}(\delta) = \text{var}(B) \left( 1 - \frac{\text{var}(A) + 2\text{cov}(A, \delta)}{ \text{var}(B) } \right) \]</p>
<p>\[ \text{var}(\delta) = \text{var}(B) \left( 1 - \frac{\text{var}(A)^2 + 2\text{var}(A)\text{cov}(A, \delta)}{ \text{var}(A) \text{var}(B) } \right) \]</p>
<p>This has been algebraic. Now add \( \text{cov}(A, \delta)^2 \) to
the fractional term's numerator. (This is an error in the estimate, to
get to the destination.)</p>
<p>\[ \text{var}(\delta) = \text{var}(B) \left( 1 - \frac{\text{var}(A)^2 + 2\text{var}(A)\text{cov}(A, \delta) + \text{cov}(A, \delta)^2 }{ \text{var}(A) \text{var}(B) } \right) \]</p>
<p>\[ \text{var}(\delta) = \text{var}(B) \left( 1 - \frac{ ( \text{var}(A) + \text{cov}(A, \delta))^2 }{ \text{var}(A) \text{var}(B) } \right) \]</p>
<p>The bit squared in the fraction's numerator is \( \sum{(A_i - \bar{A}
)^2} + \sum{(A_i - \bar{A} )(\delta_i - \bar{\delta})} \), which is
\( \sum{(A_i - \bar{A})(A_i - \bar{A} + \delta_i - \bar{\delta})}
\). Since \( B_i = A_i + \delta_i \) and \( \bar{B} = \bar{A} +
\bar{\delta} \), that's \( \sum{(A_i - \bar{A})(B_i - \bar{B})} \),
which is \( \text{cov}(A, B) \).</p>
<p>\[ \text{var}(\delta) = \text{var}(B) \left( 1 - \frac{ \text{cov}(A, B)^2 }{ \text{var}(A) \text{var}(B) } \right) \]</p>
<p>So at last, by definition:</p>
<p>\[ \text{var}(\delta) = \text{var}(B) \left( 1 - \text{corr}(A, B)^2 \right) \]</p>
<p>QED. The error introduced is \( \text{cov}(A, \delta)^2 /
\text{var}(A) \), which will tend to be considerably smaller in
absolute value than the \( 2\text{cov}(A, \delta) \) error in the
Difference of Variances method because \( 0 \le |\text{cov}(A,
\delta)| / \text{var}(A) \ll 2 \). This error is however positive in
expectation, so the estimate is biased: it will be slightly too small.</p>
<h3>Computational demonstration</h3>
<p>Generating 1,000 datasets where each of \( A \) and \( \delta \)
have 100 values drawn at random from Gaussians with variance 9 and 16,
respectively, the three methods above generate estimates of \(
\text{var}( \textbf{δ} ) \) as follows:</p>
<pre><code>| Method                  | Mean var(d) estimate | var(estimate) |
|-------------------------|----------------------|---------------|
| Variance of Differences |                15.98 |          5.10 |
| Difference of Variances |                15.95 |         10.69 |
| Using Correlation       |                15.82 |          5.12 |</code></pre>

<p>As expected, the method Using Correlation comes out more below the
true value of 16 on average, but its variance is comparable to that of
the Variance of Differences.</p>
<p>The bias is already small with just 100 samples, and gets smaller
still as samples get bigger and sample correlations tend to be
smaller.</p>
<p>Here's clumsy <a href="https://www.r-project.org/">R</a> code to do the experiment:</p>
<pre><code class="language-r">trials = 1000
samples = 100

set.seed(0)
est1 = c()
est2 = c()
est3 = c()

for (i in 1:trials) {
  A = rnorm(samples, sd=3)  # var=9
  d = rnorm(samples, sd=4)  # var=16
  B = A + d  # var=25, in population sense
  est1 = c(est1, var(B - A))  # same as var(d)
  est2 = c(est2, var(B) - var(A))
  est3 = c(est3, var(B) * (1 - cor(A, B)^2))
}

mean(est1)
## [1] 15.98799
mean(est2)
## [1] 15.9451
mean(est3)
## [1] 15.82234
var(est1)
## [1] 5.09602
var(est2)
## [1] 10.69393
var(est3)
## [1] 5.117983</code></pre>
<!-- mathjax for formulas -->

    ]]></description>
<link>http://planspace.org/20211219-correlation_can_help_approximate_variance_of_a_difference/</link>
<guid>http://planspace.org/20211219-correlation_can_help_approximate_variance_of_a_difference/</guid>
<pubDate>Sun, 19 Dec 2021 12:00:00 -0500</pubDate>
</item>
<item>
<title>Practical Data Science with Python, by George</title>
<description><![CDATA[

<p>Packt sent me a copy of this <a href="https://www.packtpub.com/product/practical-data-science-with-python/9781801071970">book</a> to review. It's 621 pages with
the best and worst characteristics of a typical data science boot
camp: very broad, necessarily shallow, frequently not quite perfect.
Even an imperfect map can tell you a lot about the territory, and it
could be the right book for you.</p>
<p>George has pulled together a lot of material, some of it good. He
includes introductory Python and command line, enough SQL to be
confused about SQL, examples with Bitcoin prices, an idiosyncratic
survey of visualization, web scraping, statistics, and the big machine
learning models, including the <a href="/20211003-xgboost_lightgbm_catboost_briefly/">big three boosted tree algorithms</a>,
which I appreciate. He includes some NLP, and even some on ethics.</p>
<p>George's own list of omissions (page 571) illustrates what he thinks
is almost in scope:</p>
<ul>
<li>Recommender systems</li>
<li>Networks and graph analysis</li>
<li>Machine learning explainability</li>
<li>Test-driven development (TDD)</li>
<li>Reinforcement learning</li>
<li>Neural networks</li>
</ul>
<p>Maybe the moral is that “data science” is too big a topic for one
book. Trying to pack so much in has a cost. Here's the complete
section on “Paired t- and z-tests”:</p>
<blockquote>
<p>One last type of t- or z-test is the paired test. This is for paired
samples, like before-and-after treatments. For example, we could
measure the blood pressure of people before and after taking a
medication to see if there is an effect. A function that can be used
for this is <code>scipy.stats.ttest_rel</code>, which can be used like this:</p>
<p><code>scipy.stats.ttest_rel(before, after)</code></p>
<p>This will return a t-statistic and p-value like with other <code>scipy</code>
t-test functions.</p>
</blockquote>
<p>If you've never heard of a paired t-test before, it's great this book
tells you about it. You can start to ask questions like: Why is this a
separate test? Does it have some advantage over a regular t-test?
Hopefully you also question some parts of the book, as when Bayesian
methods are dismissed as “much more complex to implement than a
t-test.”</p>
<p>This is a map that can point you in a lot of interesting directions,
which is valuable!</p>
<p><img alt="cover" src="cover.png"></p>    
    ]]></description>
<link>http://planspace.org/20211213-practical_data_science_with_python_by_george/</link>
<guid>http://planspace.org/20211213-practical_data_science_with_python_by_george/</guid>
<pubDate>Mon, 13 Dec 2021 12:00:00 -0500</pubDate>
</item>
<item>
<title>Significant Figures: Rounding destroys information</title>
<description><![CDATA[

<p>Significant Figures often requires rounding so that the correct level
of uncertainty is conveyed. Rounding is only supposed to happen at the
“end” of calculations, and results can vary depending on what is
considered an intermediate vs. a final result. In all cases, rounding
is harmful and only necessary because of the limitations of
Significant Figures.</p>
<p>It should be clear that 4.3 with an <a href="/20211209-significant_figures_gaussian_uncertainty/">uncertainty</a> \( \sigma \times
0.1 \) is not the same as 4.34 with the same uncertainty. It's
because Significant Digits can't say 4.34 without meaning the
uncertainty is \( \sigma \times 0.01 \) that we round to 4.3, if the
uncertainty is \( \sigma \times 0.1 \). But this has a cost: good
information is dropped.</p>
<p>Consider adding 1 + 1.4 + 1.4. If we do it in one go, we get 3.8 and
then round to 4 for significance. But what if this is done in two
steps? Maybe one team does 1 + 1.4, correctly reports their result as
2, and then a second team builds on that, adding 1.4 to get 3. The
rounding that Significant Figures requires degrades the quality of
results.</p>
<p>At some point it isn't worth tracking every digit in a result, but
Significant Figures often encourages dropping too many. It may even
give people the incorrect idea that if our uncertainty suggests two
significant figures, we can't have three figures in our best guess at
what the value is. We absolutely can, but this requires a system more
expressive than Significant Figures to report.</p>
<!-- mathjax for formulas -->

    ]]></description>
<link>http://planspace.org/20211212-significant_figures_rounding_destroys_information/</link>
<guid>http://planspace.org/20211212-significant_figures_rounding_destroys_information/</guid>
<pubDate>Sun, 12 Dec 2021 12:00:00 -0500</pubDate>
</item>
<item>
<title>“Show your work” means “Write a proof”</title>
<description><![CDATA[

<p>At any level of mathematics, “showing your work” should not be a
chore, but a chance to communicate an argument for correctness: to
write a proof. “Work” as “proof” keeps mathematics relevant,
authentic, and interesting.</p>
<p>Many school math exercises can be answered with calculators. But how
do we know the calculator is correct? The difference between inductive
(“the calculator has always been right”) and deductive (“this answer
is proven correct”) is important. The answer itself isn't important;
it's the demonstration that the answer follows from the question.</p>
<p>High school geometry is sometimes pointed to as the first place
students encounter the idea of proofs. This need not be. Arithmetic is
a process of proof, using theorems of single-digit operations and
place-based algorithms to build new results. In this way, 2+2=4 is
used to demonstrate that 22+22=44, and so on. These could be called
constructive proofs.</p>
<p>“Work” as “proof” opens up the world of math. “Checking your work” is
finding a second proof. For the advanced student who might have been
bored with “showing work,” there is an invitation to find further
proofs, understand algorithms more deeply, and be creative, rather
than being limited to mechanical processes.</p>
<p>Computational fluency is not without value, but math is about much
more. We waste opportunities to deliver on teaching mathematical ways
of thinking if students don't realize that they're constructing
logical arguments in all their math classes.</p>    
    ]]></description>
<link>http://planspace.org/20211212-show_your_work_means_write_a_proof/</link>
<guid>http://planspace.org/20211212-show_your_work_means_write_a_proof/</guid>
<pubDate>Sun, 12 Dec 2021 12:00:00 -0500</pubDate>
</item>
<item>
<title>Bernoulli's Fallacy, by Clayton</title>
<description><![CDATA[

<p><a href="https://aubreyclayton.com/bernoulli"><em>Bernoulli's Fallacy</em></a> is that the likelihood of data given a
hypothesis is enough to make inferences about that hypothesis (or
others). Clayton covers historical and modern aspects of frequentist
statistics, and lays crises of replication at the feet of significance
testing. I find it largely compelling, though perhaps it neglects
problematic contributions of non-statistical pressures in modern
academic life. I'm generally on board re: Bayesian methods.</p>
<p><img alt="cover" src="cover.jpg"></p>
<hr>
<blockquote>
<p>"...statistical methods are a means of accounting for the epistemic
role of measurement error and uncertainty..." (page x)</p>
</blockquote>
<hr>
<blockquote>
<p>"...an effect—if it <em>is</em> found—is likely overstated and unlikely to
be replicable, a paradox known as the “<a href="https://en.wikipedia.org/wiki/Winner%27s_curse">winner's curse</a>.”" (page
xi)</p>
</blockquote>
<hr>
<blockquote>
<p>The common theme is that there would be no need to continually treat
the symptoms of statistical misuse if the underlying disease were
addressed.</p>
<p>I offer the following for consideration:</p>
<p>Hypothesizing after the results of an experiment are known does not
necessarily present a problem and in fact is the way that most
hypotheses are ever constructed.</p>
<p>No penalty need be paid, or correction made, for testing multiple
hypotheses at once using the same data.</p>
<p>The conditions causing an experiment to be terminated are largely
immaterial to the inferences drawn from it. In particular, an
experimenter is free to keep conducting trials until achieving a
desired result, with no harm to the resulting inferences.</p>
<p>No special care is required to avoid “overfitting” a model to the
data, and validating the model against a separate set of test data
is generally a waste.</p>
<p>No corrections need to be made to statistical estimators (such as
the sample variance as an estimate of population variance) to ensure
they are “unbiased.” In fact, by doing so the quality of those
estimators may be made worse.</p>
<p>It is impossible to “measure” a probability by experimentation.
Furthermore, all statements that begin “The probability is ...”
commit a category mistake. There is no such thing as “objective”
probability.</p>
<p>Extremely improbably events are not necessarily noteworthy or reason
to call into question whatever assumed hypotheses implied they were
improbable in the first place.</p>
<p>Statistical methods requiring an assumption of a particular
distribution (for example, the normal distribution) for the error in
measurement are perfectly valid whether or not the data “actually
is” normally distributed.</p>
<p>It makes no sense to talk about whether data “actually is” normally
distributed or could have been sampled from a normally distributed
population, or any other such consideration.</p>
<p>There is no need to memorize a complex menagerie of different tests
or estimators to apply to different kinds of problems with different
distributional assumptions. Fundamentally, all statistical problems
are the same.</p>
<p>“Rejecting” or “accepting” a hypothesis is not the proper function
of statistics and is, in fact, dangerously misleading and
destructive.</p>
<p>The point of statistical inference is not to produce the right
answers with high frequency, but rather to <em>always</em> produce the
inferences best supported by the data at hand when combined with
existing background knowledge and assumptions.</p>
<p>Science is largely not a process of falsifying claims definitively,
but rather assigning them probabilities and updating those
probabilities in light of observation. This process is endless. No
proposition apart from a logical contradiction should ever get
assigned probability 0, and nothing short of a logical tautology
should get probability 1.</p>
<p>The more unexpected, surprising, or contrary to established theory a
proposition seems, the more impressive the evidence must be before
that proposition is taken seriously.</p>
</blockquote>
<hr>
<p>Heavily influenced by <a href="https://www.cambridge.org/core/books/probability-theory/9CA08E224FF30123304E6D8935CF1A99">Probability Theory: The Logic of Science</a> by
Edwin Jaynes.</p>
<hr>
<blockquote>
<p>"We can, for example, set <em>s</em> = 0.01 and by 99 percent sure.
Bernoulli called this “moral certainty,” as distinct from absolute
certainty of the kind only logical deduction can provide." (page 8)</p>
</blockquote>
<hr>
<blockquote>
<p>"<em>statistics is both much easier and much harder than we have been
led to believe.</em>" (page 17, italics in original)</p>
</blockquote>
<hr>
<blockquote>
<p>"Aristotle's <em>Rhetoric</em> described “the Probably” (in Greek, <em>eikos</em>,
from <em>eoika</em> meaning “to seem”) as “that which happens generally but
not invariably.” The context for this was his classification of the
arguments one could use in a courtroom or legislative debate, where
perfect logical deductions may not be available. He called this form
of argument an <em>enthymeme</em>, to be distinguished from the purely
logical form of argument known as the <em>syllogism</em>, which links
together a set of assumed premises to reach deductive
conclusions..." (page 22)</p>
</blockquote>
<hr>
<blockquote>
<p>"Hume's general point
[in <em>An Enquiry Concerning Human Understanding</em>], later referred to
as the <em>problem of induction</em>, was that we have no way of <em>knowing</em>
experience is a guide for valid conclusions about the future because
if we did, that claim could be based only on past experience." (page
35)</p>
</blockquote>
<hr>
<p>I kind of like the names Clayton uses in his tables of calculations:
"Prior probability" is normal, then "Sampling probability" is used for
the likelihood of the data, and then he multiplies them together to
get a "Pathway probability."</p>
<hr>
<blockquote>
<p>"Whether Bayes himself believed he had disproved Hume we have no way
of knowing. Some historians such as Stephen Stigler at the
University of Chicago have suggested that since Bayes did not find
the counterexample sufficiently convincing because it relied on some
assumptions he could not justify, he delayed publishing his results.
When presenting Bayes's results to the world, Price did not shy away
from emphasizing their philosophical and religious significance.
Contemporary reprints of the essay show Price intended the title to
be “A Method of Calculating the Exact Probability of All Conclusions
founded on Induction.” In his publication, he added this preamble:
“The purpose I mean is, to shew what reason we have for believing
that there are in the constitution of things fixt laws according to
which things happen, and that, therefore, the frame of the world
must be the effect of the wisdom and power of an intelligent cause;
and thus to confirm the argument taken from final causes for the
existence of the Deity.” That is, somewhere in the calculation of
probabilities for Bayes's rule, Price thought he saw evidence for
God." (page 41)</p>
</blockquote>
<hr>
<blockquote>
<p>"... logical deduction is just a special case of reasoning with
probabilities, in which all the probability values are zeros or
ones." (page 53)</p>
</blockquote>
<hr>
<blockquote>
<p>"Jaynes's essential point bears repeating: <em>probability is about
information.</em>" (page 68, italics in original)</p>
</blockquote>
<hr>
<blockquote>
<p>"Base rate neglect and the prosecutor's fallacy are the same thing,
and both are examples of Bernoulli's Fallacy." (page 103)</p>
</blockquote>
<hr>
<blockquote>
<p>"... a new general trend of collecting data in service to the social
good. John Graunt, haberdasher by day and demographer by night, had
made a breakthrough in London in 1662 when he used weekly mortality
records to design an early warning system to detect outbreaks of
bubonic plague in the city. Even though the system was never
actually deployed, it opened people's eyes to the rich possibilities
of data gathering and its usefulness to the state. By the 1740s,
prominent thinkers such as the German philosopher Gottfried
Achenwall had taken to calling this kind of data <em>statistics</em>
(<em>statistik</em> in German), the root of which is the Latin word
<em>statisticum</em> meaning “of the state.”" (page 109)</p>
</blockquote>
<hr>
<blockquote>
<p>"His [Quetelet's] goal, perhaps antagonized by the Baron de
Keverberg's skepticism, was to investigate analytically all the ways
people <em>were</em> the same or different and to create a theory of
<em>social physics</em>, a set of laws governing society that could be an
equivalent of Kepler's laws of planetary motion and other immutable
principles of the hard sciences." (page 113)</p>
</blockquote>
<p>This reminds me of <a href="/20200714-foundation_trilogy/">psychohistory</a>.</p>
<hr>
<blockquote>
<p>"George Pólya gave it the lofty name the <em>central limit theorem</em>"
(page 120)</p>
</blockquote>
<p>Huh!</p>
<hr>
<blockquote>
<p>"He [Quetelet] would later be harshly ridiculed for his love of the
normal distribution by statisticians like Francis Edgeworth, who
wrote in 1922: “The theory [of errors] is to be distinguished from
the doctrine, the false doctrine, that generally, wherever there is
a curve with a single apex representing a group of statistics ...
that the curve must be of the ‘normal’ species. The doctrine has
been nick-named ‘Quetelismus,’ on the ground that Quetelet
exaggerated the prevalence of the normal law.”" (page 122)</p>
</blockquote>
<hr>
<p>Interesting/weird idea from Galton: "statistics by intercomparison."
If you can only order people on some characteristic (say
intelligence), then do that and then assume it's quantitatively
normal. Sort of like QQ plots. Sort of. (page 136)</p>
<hr>
<p>On page 142 it seems to be saying that Pearson's chi-squared is for
general testing of whether data comes from a certain distribution...
Is that right? Does this just mean binning out data and comparing
counts to expected? Maybe that's it?</p>
<hr>
<blockquote>
<p>"For an experimental scientist without advanced mathematical
training, the book
[Fisher's Statistical Methods for Research Workers] was a godsend.
All such a person had to do was find the procedure corresponding to
their problem and follow the instructions." (page 153)</p>
</blockquote>
<hr>
<blockquote>
<p>"He [Fisher] proved what he called the <a href="https://en.wikipedia.org/wiki/Fisher%27s_fundamental_theorem_of_natural_selection">fundamental theorem</a> of
natural selection: “The rate of increase in fitness of any organism
at any time is equal to its genetic variance in fitness at that
time.”"</p>
</blockquote>
<p>Is this in conflict with Fisher as eugenicist? It seems to be
pro-diversity? At least some kinds of diversity...</p>
<hr>
<p>Interesting comparison between choosing one- or two-sided testing, and
Bayesian priors: you're not really bringing zero information to the
problem.</p>
<hr>
<p>Huh - there really is a <a href="https://www.socscistatistics.com/tests/what_stats_test_wizard.aspx">Social Science Statistics online wizard</a>.</p>
<hr>
<blockquote>
<p>"There is no coherent theory to orthodox statistics, only a loose
amalgam of half-baked ideas held together by suggestive naming,
catchy slogans, and folk superstition." (page 196)</p>
</blockquote>
<hr>
<blockquote>
<p>"As Fisher wrote in <em>Statistical Methods for Research Workers</em>, “No
human mind is capable of grasping in its entirety the meaning of any
considerable quantity of numerical data. We want to be able to
express all the relevant information contained in the mass by means
of comparatively few numerical values. This is a purely practical
need which the science of statistics is able to some extent to
meet." (page 233)</p>
</blockquote>
<hr>
<p><a href="http://www.psych.ualberta.ca/~rozeboom/files/1960_The_fallacy_of_the_null_hypothesis_significance_test.pdf">The Fallacy of the Null-Hypothesis Significance Test</a></p>
<hr>
<p><a href="https://www.press.umich.edu/186351/cult_of_statistical_significance">The Cult of Statistical Significance</a></p>
<hr>
<blockquote>
<p>"Harold Jeffreys first proposed the idea of Bayes factors in his
<em>Theory of Probability</em>." (page 262)</p>
</blockquote>
<hr>
<p>Daryl Bem (who published in support of psi) amusingly wrote (quoted
page 264):</p>
<blockquote>
<p>To compensate for this remoteness from our participants, let us at
least become intimately familiar with the record of their behavior:
the data. Examine them from every angle. Analyze the sexes
separately. Make up new composite indexes. If a datum suggests a new
hypothesis, try to find further evidence for it elsewhere in the
data. If you see dim traces of interesting patterns, try to
reorganize the data to bring them into bolder relief. If there are
participants you don't like, or trials, observers, or interviewers
who gave you anomalous results, place them aside temporarily and see
if any coherent patterns emerge. Go on a fishing expedition for
something—anything—interesting.</p>
</blockquote>
<p>That's from <a href="https://psychology.yale.edu/sites/default/files/bemempirical.pdf">Writing the Empirical Journal Article</a>.</p>
<hr>
<p><a href="https://amstat.tandfonline.com/doi/full/10.1080/00031305.2016.1154108#.YbT7Nn1KiVK">The ASA Statement on p-Values: Context, Process, and Purpose</a></p>
<hr>
<p><a href="https://www.tandfonline.com/toc/utas20/73/sup1">Statistical Inference in the 21st Century: A World Beyond p &lt; 0.05</a></p>
<hr>
<blockquote>
<p>"Significance testing was always based on a classification of
results into significant/insignificant without regard to effect size
or importance; no attempts to rehabilitate it now can change that
fundamental aspect nor repair the damage significance testing has
already caused. This yes/no binary has well and truly mixed things
up." (page 275)</p>
</blockquote>
<hr>
<blockquote>
<p>"The better, more complete interpretation of probability is that it
measures the <em>plausibility</em> of a proposition given some assumed
<em>information</em>. This extends the notion of deductive reasoning—in
which a proposition is derivable as a logical consequence of a set
of premises—to situations of incomplete information, where the
proposition is made more or less plausible, depending on what is
assumed to be known." (page 283)</p>
</blockquote>
<hr>
<blockquote>
<p>"All probability is conditional." (page 284)</p>
</blockquote>
<hr>
<blockquote>
<p>"Once we jettison the bureaucracy of frequentist statistics, we can
spend more time doing actual science." (page 287)</p>
</blockquote>
<hr>
<blockquote>
<p>"Getting rid of the useless concepts (significance testing,
estimators, sufficient and ancillary statistics, stochastic
processes) will amount to cutting out probably 90 percent of the
standard statistics curriculum. It might even mean giving up on
statistics as a separate academic discipline altogether, but that's
alright. Probability as a topic should rightfully split time between
its parents, math and philosophy, the way logic does. Bayesian
statistical inference contains exactly one theorem of importance
anyway, and its practical techniques can be taught in a single
semester-long course in applied math. There needn't be a whole
university department dedicated to it, any more than there needs to
be a department of the quadratic formula." (page 287)</p>
</blockquote>
<hr>
<blockquote>
<p>"We should no more be teaching <em>p</em>-values in statistics courses than
we should be teaching phrenology in medical schools." (page 293)</p>
</blockquote>
<hr>
<blockquote>
<p>"Joseph Berkson called this the “interocular traumatic test”; you
know what the data means when the conclusion hits you right between
the eyes." (page 297)</p>
</blockquote>
<p>That's quoted from "Bayesian statistical inference for psychological
research."</p>
<p>That source cites "J. Berkson, personal communication, July 14, 1958"
and goes on:</p>
<blockquote>
<p>"The interocular traumatic test is simple, commands general
agreement, and is often applicable; well-conducted experiments often
come out that way. But the enthusiast's interocular trauma may be
the skeptic's random error. A little arithmetic to verify the extent
of the trauma can yield great peace of mind for little cost." (page
217)</p>
</blockquote>
<hr>
<blockquote>
<p>"The results of experiments, particularly surprising or
controversial ones, can be trusted noly if the experiments are known
to be sound; however, as is often the case, an experiment is <em>known</em>
to be sound only if it produces the results we expect. So it would
seem that no experiment can ever convince us of something
surprising. This situation was anticipated by the ancient Greek
philosopher Sextus Empiricus. In a skepticism of induction that
predated David Hume's by 1,500 years, he wrote: “If they shall judge
the intellects by the senses, and the senses by the intellect, this
involves circular reasoning inasmuch as it is required that the
intellects should be judged first in order that the sense may be
judged, and the senses be first scrutinized in order that the
intellects may be tested [hence] we possess no means by which to
judge objects.”" (page 301)</p>
</blockquote>    
    ]]></description>
<link>http://planspace.org/20211211-bernoullis_fallacy_by_clayton/</link>
<guid>http://planspace.org/20211211-bernoullis_fallacy_by_clayton/</guid>
<pubDate>Sat, 11 Dec 2021 12:00:00 -0500</pubDate>
</item>
<item>
<title>Significant Figures: Addition is too precise</title>
<description><![CDATA[

<p>The usual <a href="https://en.wikipedia.org/wiki/Significance_arithmetic#Addition_and_subtraction_using_significance_arithmetic" title="Wikipedia: Addition and subtraction using significance arithmetic">rules</a> for adding and subtracting numbers with
significant digits often propagate uncertainly approximately
correctly, but always give results more precise than they actually are
because they don't (and can't, generally) follow the
<a href="/20201030-the_variance_sum_law_is_interesting/" title="The Variance Sum Law is Interesting">Variance Sum Law</a>.</p>
<p>Say a <a href="/20211209-significant_figures_gaussian_uncertainty/" title="Significant Figures: Gaussian uncertainty, σ=2.5eN">number</a> in Significant Figures with rightmost significant
digit \( D \times 10^N \) has uncertainty with standard deviation
\( \sigma \times 10^N \), and assume errors are always uncorrelated.</p>
<p>So the number 12.3, with three significant figures, has uncertainty
\( \sigma \times 0.1 \), and 2.48 has \( \sigma \times 0.01 \).
Adding them gives 14.8, which has the same uncertainty as 12.3. By the
<a href="/20201030-the_variance_sum_law_is_interesting/" title="The Variance Sum Law is Interesting">Variance Sum Law</a>, the true uncertainty is \( \sigma \times 0.1005
\), but that's pretty close to \( \sigma \times 0.1 \). In this
way, the usual rule for adding with significant figures is often
reasonable-seeming.</p>
<p>With many numbers of the same precision, however, the usual rules are
more problematic. If you add 1.2 + 3.4 + 5.6 + 7.8, the result 18.0
implies \( \sigma \times 0.1 \), but in fact uncertainty has doubled
to \( \sigma \times 0.2 \). Significant Figures has no way to convey
this, because it only communicates in powers of ten.</p>
<p>Adding and subtracting 100 numbers with the same precision, then,
should give a result with exactly one fewer significant figures. With
25 numbers the standard deviation could “round up” to the next power
of ten, arguably. It may not be common to add so many numbers with
significant figures, but even with just a few, Sig Figs is a course
approximation of correct propagation of uncertainty.</p>
<!-- mathjax for formulas -->

    ]]></description>
<link>http://planspace.org/20211211-significant_figures_addition_is_too_precise/</link>
<guid>http://planspace.org/20211211-significant_figures_addition_is_too_precise/</guid>
<pubDate>Sat, 11 Dec 2021 12:00:00 -0500</pubDate>
</item>
  </channel>
</rss>
