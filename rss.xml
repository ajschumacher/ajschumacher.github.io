<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>plan ➔ space</title>
    <link>http://planspace.org/</link>
    <description>plan space from outer nine</description>
    <language>en-us</language>
    <atom:link href="http://planspace.org/rss.xml" rel="self" type="application/rss+xml" />
<item>
<title>The Genetic Lottery, by Harden</title>
<description><![CDATA[

<p>Harden has something for everyone to dislike, telling the left that
environment isn't everything and telling the right that
self-determination isn't everything. I think her recommendation to use
genetic controls in statistical analysis is interesting but hard.</p>
<blockquote>
<p>"The biggest contribution of genetics to the social sciences is to
give researchers an additional set of tools to do basic research by
measuring and statistically controlling for a variable—DNA—that has
previously been very difficult to measure and statistically control
for." (page 192)</p>
</blockquote>
<p><a href="https://press.princeton.edu/books/hardcover/9780691190808/the-genetic-lottery" title="The Genetic Lottery: Why DNA Matters for Social Equality">The Genetic Lottery</a> has received criticism. Here's
<a href="https://www.lareviewofbooks.org/article/why-dna-is-no-key-to-social-equality-on-kathryn-paige-hardens-the-genetic-lottery" title="Why DNA Is No Key to Social Equality: On Kathryn Paige Harden’s “The Genetic Lottery”">Henn et al.</a>:</p>
<blockquote>
<p>"Ultimately, [Harden's] focus on genetics as a fundamental cause of
social inequality reduces her version of social justice to
benevolent paternalism."</p>
</blockquote>
<p>I think Harden does a fair job of being clear that genetics can be a
cause, but certainly not the only cause, and not a cause that can't be
redressed. On “benevolent paternalism,” I think Henn et al. intend the
phrase to have negative connotation, but couldn't any attempt at
social justice (or social safety nets) be referred to in this way?</p>
<p>As in the “equality vs. equity” <a href="https://interactioninstitute.org/illustrating-equality-vs-equity/">cartoon</a> on page 162, I take it as
a given that everyone should be able to see over the fence—the hard
question is how high the fence that we're trying to get everybody over
is. What's the level that should be guaranteed, and what are
exceptions for extreme cases?</p>
<p>Bird has <a href="https://massivesci.com/articles/genetic-lottery-review-paige-harden-kevin-bird/" title="The Genetic Lottery is a bust for both genetics and policy">issues</a> with the presentation of the science, and I agree
I would have liked more detail and precision in the presentation, but
Harden is also trying to reach a broad audience and cover a lot of
material. I think Bird is incorrect in his accusation that Harden
doesn't engage with the history of eugenics.</p>
<p>Harden's point that genetic confounds can affect optimal policy
recommendations seems meaningful to me. If parents with lots of books
in their home have kids who learn to read, does that imply giving
everyone a stack of books is all we need to do?</p>
<p>I think Harden is right that if the well-intentioned don't engage with
genetics, their impact is muted by confounding while the
ill-intentioned advertise “forbidden knowledge” to the benefit of
none. But I don't expect a major shift in Harden's lifetime.</p>
<p><img alt="cover" src="cover.jpg"></p>
<hr>
<blockquote>
<p>"Building a commitment to egalitarianism on our genetic uniformity
is building a house on sand." (page 19)</p>
</blockquote>
<hr>
<blockquote>
<p>"A study of what is correlated with succeeding in an education
system doesn't tell you whether that system is good, or fair, or
just." (page 60)</p>
</blockquote>
<hr>
<p>Quoting the organizers of the <a href="https://www.fragilefamilieschallenge.org/">Fragile Families Challenge</a>:</p>
<blockquote>
<p>“If one measures our degree of understanding by our ability to
predict, then results ... suggest that our understanding of child
development and the life course is actually quite poor.” (page 70)</p>
</blockquote>
<hr>
<blockquote>
<p>"I think we must dismantle the false distinction between
“inequalities that society is responsible for addressing” and
“inequalities that are caused by differences in biology.”" (page 91)</p>
</blockquote>
<hr>
<p><a href="https://www.nber.org/papers/w22595">Understanding and Misunderstanding Randomized Controlled Trials</a></p>
<hr>
<p>Interesting unit: <a href="https://en.wikipedia.org/wiki/Centimorgan">centimorgan</a></p>
<hr>
<p><a href="https://www.nature.com/articles/nrg2322">Heritability in the genomics era — concepts and misconceptions</a></p>
<blockquote>
<p>"half of the additive genetic variance is between families and half
is within families"</p>
</blockquote>
<hr>
<p><a href="https://www.nature.com/articles/456018a">Personal genomes: The case of the missing heritability</a></p>
<hr>
<p>Dang, I would like to see some worked examples for how heritabilities
are calculated...</p>
<hr>
<p><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2762790/">Individual Differences in Executive Functions Are Almost Entirely Genetic in Origin</a></p>
<hr>
<blockquote>
<p>"As Dostoevsky reminded us, “It takes something more than
intelligence to act intelligently.”" (page 141, referring to <em>Crime
and Punishment</em>)</p>
</blockquote>
<hr>
<p><a href="https://www.pnas.org/content/115/31/E7275">Genetic analysis of social-class mobility in five longitudinal studies</a></p>
<p>In its conclusion, a sentiment shared by Harding:</p>
<blockquote>
<p>"A long-term goal of our sociogenomic research is to use genetics to
reveal novel environmental intervention approaches to mitigating
socioeconomic disadvantage."</p>
</blockquote>
<hr>
<p><a href="https://pubmed.ncbi.nlm.nih.gov/27337136/">Genetically-mediated associations between measures of childhood character and academic achievement</a></p>
<p>In Figure 7.3 of the book, a list based on that ref:</p>
<ul>
<li>grit<ul>
<li>passion and perseverance for long-term goals</li>
</ul>
</li>
<li>growth mindset<ul>
<li>belief that intelligence is malleable</li>
</ul>
</li>
<li>intellectual curiosity<ul>
<li>desire to think about difficult or new problems</li>
</ul>
</li>
<li>mastery orientation<ul>
<li>motivation to learn for the sake of learning</li>
</ul>
</li>
<li>self-concept<ul>
<li>belief that one is smart and capable of learning</li>
</ul>
</li>
<li>test motivation<ul>
<li>trying hard on tests</li>
</ul>
</li>
</ul>
<blockquote>
<p>"The SNPs correlated with non-cognitive skills were correlated with
<em>higher</em> risk for several mental disorders, including schizophrenia,
bipolar disorder, anorexia nervosa, and obsessive-compulsive
disorder. This result warns us against viewing the genetic variants
that are associated with going further in current systems of formal
education as being inherently “good” things. A single genetic
variant might make it a tiny bit more likely that someone will go
further in school, but that same variant might also elevate their
risk of developing schizophrenia or another serious mental
disorder." (page 144)</p>
</blockquote>
<hr>
<blockquote>
<p>"Unfortunately, the mistaken idea that genetic influences are an
impermeable barrier to social change is also widely endorsed not
just by those who are trying to naturalize inequality, but also by
their ideological and political opponents." (page 155)</p>
</blockquote>
<hr>
<p><a href="https://pubmed.ncbi.nlm.nih.gov/27359131/">Strong genetic overlap between executive functions and intelligence</a></p>
<hr>
<blockquote>
<p>"I could quote the Bible verse from Thessalonians that was quoted to
me as a child: “The one who is unwilling to work shall not eat.”"
(page 212)</p>
</blockquote>
<hr>
<blockquote>
<p>"There is no measure of so-called “merit” that is somehow free of
genetic influence or untethered from biology." (page 247)</p>
</blockquote>    
    ]]></description>
<link>http://planspace.org/20211224-genetic_lottery_harden/</link>
<guid>http://planspace.org/20211224-genetic_lottery_harden/</guid>
<pubDate>Fri, 24 Dec 2021 12:00:00 -0500</pubDate>
</item>
<item>
<title>Correlation can help approximate variance of a difference</title>
<description><![CDATA[

<p>Even if random variables \( \textbf{A} \) and \( \textbf{δ} \) are
uncorrelated, realized data \( A \) and \( \delta \) will happen
to have some covariance. If \( B = A + \delta \), then \(
\text{var}(B) = \text{var}(A) + \text{var}(\delta) + 2\text{cov}(A,
\delta) \), and estimating any one variance by the others will be off
by the \( 2\text{cov}(A, \delta) \) term. For \( \text{var}(\delta)
\), however, the estimate \( \text{var}(B)(1 - \text{corr}(A, B)^2)
\) will usually be closer to correct, at the cost of being slightly
biased. (But don't forget that doing \( \text{var}( \delta ) \)
directly is a great first choice.)</p>
<h3>Three methods for \( \text{var}( \delta ) \), when \( B = A + \delta \)</h3>
<p>The case of \( B = A + \delta \) (so that \( \delta = B - A \)) is
central to the paired t-test, for example. The variance of \( A \)
and \( B \) could each be large, but the variance of \( \delta \)
can still be small, making it easier to reject the null for \(
\textbf{δ} \), for example.</p>
<p><em>Be careful: We're trying to estimate a variance, so we're interested
in the variance of the estimate of the variance, which can be
confusing. Hopefully the language is clear enough here.</em></p>
<h4>1) Variance of Differences</h4>
<p>The obvious thing to do is to subtract and calculate the variance of
the differences: \( \text{var}( \delta ) = \text{var}( B - A ) \).
This is a good idea and what you should do. It's unbiased and has low
variance (for the estimate of the true variance of \( \textbf{δ}
\)).</p>
<p>Why not do it like this? Honestly I'm not sure. Maybe you don't have
complete data, and you're looking for a \( \delta_1 - \delta_2 \)
where the means of \( A_1 \) and \( A_2 \) are assumed equal so
you're using \( B_1 - B_2 \) but you want the (smaller) variance of
\( \delta_1 - \delta_2 \)? So you'll estimate some things with
complete cases even though the main effect is estimated with all \( B
\) data? Why not still use this method with the complete cases? Maybe
you're not using simple subtraction, but some more complex regression
with multiple variables? In that case, why not use variance of the
residuals directly? Do you just want a more complicated method?</p>
<h4>2) Difference of Variances</h4>
<p>By the <a href="/20201030-the_variance_sum_law_is_interesting/">variance sum law</a>, \( \text{var}( \textbf{B} ) =
\text{var}( \textbf{A} ) + \text{var}( \textbf{δ} ) \), if \(
\textbf{A} \) and \( \textbf{δ} \) are uncorrelated, so \(
\text{var}( \textbf{δ} ) = \text{var}( \textbf{B} ) - \text{var}(
\textbf{A} ) \). Real data is not generally perfectly uncorrelated,
however: \( \text{var}(\delta) = \text{var}(B) - \text{var}(A) -
2\text{cov}(A, \delta) \). The covariance term is zero in
expectation, so estimating \( \text{var}(\delta) = \text{var}(B) -
\text{var}(A) \) is unbiased. But the \( 2\text{cov}(A, \delta) \)
is a kind of noise term, and adds variance to the estimate of \(
\text{var}( \textbf{δ} )\).</p>
<h4>3) Using Correlation</h4>
<p>It isn't obvious, but using \( \text{var}(B)(1 - \text{corr}(A, B)^2)
\) to estimate \( \text{var}( \textbf{δ} ) \) is much like using
the Difference of Variances, but with a generally smaller (but always
positive) error coming from \( \text{cov}(A, \delta) \). Proceeding
in small steps:</p>
<p>\[ \text{var}(\delta) = \text{var}(B) - \text{var}(A) - 2\text{cov}(A, \delta) \]</p>
<p>\[ \text{var}(\delta) = \text{var}(B) - \left( \text{var}(A) + 2\text{cov}(A, \delta) \right) \]</p>
<p>\[ \text{var}(\delta) = \text{var}(B) \left( 1 - \frac{\text{var}(A) + 2\text{cov}(A, \delta)}{ \text{var}(B) } \right) \]</p>
<p>\[ \text{var}(\delta) = \text{var}(B) \left( 1 - \frac{\text{var}(A)^2 + 2\text{var}(A)\text{cov}(A, \delta)}{ \text{var}(A) \text{var}(B) } \right) \]</p>
<p>This has been algebraic. Now add \( \text{cov}(A, \delta)^2 \) to
the fractional term's numerator. (This is an error in the estimate, to
get to the destination.)</p>
<p>\[ \text{var}(\delta) = \text{var}(B) \left( 1 - \frac{\text{var}(A)^2 + 2\text{var}(A)\text{cov}(A, \delta) + \text{cov}(A, \delta)^2 }{ \text{var}(A) \text{var}(B) } \right) \]</p>
<p>\[ \text{var}(\delta) = \text{var}(B) \left( 1 - \frac{ ( \text{var}(A) + \text{cov}(A, \delta))^2 }{ \text{var}(A) \text{var}(B) } \right) \]</p>
<p>The bit squared in the fraction's numerator is \( \sum{(A_i - \bar{A}
)^2} + \sum{(A_i - \bar{A} )(\delta_i - \bar{\delta})} \), which is
\( \sum{(A_i - \bar{A})(A_i - \bar{A} + \delta_i - \bar{\delta})}
\). Since \( B_i = A_i + \delta_i \) and \( \bar{B} = \bar{A} +
\bar{\delta} \), that's \( \sum{(A_i - \bar{A})(B_i - \bar{B})} \),
which is \( \text{cov}(A, B) \).</p>
<p>\[ \text{var}(\delta) = \text{var}(B) \left( 1 - \frac{ \text{cov}(A, B)^2 }{ \text{var}(A) \text{var}(B) } \right) \]</p>
<p>So at last, by definition:</p>
<p>\[ \text{var}(\delta) = \text{var}(B) \left( 1 - \text{corr}(A, B)^2 \right) \]</p>
<p>QED. The error introduced is \( \text{cov}(A, \delta)^2 /
\text{var}(A) \), which will tend to be considerably smaller in
absolute value than the \( 2\text{cov}(A, \delta) \) error in the
Difference of Variances method because \( 0 \le |\text{cov}(A,
\delta)| / \text{var}(A) \ll 2 \). This error is however positive in
expectation, so the estimate is biased: it will be slightly too small.</p>
<h3>Computational demonstration</h3>
<p>Generating 1,000 datasets where each of \( A \) and \( \delta \)
have 100 values drawn at random from Gaussians with variance 9 and 16,
respectively, the three methods above generate estimates of \(
\text{var}( \textbf{δ} ) \) as follows:</p>
<pre><code>| Method                  | Mean var(d) estimate | var(estimate) |
|-------------------------|----------------------|---------------|
| Variance of Differences |                15.98 |          5.10 |
| Difference of Variances |                15.95 |         10.69 |
| Using Correlation       |                15.82 |          5.12 |</code></pre>

<p>As expected, the method Using Correlation comes out more below the
true value of 16 on average, but its variance is comparable to that of
the Variance of Differences.</p>
<p>The bias is already small with just 100 samples, and gets smaller
still as samples get bigger and sample correlations tend to be
smaller.</p>
<p>Here's clumsy <a href="https://www.r-project.org/">R</a> code to do the experiment:</p>
<pre><code class="language-r">trials = 1000
samples = 100

set.seed(0)
est1 = c()
est2 = c()
est3 = c()

for (i in 1:trials) {
  A = rnorm(samples, sd=3)  # var=9
  d = rnorm(samples, sd=4)  # var=16
  B = A + d  # var=25, in population sense
  est1 = c(est1, var(B - A))  # same as var(d)
  est2 = c(est2, var(B) - var(A))
  est3 = c(est3, var(B) * (1 - cor(A, B)^2))
}

mean(est1)
## [1] 15.98799
mean(est2)
## [1] 15.9451
mean(est3)
## [1] 15.82234
var(est1)
## [1] 5.09602
var(est2)
## [1] 10.69393
var(est3)
## [1] 5.117983</code></pre>
<!-- mathjax for formulas -->

    ]]></description>
<link>http://planspace.org/20211219-correlation_can_help_approximate_variance_of_a_difference/</link>
<guid>http://planspace.org/20211219-correlation_can_help_approximate_variance_of_a_difference/</guid>
<pubDate>Sun, 19 Dec 2021 12:00:00 -0500</pubDate>
</item>
<item>
<title>Practical Data Science with Python, by George</title>
<description><![CDATA[

<p>Packt sent me a copy of this <a href="https://www.packtpub.com/product/practical-data-science-with-python/9781801071970">book</a> to review. It's 621 pages with
the best and worst characteristics of a typical data science boot
camp: very broad, necessarily shallow, frequently not quite perfect.
Even an imperfect map can tell you a lot about the territory, and it
could be the right book for you.</p>
<p>George has pulled together a lot of material, some of it good. He
includes introductory Python and command line, enough SQL to be
confused about SQL, examples with Bitcoin prices, an idiosyncratic
survey of visualization, web scraping, statistics, and the big machine
learning models, including the <a href="/20211003-xgboost_lightgbm_catboost_briefly/">big three boosted tree algorithms</a>,
which I appreciate. He includes some NLP, and even some on ethics.</p>
<p>George's own list of omissions (page 571) illustrates what he thinks
is almost in scope:</p>
<ul>
<li>Recommender systems</li>
<li>Networks and graph analysis</li>
<li>Machine learning explainability</li>
<li>Test-driven development (TDD)</li>
<li>Reinforcement learning</li>
<li>Neural networks</li>
</ul>
<p>Maybe the moral is that “data science” is too big a topic for one
book. Trying to pack so much in has a cost. Here's the complete
section on “Paired t- and z-tests”:</p>
<blockquote>
<p>One last type of t- or z-test is the paired test. This is for paired
samples, like before-and-after treatments. For example, we could
measure the blood pressure of people before and after taking a
medication to see if there is an effect. A function that can be used
for this is <code>scipy.stats.ttest_rel</code>, which can be used like this:</p>
<p><code>scipy.stats.ttest_rel(before, after)</code></p>
<p>This will return a t-statistic and p-value like with other <code>scipy</code>
t-test functions.</p>
</blockquote>
<p>If you've never heard of a paired t-test before, it's great this book
tells you about it. You can start to ask questions like: Why is this a
separate test? Does it have some advantage over a regular t-test?
Hopefully you also question some parts of the book, as when Bayesian
methods are dismissed as “much more complex to implement than a
t-test.”</p>
<p>This is a map that can point you in a lot of interesting directions,
which is valuable!</p>
<p><img alt="cover" src="cover.png"></p>    
    ]]></description>
<link>http://planspace.org/20211213-practical_data_science_with_python_by_george/</link>
<guid>http://planspace.org/20211213-practical_data_science_with_python_by_george/</guid>
<pubDate>Mon, 13 Dec 2021 12:00:00 -0500</pubDate>
</item>
<item>
<title>Significant Figures: Rounding destroys information</title>
<description><![CDATA[

<p>Significant Figures often requires rounding so that the correct level
of uncertainty is conveyed. Rounding is only supposed to happen at the
“end” of calculations, and results can vary depending on what is
considered an intermediate vs. a final result. In all cases, rounding
is harmful and only necessary because of the limitations of
Significant Figures.</p>
<p>It should be clear that 4.3 with an <a href="/20211209-significant_figures_gaussian_uncertainty/">uncertainty</a> \( \sigma \times
0.1 \) is not the same as 4.34 with the same uncertainty. It's
because Significant Digits can't say 4.34 without meaning the
uncertainty is \( \sigma \times 0.01 \) that we round to 4.3, if the
uncertainty is \( \sigma \times 0.1 \). But this has a cost: good
information is dropped.</p>
<p>Consider adding 1 + 1.4 + 1.4. If we do it in one go, we get 3.8 and
then round to 4 for significance. But what if this is done in two
steps? Maybe one team does 1 + 1.4, correctly reports their result as
2, and then a second team builds on that, adding 1.4 to get 3. The
rounding that Significant Figures requires degrades the quality of
results.</p>
<p>At some point it isn't worth tracking every digit in a result, but
Significant Figures often encourages dropping too many. It may even
give people the incorrect idea that if our uncertainty suggests two
significant figures, we can't have three figures in our best guess at
what the value is. We absolutely can, but this requires a system more
expressive than Significant Figures to report.</p>
<!-- mathjax for formulas -->

    ]]></description>
<link>http://planspace.org/20211212-significant_figures_rounding_destroys_information/</link>
<guid>http://planspace.org/20211212-significant_figures_rounding_destroys_information/</guid>
<pubDate>Sun, 12 Dec 2021 12:00:00 -0500</pubDate>
</item>
<item>
<title>“Show your work” means “Write a proof”</title>
<description><![CDATA[

<p>At any level of mathematics, “showing your work” should not be a
chore, but a chance to communicate an argument for correctness: to
write a proof. “Work” as “proof” keeps mathematics relevant,
authentic, and interesting.</p>
<p>Many school math exercises can be answered with calculators. But how
do we know the calculator is correct? The difference between inductive
(“the calculator has always been right”) and deductive (“this answer
is proven correct”) is important. The answer itself isn't important;
it's the demonstration that the answer follows from the question.</p>
<p>High school geometry is sometimes pointed to as the first place
students encounter the idea of proofs. This need not be. Arithmetic is
a process of proof, using theorems of single-digit operations and
place-based algorithms to build new results. In this way, 2+2=4 is
used to demonstrate that 22+22=44, and so on. These could be called
constructive proofs.</p>
<p>“Work” as “proof” opens up the world of math. “Checking your work” is
finding a second proof. For the advanced student who might have been
bored with “showing work,” there is an invitation to find further
proofs, understand algorithms more deeply, and be creative, rather
than being limited to mechanical processes.</p>
<p>Computational fluency is not without value, but math is about much
more. We waste opportunities to deliver on teaching mathematical ways
of thinking if students don't realize that they're constructing
logical arguments in all their math classes.</p>    
    ]]></description>
<link>http://planspace.org/20211212-show_your_work_means_write_a_proof/</link>
<guid>http://planspace.org/20211212-show_your_work_means_write_a_proof/</guid>
<pubDate>Sun, 12 Dec 2021 12:00:00 -0500</pubDate>
</item>
<item>
<title>Bernoulli's Fallacy, by Clayton</title>
<description><![CDATA[

<p><a href="https://aubreyclayton.com/bernoulli"><em>Bernoulli's Fallacy</em></a> is that the likelihood of data given a
hypothesis is enough to make inferences about that hypothesis (or
others). Clayton covers historical and modern aspects of frequentist
statistics, and lays crises of replication at the feet of significance
testing. I find it largely compelling, though perhaps it neglects
problematic contributions of non-statistical pressures in modern
academic life. I'm generally on board re: Bayesian methods.</p>
<p><img alt="cover" src="cover.jpg"></p>
<hr>
<blockquote>
<p>"...statistical methods are a means of accounting for the epistemic
role of measurement error and uncertainty..." (page x)</p>
</blockquote>
<hr>
<blockquote>
<p>"...an effect—if it <em>is</em> found—is likely overstated and unlikely to
be replicable, a paradox known as the “<a href="https://en.wikipedia.org/wiki/Winner%27s_curse">winner's curse</a>.”" (page
xi)</p>
</blockquote>
<hr>
<blockquote>
<p>The common theme is that there would be no need to continually treat
the symptoms of statistical misuse if the underlying disease were
addressed.</p>
<p>I offer the following for consideration:</p>
<p>Hypothesizing after the results of an experiment are known does not
necessarily present a problem and in fact is the way that most
hypotheses are ever constructed.</p>
<p>No penalty need be paid, or correction made, for testing multiple
hypotheses at once using the same data.</p>
<p>The conditions causing an experiment to be terminated are largely
immaterial to the inferences drawn from it. In particular, an
experimenter is free to keep conducting trials until achieving a
desired result, with no harm to the resulting inferences.</p>
<p>No special care is required to avoid “overfitting” a model to the
data, and validating the model against a separate set of test data
is generally a waste.</p>
<p>No corrections need to be made to statistical estimators (such as
the sample variance as an estimate of population variance) to ensure
they are “unbiased.” In fact, by doing so the quality of those
estimators may be made worse.</p>
<p>It is impossible to “measure” a probability by experimentation.
Furthermore, all statements that begin “The probability is ...”
commit a category mistake. There is no such thing as “objective”
probability.</p>
<p>Extremely improbably events are not necessarily noteworthy or reason
to call into question whatever assumed hypotheses implied they were
improbable in the first place.</p>
<p>Statistical methods requiring an assumption of a particular
distribution (for example, the normal distribution) for the error in
measurement are perfectly valid whether or not the data “actually
is” normally distributed.</p>
<p>It makes no sense to talk about whether data “actually is” normally
distributed or could have been sampled from a normally distributed
population, or any other such consideration.</p>
<p>There is no need to memorize a complex menagerie of different tests
or estimators to apply to different kinds of problems with different
distributional assumptions. Fundamentally, all statistical problems
are the same.</p>
<p>“Rejecting” or “accepting” a hypothesis is not the proper function
of statistics and is, in fact, dangerously misleading and
destructive.</p>
<p>The point of statistical inference is not to produce the right
answers with high frequency, but rather to <em>always</em> produce the
inferences best supported by the data at hand when combined with
existing background knowledge and assumptions.</p>
<p>Science is largely not a process of falsifying claims definitively,
but rather assigning them probabilities and updating those
probabilities in light of observation. This process is endless. No
proposition apart from a logical contradiction should ever get
assigned probability 0, and nothing short of a logical tautology
should get probability 1.</p>
<p>The more unexpected, surprising, or contrary to established theory a
proposition seems, the more impressive the evidence must be before
that proposition is taken seriously.</p>
</blockquote>
<hr>
<p>Heavily influenced by <a href="https://www.cambridge.org/core/books/probability-theory/9CA08E224FF30123304E6D8935CF1A99">Probability Theory: The Logic of Science</a> by
Edwin Jaynes.</p>
<hr>
<blockquote>
<p>"We can, for example, set <em>s</em> = 0.01 and by 99 percent sure.
Bernoulli called this “moral certainty,” as distinct from absolute
certainty of the kind only logical deduction can provide." (page 8)</p>
</blockquote>
<hr>
<blockquote>
<p>"<em>statistics is both much easier and much harder than we have been
led to believe.</em>" (page 17, italics in original)</p>
</blockquote>
<hr>
<blockquote>
<p>"Aristotle's <em>Rhetoric</em> described “the Probably” (in Greek, <em>eikos</em>,
from <em>eoika</em> meaning “to seem”) as “that which happens generally but
not invariably.” The context for this was his classification of the
arguments one could use in a courtroom or legislative debate, where
perfect logical deductions may not be available. He called this form
of argument an <em>enthymeme</em>, to be distinguished from the purely
logical form of argument known as the <em>syllogism</em>, which links
together a set of assumed premises to reach deductive
conclusions..." (page 22)</p>
</blockquote>
<hr>
<blockquote>
<p>"Hume's general point
[in <em>An Enquiry Concerning Human Understanding</em>], later referred to
as the <em>problem of induction</em>, was that we have no way of <em>knowing</em>
experience is a guide for valid conclusions about the future because
if we did, that claim could be based only on past experience." (page
35)</p>
</blockquote>
<hr>
<p>I kind of like the names Clayton uses in his tables of calculations:
"Prior probability" is normal, then "Sampling probability" is used for
the likelihood of the data, and then he multiplies them together to
get a "Pathway probability."</p>
<hr>
<blockquote>
<p>"Whether Bayes himself believed he had disproved Hume we have no way
of knowing. Some historians such as Stephen Stigler at the
University of Chicago have suggested that since Bayes did not find
the counterexample sufficiently convincing because it relied on some
assumptions he could not justify, he delayed publishing his results.
When presenting Bayes's results to the world, Price did not shy away
from emphasizing their philosophical and religious significance.
Contemporary reprints of the essay show Price intended the title to
be “A Method of Calculating the Exact Probability of All Conclusions
founded on Induction.” In his publication, he added this preamble:
“The purpose I mean is, to shew what reason we have for believing
that there are in the constitution of things fixt laws according to
which things happen, and that, therefore, the frame of the world
must be the effect of the wisdom and power of an intelligent cause;
and thus to confirm the argument taken from final causes for the
existence of the Deity.” That is, somewhere in the calculation of
probabilities for Bayes's rule, Price thought he saw evidence for
God." (page 41)</p>
</blockquote>
<hr>
<blockquote>
<p>"... logical deduction is just a special case of reasoning with
probabilities, in which all the probability values are zeros or
ones." (page 53)</p>
</blockquote>
<hr>
<blockquote>
<p>"Jaynes's essential point bears repeating: <em>probability is about
information.</em>" (page 68, italics in original)</p>
</blockquote>
<hr>
<blockquote>
<p>"Base rate neglect and the prosecutor's fallacy are the same thing,
and both are examples of Bernoulli's Fallacy." (page 103)</p>
</blockquote>
<hr>
<blockquote>
<p>"... a new general trend of collecting data in service to the social
good. John Graunt, haberdasher by day and demographer by night, had
made a breakthrough in London in 1662 when he used weekly mortality
records to design an early warning system to detect outbreaks of
bubonic plague in the city. Even though the system was never
actually deployed, it opened people's eyes to the rich possibilities
of data gathering and its usefulness to the state. By the 1740s,
prominent thinkers such as the German philosopher Gottfried
Achenwall had taken to calling this kind of data <em>statistics</em>
(<em>statistik</em> in German), the root of which is the Latin word
<em>statisticum</em> meaning “of the state.”" (page 109)</p>
</blockquote>
<hr>
<blockquote>
<p>"His [Quetelet's] goal, perhaps antagonized by the Baron de
Keverberg's skepticism, was to investigate analytically all the ways
people <em>were</em> the same or different and to create a theory of
<em>social physics</em>, a set of laws governing society that could be an
equivalent of Kepler's laws of planetary motion and other immutable
principles of the hard sciences." (page 113)</p>
</blockquote>
<p>This reminds me of <a href="/20200714-foundation_trilogy/">psychohistory</a>.</p>
<hr>
<blockquote>
<p>"George Pólya gave it the lofty name the <em>central limit theorem</em>"
(page 120)</p>
</blockquote>
<p>Huh!</p>
<hr>
<blockquote>
<p>"He [Quetelet] would later be harshly ridiculed for his love of the
normal distribution by statisticians like Francis Edgeworth, who
wrote in 1922: “The theory [of errors] is to be distinguished from
the doctrine, the false doctrine, that generally, wherever there is
a curve with a single apex representing a group of statistics ...
that the curve must be of the ‘normal’ species. The doctrine has
been nick-named ‘Quetelismus,’ on the ground that Quetelet
exaggerated the prevalence of the normal law.”" (page 122)</p>
</blockquote>
<hr>
<p>Interesting/weird idea from Galton: "statistics by intercomparison."
If you can only order people on some characteristic (say
intelligence), then do that and then assume it's quantitatively
normal. Sort of like QQ plots. Sort of. (page 136)</p>
<hr>
<p>On page 142 it seems to be saying that Pearson's chi-squared is for
general testing of whether data comes from a certain distribution...
Is that right? Does this just mean binning out data and comparing
counts to expected? Maybe that's it?</p>
<hr>
<blockquote>
<p>"For an experimental scientist without advanced mathematical
training, the book
[Fisher's Statistical Methods for Research Workers] was a godsend.
All such a person had to do was find the procedure corresponding to
their problem and follow the instructions." (page 153)</p>
</blockquote>
<hr>
<blockquote>
<p>"He [Fisher] proved what he called the <a href="https://en.wikipedia.org/wiki/Fisher%27s_fundamental_theorem_of_natural_selection">fundamental theorem</a> of
natural selection: “The rate of increase in fitness of any organism
at any time is equal to its genetic variance in fitness at that
time.”"</p>
</blockquote>
<p>Is this in conflict with Fisher as eugenicist? It seems to be
pro-diversity? At least some kinds of diversity...</p>
<hr>
<p>Interesting comparison between choosing one- or two-sided testing, and
Bayesian priors: you're not really bringing zero information to the
problem.</p>
<hr>
<p>Huh - there really is a <a href="https://www.socscistatistics.com/tests/what_stats_test_wizard.aspx">Social Science Statistics online wizard</a>.</p>
<hr>
<blockquote>
<p>"There is no coherent theory to orthodox statistics, only a loose
amalgam of half-baked ideas held together by suggestive naming,
catchy slogans, and folk superstition." (page 196)</p>
</blockquote>
<hr>
<blockquote>
<p>"As Fisher wrote in <em>Statistical Methods for Research Workers</em>, “No
human mind is capable of grasping in its entirety the meaning of any
considerable quantity of numerical data. We want to be able to
express all the relevant information contained in the mass by means
of comparatively few numerical values. This is a purely practical
need which the science of statistics is able to some extent to
meet." (page 233)</p>
</blockquote>
<hr>
<p><a href="http://www.psych.ualberta.ca/~rozeboom/files/1960_The_fallacy_of_the_null_hypothesis_significance_test.pdf">The Fallacy of the Null-Hypothesis Significance Test</a></p>
<hr>
<p><a href="https://www.press.umich.edu/186351/cult_of_statistical_significance">The Cult of Statistical Significance</a></p>
<hr>
<blockquote>
<p>"Harold Jeffreys first proposed the idea of Bayes factors in his
<em>Theory of Probability</em>." (page 262)</p>
</blockquote>
<hr>
<p>Daryl Bem (who published in support of psi) amusingly wrote (quoted
page 264):</p>
<blockquote>
<p>To compensate for this remoteness from our participants, let us at
least become intimately familiar with the record of their behavior:
the data. Examine them from every angle. Analyze the sexes
separately. Make up new composite indexes. If a datum suggests a new
hypothesis, try to find further evidence for it elsewhere in the
data. If you see dim traces of interesting patterns, try to
reorganize the data to bring them into bolder relief. If there are
participants you don't like, or trials, observers, or interviewers
who gave you anomalous results, place them aside temporarily and see
if any coherent patterns emerge. Go on a fishing expedition for
something—anything—interesting.</p>
</blockquote>
<p>That's from <a href="https://psychology.yale.edu/sites/default/files/bemempirical.pdf">Writing the Empirical Journal Article</a>.</p>
<hr>
<p><a href="https://amstat.tandfonline.com/doi/full/10.1080/00031305.2016.1154108#.YbT7Nn1KiVK">The ASA Statement on p-Values: Context, Process, and Purpose</a></p>
<hr>
<p><a href="https://www.tandfonline.com/toc/utas20/73/sup1">Statistical Inference in the 21st Century: A World Beyond p &lt; 0.05</a></p>
<hr>
<blockquote>
<p>"Significance testing was always based on a classification of
results into significant/insignificant without regard to effect size
or importance; no attempts to rehabilitate it now can change that
fundamental aspect nor repair the damage significance testing has
already caused. This yes/no binary has well and truly mixed things
up." (page 275)</p>
</blockquote>
<hr>
<blockquote>
<p>"The better, more complete interpretation of probability is that it
measures the <em>plausibility</em> of a proposition given some assumed
<em>information</em>. This extends the notion of deductive reasoning—in
which a proposition is derivable as a logical consequence of a set
of premises—to situations of incomplete information, where the
proposition is made more or less plausible, depending on what is
assumed to be known." (page 283)</p>
</blockquote>
<hr>
<blockquote>
<p>"All probability is conditional." (page 284)</p>
</blockquote>
<hr>
<blockquote>
<p>"Once we jettison the bureaucracy of frequentist statistics, we can
spend more time doing actual science." (page 287)</p>
</blockquote>
<hr>
<blockquote>
<p>"Getting rid of the useless concepts (significance testing,
estimators, sufficient and ancillary statistics, stochastic
processes) will amount to cutting out probably 90 percent of the
standard statistics curriculum. It might even mean giving up on
statistics as a separate academic discipline altogether, but that's
alright. Probability as a topic should rightfully split time between
its parents, math and philosophy, the way logic does. Bayesian
statistical inference contains exactly one theorem of importance
anyway, and its practical techniques can be taught in a single
semester-long course in applied math. There needn't be a whole
university department dedicated to it, any more than there needs to
be a department of the quadratic formula." (page 287)</p>
</blockquote>
<hr>
<blockquote>
<p>"We should no more be teaching <em>p</em>-values in statistics courses than
we should be teaching phrenology in medical schools." (page 293)</p>
</blockquote>
<hr>
<blockquote>
<p>"Joseph Berkson called this the “interocular traumatic test”; you
know what the data means when the conclusion hits you right between
the eyes." (page 297)</p>
</blockquote>
<p>That's quoted from "Bayesian statistical inference for psychological
research."</p>
<p>That source cites "J. Berkson, personal communication, July 14, 1958"
and goes on:</p>
<blockquote>
<p>"The interocular traumatic test is simple, commands general
agreement, and is often applicable; well-conducted experiments often
come out that way. But the enthusiast's interocular trauma may be
the skeptic's random error. A little arithmetic to verify the extent
of the trauma can yield great peace of mind for little cost." (page
217)</p>
</blockquote>
<hr>
<blockquote>
<p>"The results of experiments, particularly surprising or
controversial ones, can be trusted noly if the experiments are known
to be sound; however, as is often the case, an experiment is <em>known</em>
to be sound only if it produces the results we expect. So it would
seem that no experiment can ever convince us of something
surprising. This situation was anticipated by the ancient Greek
philosopher Sextus Empiricus. In a skepticism of induction that
predated David Hume's by 1,500 years, he wrote: “If they shall judge
the intellects by the senses, and the senses by the intellect, this
involves circular reasoning inasmuch as it is required that the
intellects should be judged first in order that the sense may be
judged, and the senses be first scrutinized in order that the
intellects may be tested [hence] we possess no means by which to
judge objects.”" (page 301)</p>
</blockquote>    
    ]]></description>
<link>http://planspace.org/20211211-bernoullis_fallacy_by_clayton/</link>
<guid>http://planspace.org/20211211-bernoullis_fallacy_by_clayton/</guid>
<pubDate>Sat, 11 Dec 2021 12:00:00 -0500</pubDate>
</item>
<item>
<title>Significant Figures: Addition is too precise</title>
<description><![CDATA[

<p>The usual <a href="https://en.wikipedia.org/wiki/Significance_arithmetic#Addition_and_subtraction_using_significance_arithmetic" title="Wikipedia: Addition and subtraction using significance arithmetic">rules</a> for adding and subtracting numbers with
significant digits often propagate uncertainly approximately
correctly, but always give results more precise than they actually are
because they don't (and can't, generally) follow the
<a href="/20201030-the_variance_sum_law_is_interesting/" title="The Variance Sum Law is Interesting">Variance Sum Law</a>.</p>
<p>Say a <a href="/20211209-significant_figures_gaussian_uncertainty/" title="Significant Figures: Gaussian uncertainty, σ=2.5eN">number</a> in Significant Figures with rightmost significant
digit \( D \times 10^N \) has uncertainty with standard deviation
\( \sigma \times 10^N \), and assume errors are always uncorrelated.</p>
<p>So the number 12.3, with three significant figures, has uncertainty
\( \sigma \times 0.1 \), and 2.48 has \( \sigma \times 0.01 \).
Adding them gives 14.8, which has the same uncertainty as 12.3. By the
<a href="/20201030-the_variance_sum_law_is_interesting/" title="The Variance Sum Law is Interesting">Variance Sum Law</a>, the true uncertainty is \( \sigma \times 0.1005
\), but that's pretty close to \( \sigma \times 0.1 \). In this
way, the usual rule for adding with significant figures is often
reasonable-seeming.</p>
<p>With many numbers of the same precision, however, the usual rules are
more problematic. If you add 1.2 + 3.4 + 5.6 + 7.8, the result 18.0
implies \( \sigma \times 0.1 \), but in fact uncertainty has doubled
to \( \sigma \times 0.2 \). Significant Figures has no way to convey
this, because it only communicates in powers of ten.</p>
<p>Adding and subtracting 100 numbers with the same precision, then,
should give a result with exactly one fewer significant figures. With
25 numbers the standard deviation could “round up” to the next power
of ten, arguably. It may not be common to add so many numbers with
significant figures, but even with just a few, Sig Figs is a course
approximation of correct propagation of uncertainty.</p>
<!-- mathjax for formulas -->

    ]]></description>
<link>http://planspace.org/20211211-significant_figures_addition_is_too_precise/</link>
<guid>http://planspace.org/20211211-significant_figures_addition_is_too_precise/</guid>
<pubDate>Sat, 11 Dec 2021 12:00:00 -0500</pubDate>
</item>
<item>
<title>Significant Figures: Gaussian uncertainty, σ=2.5eN</title>
<description><![CDATA[

<p>Significant Figures (or Significant Digits) is a tool for an important
task: tracking precision. Ironically, the meaning of a number with
Significant Figures is often not defined precisely. I propose that if
a number's rightmost significant digit is \( D \times 10^N \), this
means uncertainty is Gaussian with standard deviation \( \sigma = 2.5
\times 10^N \).</p>
<p><img alt="" src="gaussian.png"></p>
<p>The final digit is “<a href="https://chem.libretexts.org/Courses/University_of_British_Columbia/CHEM_100%3A_Foundations_of_Chemistry/02%3A_Measurement_and_Problem_Solving/2.03%3A_Significant_Figures_-_Writing_Numbers_to_Reflect_Precision">significant but not certain</a>” — which shouldn't
mean we know <em>nothing</em> about what the digit is likely to be, and
shouldn't mean that there's no chance the neighboring digit could be
wrong. A purely digit-based interpretation is unnatural.</p>
<p><img alt="" src="uniform.png"></p>
<p>A particular instrument (or other source) may generate measurements
with a different distribution of uncertainty. In such cases, that
information should be captured specifically, and Significant Figures
alone is not sufficient. For measurements where there isn't more
specific information, a Gaussian distribution is a good choice.</p>
<p>Even when given a precise interpretation, Significant Figures is a
system tied to base ten numbers and manual measurement that asks a
single number to convey both value and uncertainty. There's only so
much one number can mean. Sig Figs is better than not tracking
precision at all, but better yet is to be explicit about precision.</p>
<hr>
<p>I would love to find other sources or interpretations that agree or
disagree with this definition. So far I haven't found anything
explicit enough to really compare one way or another. References (and
feedback of any kind) are especially welcome here!</p>
<p>Visualization code is <a href="https://github.com/ajschumacher/sigfigs">on GitHub</a>.</p>
<!-- mathjax for formulas -->

    ]]></description>
<link>http://planspace.org/20211209-significant_figures_gaussian_uncertainty/</link>
<guid>http://planspace.org/20211209-significant_figures_gaussian_uncertainty/</guid>
<pubDate>Thu, 09 Dec 2021 12:00:00 -0500</pubDate>
</item>
<item>
<title>Try HoloViz for Python plotting</title>
<description><![CDATA[

<p>Yang <a href="https://sophiamyang.github.io/DS/visualization/holoviz/holoviz.html" title="Visualization and Interactive Dashboard in Python">inspired</a> me to check out <a href="https://holoviz.org/" title="High-level tools to simplify visualization in Python">HoloViz</a>, which makes it easy,
among other things, to do interactive scatterplots in Jupyter
Notebooks—and stick them in any HTML!</p>
<p><link rel="stylesheet" href="https://unpkg.com/@holoviz/panel@0.12.5/dist/css/alerts.css" type="text/css">
<link rel="stylesheet" href="https://unpkg.com/@holoviz/panel@0.12.5/dist/css/card.css" type="text/css">
<link rel="stylesheet" href="https://unpkg.com/@holoviz/panel@0.12.5/dist/css/widgets.css" type="text/css">
<link rel="stylesheet" href="https://unpkg.com/@holoviz/panel@0.12.5/dist/css/markdown.css" type="text/css">
<link rel="stylesheet" href="https://unpkg.com/@holoviz/panel@0.12.5/dist/css/json.css" type="text/css">
<link rel="stylesheet" href="https://unpkg.com/@holoviz/panel@0.12.5/dist/css/loading.css" type="text/css">
<link rel="stylesheet" href="https://unpkg.com/@holoviz/panel@0.12.5/dist/css/dataframe.css" type="text/css"></p>
<style>
                    .bk.pn-loading.arcs:before {
                      background-image: url("data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiBzdHlsZT0ibWFyZ2luOiBhdXRvOyBiYWNrZ3JvdW5kOiBub25lOyBkaXNwbGF5OiBibG9jazsgc2hhcGUtcmVuZGVyaW5nOiBhdXRvOyIgdmlld0JveD0iMCAwIDEwMCAxMDAiIHByZXNlcnZlQXNwZWN0UmF0aW89InhNaWRZTWlkIj4gIDxjaXJjbGUgY3g9IjUwIiBjeT0iNTAiIHI9IjMyIiBzdHJva2Utd2lkdGg9IjgiIHN0cm9rZT0iI2MzYzNjMyIgc3Ryb2tlLWRhc2hhcnJheT0iNTAuMjY1NDgyNDU3NDM2NjkgNTAuMjY1NDgyNDU3NDM2NjkiIGZpbGw9Im5vbmUiIHN0cm9rZS1saW5lY2FwPSJyb3VuZCI+ICAgIDxhbmltYXRlVHJhbnNmb3JtIGF0dHJpYnV0ZU5hbWU9InRyYW5zZm9ybSIgdHlwZT0icm90YXRlIiByZXBlYXRDb3VudD0iaW5kZWZpbml0ZSIgZHVyPSIxcyIga2V5VGltZXM9IjA7MSIgdmFsdWVzPSIwIDUwIDUwOzM2MCA1MCA1MCI+PC9hbmltYXRlVHJhbnNmb3JtPiAgPC9jaXJjbGU+PC9zdmc+");
                      max-height: 400px;
                    }
</style>

<div class="bk-root" id="7b0543dc-c84e-416d-896c-1741cbecf45a" data-root-id="3831"></div>

<p>I've wanted this to be standard functionality for a long time; six
years ago I briefly started a <a href="/20150119-gog_a_separate_layer_for_visualization/" title="gog: a separate layer for visualization">project</a> that included it in its
proof of concept, as in this screenshot:</p>
<p><img alt="gog screenshot" src="gogi_iris.png"></p>
<p>The interactive example above is the hvPlot <a href="https://hvplot.holoviz.org/reference/pandas/scatter.html" title="Scatter">Scatter example</a> saved
to HTML as in <a href="https://hvplot.holoviz.org/user_guide/Viewing.html#saving-plots" title="Saving plots">Saving plots</a> (code is <a href="https://github.com/ajschumacher/try_holoviz">on GitHub</a>) and then
copy-pasted into my blog Markdown file. The interactive plots
unfortunately don't show up in GitHub's Jupyter Notebook preview.</p>
<p><a href="https://hvplot.holoviz.org/">hvPlot</a> is just one corner of HoloViz and related packages, but the
interactive scatterplot functionality alone is enough for me to be a
big fan.</p>    
    ]]></description>
<link>http://planspace.org/20211126-try_holoviz_for_python_plotting/</link>
<guid>http://planspace.org/20211126-try_holoviz_for_python_plotting/</guid>
<pubDate>Fri, 26 Nov 2021 12:00:00 -0500</pubDate>
</item>
<item>
<title>The Essence of Progress and Poverty, by Henry George</title>
<description><![CDATA[

<p>This is a 2020 version of the 1928 abridging (endorsed by
<a href="https://en.wikipedia.org/wiki/John_Dewey" title="John Dewey (Wikipedia)">John Dewey</a>) of the 1879 original. It's enough to
<a href="https://www.henrygeorge.org/catsup.htm" title="“Seeing the Cat” has long been a slang term for achieving an understanding of Henry George's ideas.">“see the cat”</a>, I think, but I'm not yet convinced <a href="https://en.wikipedia.org/wiki/Georgism" title="Georgism (Wikipedia)">Georgism</a> is
<a href="/20211017-how_the_world_could_be_made_a_good_and_happy_place/" title="(a simple solution to solve all problems)">“how the world could be made a good and happy place”</a>. The single
tax on land seems like a decent idea, but I'm not sure it's sufficient
for Star Trek style post-scarcity utopia (<a href="/20210401-trekonomics_by_saadia/" title="Trekonomics, by Saadia">a</a>, <a href="/20201109-economics_of_star_trek_by_webb/" title="The Economics of Star Trek, by Webb">b</a>).</p>
<p>It's nice to encounter an idea that's <em>trying</em> to be a big solution. I
don't know a good reason not to try it, apart from that those who are
currently own land wouldn't like it.</p>
<p><img alt="cover" src="cover.jpg"></p>
<hr>
<blockquote>
<p>"It has always been to the newer countries—that is, to the countries
where material progress is yet in its earlier stages—that laborers
have emigrated in search of higher wages, and capital has flowed in
search of higher interest. It is in the older countries—that is to
say, the countries where material progress has reached later
stages—that widespread destitution is found in the midst of the
greatest abundance. Go into a new community where Anglo-Saxon vigor
is just beginning the race of progress; where the machinery of
production and exchange is yet rude and inefficient; where the
increment of wealth is not yet great enough to enable any class to
live in ease and luxury; where the best house is but a cabin of logs
or a cloth and paper shanty; and the richest man is forced to daily
work—and though you will find an absence of wealth and all its
concomitants, you will find no beggars." (page 4)</p>
</blockquote>
<p>Couple things.</p>
<ul>
<li>"Anglo-Saxon vigor"? How deep is this racist-imperialist thinking?
   Does he think only white people are people?</li>
<li>Was this ever true, that net migration was from richer to poorer
   countries? Is it true now? Certainly the stereotype is that people
   prefer to move from poorer to richer countries "in search of higher
   wages".</li>
<li>Is it really true that the situation he describes is somehow a cure
   for poverty, or is it that George is wrong, or perhaps that people
   who would be poor cannot survive in such settings, so they aren't
   observed, or some combination?</li>
</ul>
<hr>
<blockquote>
<p>"To educate men who must be condemned to poverty, is but to make
them restive; to base on a state of most glaring social inequality
political institutions under which men are theoretically equal, is
to stand a pyramid on its apex." (page 7)</p>
</blockquote>
<p>This makes me think of antagonism toward "elites"... Not a perfect
connection though.</p>
<hr>
<blockquote>
<p>"The reason why, in spite of the increase of productive power, wages
constantly tend to a minimum which will give but a bare living, is
that, with increase in productive power, rent tends to even greater
increase, thus producing a constant tendency to the forcing down of
wages." (page 26)</p>
</blockquote>
<p>But it also isn't that employers keep throwing more money at
employees, and then landlords respond by increasing rents. If rent was
suddenly free, wouldn't employers lower wages in response to the
changed cost of living? Currently we see employers adjusting salaries
of remote employees based on cost of living in various markets, which
is largely driven by cost of housing.</p>
<hr>
<blockquote>
<p>"To extirpate poverty, to make wages what justice demands they
should be, the full earnings of the laborer, we must therefore
substitute for the individual ownership of land a common ownership.</p>
<p>"The right of ownership that springs from labor excludes the
possibility of any other right of ownership." (page 32)</p>
</blockquote>
<hr>
<blockquote>
<p>"It [tax upon land values] is the taking by the community, for the
use of the community, of that value which is the creation of the
community." (page 37)</p>
</blockquote>
<hr>
<blockquote>
<p>"Every productive enterprise, besides its return to those who
undertake it, yields collateral advantages to others." (page 43)</p>
</blockquote>
<p>This is tautological or incorrect.</p>
<hr>
<blockquote>
<p>"It seems to me that in a condition of society in which no one need
fear poverty, no one would desire great wealth—at least, no one
would take the trouble to strive and to strain for it as men do
now." (page 47)</p>
</blockquote>
<hr>
<blockquote>
<p>"Shortsighted is the philosophy which counts on selfishness as the
master motive of human action." (page 57)</p>
</blockquote>    
    ]]></description>
<link>http://planspace.org/20211121-essence_of_progress_and_poverty/</link>
<guid>http://planspace.org/20211121-essence_of_progress_and_poverty/</guid>
<pubDate>Sun, 21 Nov 2021 12:00:00 -0500</pubDate>
</item>
<item>
<title>Come work at Meta! (Facebook/Instagram/WhatsApp/etc.)</title>
<description><![CDATA[

<p>People have started asking me to refer them for jobs at <a href="https://meta.com/">Meta</a>,
which is great because Meta is <a href="https://www.facebookcareers.com/jobs">hiring</a>! I'd be happy to refer you;
send <a href="/aaron/">me</a> the following:</p>
<ol>
<li>Your full name</li>
<li>Your email address</li>
<li>Exactly one of:<ul>
<li>Your resume in PDF format</li>
<li>A link to your LinkedIn profile</li>
<li>Tell me to put in your email address and you'll be prompted to
  provide information yourself. (I've never tried this.)</li>
</ul>
</li>
<li>One to three job links from the <a href="https://www.facebookcareers.com/jobs">official jobs site</a><ul>
<li>Not from LinkedIn, not from Indeed, etc. They have to be from
  the official jobs site to work in the system.</li>
<li>Ensure you meet the minimum requirements listed for each, and
  that the locations listed (including remote if applicable) are
  aligned with your preferences.</li>
<li>If you're interested in working remotely, mention that to me.</li>
<li>If you're interested in a position based in an EU or UK office,
  mention that to me.</li>
</ul>
</li>
<li>Are you either of: a current university student or graduated from
   university within the last nine months? (yes/no)</li>
<li>How many years of industry experience do you have (not counting
   internships)? (one number)</li>
<li>Do you live in either of: the EU or UK? (yes/no)</li>
<li>Optional things<ul>
<li>LinkedIn profile link</li>
<li>GitHub profile link</li>
<li>Portfolio website link</li>
<li>Brief note (one paragraph or less) on why you should be hired</li>
</ul>
</li>
</ol>
<p>Once I have the above, I can put you into the system. After that, I
won't hear anything before you do. I can't guarantee any particular
next steps, but I wish you the best of luck and hope you do well!</p>    
    ]]></description>
<link>http://planspace.org/20211109-come_work_at_meta/</link>
<guid>http://planspace.org/20211109-come_work_at_meta/</guid>
<pubDate>Tue, 09 Nov 2021 12:00:00 -0500</pubDate>
</item>
<item>
<title>Parallel coordinate plots for visualizing functions</title>
<description><![CDATA[

<p>While thinking about the <a href="https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Arcsine_transformation">arcsin transform</a> for <a href="/20211106-visualizing_cohens_h/">Cohen's <em>h</em></a>, I
described the function as “stretching” its inputs, and compared it to
the <a href="https://en.wikipedia.org/wiki/Logit">logit function</a>. I wondered whether a
<a href="https://en.wikipedia.org/wiki/Parallel_coordinates">parallel coordinates plot</a> could visualize the “stretching” more
clearly than the traditional Cartesian plot. I think it kind of works!</p>
<p><img alt="parallel coordinates plots" src="parallel.png"></p>
<p>This left-to-right function input-to-output view is pleasing in some
ways. I think it succeeds in showing how the function is “stretching”
the input, as desired. It's hard to show very many input/output pairs
though, and it would be hard to show non-monotonic functions well, or
show multiple functions on the same axes. For the logit, I think it's
less clear in implying the function is unbounded than with the
Cartesian version.</p>
<p><img alt="traditional cartesian plots" src="cartesian.png"></p>
<p>The usual way of plotting functions feels a little drier. “Stretching”
is greater where the plot is more vertical; I feel this less
immediately here than with the parallel coordinates version. Overall
this is still a great way to visualize functions, of course. I
wouldn't mind the occasional parallel coordinates plot as an
alternative visualization though!</p>
<hr>
<p>Code is <a href="https://github.com/ajschumacher/parallel_coordinate_functions">on GitHub</a>.</p>    
    ]]></description>
<link>http://planspace.org/20211107-parallel_coordinate_plots_for_visualizing_functions/</link>
<guid>http://planspace.org/20211107-parallel_coordinate_plots_for_visualizing_functions/</guid>
<pubDate>Sun, 07 Nov 2021 12:00:00 -0500</pubDate>
</item>
<item>
<title>Visualizing Cohen's h</title>
<description><![CDATA[

<p><a href="https://en.wikipedia.org/wiki/Cohen%27s_h">Cohen's <em>h</em></a> is an effect size for differences between two
proportions. It ranges from 0 to \( \pi \) (when nondirectional) and
adjusts for whether the proportions are “extreme” (near 0 or 1) so
that equal Cohen's <em>h</em> means equal statistical detectability (Cohen
1988).</p>
<p><img alt="Cohen's h" src="cohens_h.png"></p>
<p>The heuristic values of 0.2 for “small,” 0.5 for “medium,” and 0.8 for
“large” are analogous to the 0.05 confidence level: often taken more
seriously than they should be.</p>
<p>Cohen's <em>h</em> is the difference between the two proportions'
<a href="https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Arcsine_transformation">arcsin transforms</a>.</p>
<p><img alt="arcsin transformation" src="arcsin_transform.png"></p>
<p>The arcsin transform stretches out values near 0 and 1 more than
values around 0.5. It is bounded though, so for example if your
baseline proportion is 0.95, there is no proportion higher than that
which will register as a “medium” effect size.</p>
<p>You could replace the arcsin transform with <a href="https://en.wikipedia.org/wiki/Logit">log odds</a>; the logit
would allow any size difference anywhere, at least in principle. But I
don't think it would have any useful connection to variance, as the
<a href="https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Arcsine_transformation">arcsin transformation</a> does.</p>
<hr>
<p>Code is <a href="https://github.com/ajschumacher/cohens_h">on GitHub</a>.</p>
<!-- mathjax for formulas -->

    ]]></description>
<link>http://planspace.org/20211106-visualizing_cohens_h/</link>
<guid>http://planspace.org/20211106-visualizing_cohens_h/</guid>
<pubDate>Sat, 06 Nov 2021 12:00:00 -0500</pubDate>
</item>
<item>
<title>Simple diff-in-diff</title>
<description><![CDATA[

<p>Things are always changing. <a href="https://en.wikipedia.org/wiki/Difference_in_differences">Difference in differences</a> tries to
identify how much of a change is due to some treatment by comparing to
a control that's also changing over time.</p>
<p><img alt="diff-in-diff diagram" src="diff_in_diff.jpg"></p>
<p>This is often presented graphically with response value against time.</p>
<p><img alt="diff-in-diff graph" src="diff_in_diff_graph.jpg"></p>
<p>A noiseless design matrix illustrates how this becomes a diff-in-diff
regression.</p>
<pre><code>| treatment base | control base | common diff | treatment diff | response |
|----------------|--------------|-------------|----------------|----------|
| 1              | 0            | 0           | 0              |  3       |
| 0              | 1            | 0           | 0              |  2       |
| 1              | 0            | 1           | 1              | 12       |
| 0              | 1            | 1           | 0              |  7       |
</code></pre>

<p>Real data will have noise; here's a simulation in <a href="https://www.r-project.org/">R</a>.</p>
<pre><code class="language-r">treatment_base = c(rep(1, 100), rep(0, 100), rep(1, 100), rep(0, 100))
control_base   = c(rep(0, 100), rep(1, 100), rep(0, 100), rep(1, 100))
common_diff    = c(rep(0, 100), rep(0, 100), rep(1, 100), rep(1, 100))
treatment_diff = c(rep(0, 100), rep(0, 100), rep(1, 100), rep(0, 100))
response = c(rnorm(100, mean=3),
             rnorm(100, mean=2),
             rnorm(100, mean=3) + rnorm(100, mean=5) + rnorm(100, mean=4),
             rnorm(100, mean=2) + rnorm(100, mean=5))
summary(lm(response ~ treatment_base + control_base + common_diff + treatment_diff + 0))
##                Estimate Std. Error t value Pr(&gt;|t|)
## treatment_base   3.0747     0.1367   22.50   &lt;2e-16 ***
## control_base     1.9798     0.1367   14.49   &lt;2e-16 ***
## common_diff      5.0990     0.1933   26.38   &lt;2e-16 ***
## treatment_diff   3.9453     0.2733   14.44   &lt;2e-16 ***</code></pre>

<p>Diff-in-diff can get <a href="https://andrewcbaker.netlify.app/2019/09/25/difference-in-differences-methodology/">more complicated</a>, but the simple version
isn't too bad.</p>
<hr>
<p>Thanks to Dr. Erica Blom for valuable discussion and references on
this topic.</p>
<hr>
<h3>See also</h3>
<ul>
<li>Four “back door” regression situations:
   <a href="/20200912-what_should_be_in_your_regression/">What should be in your regression?</a></li>
<li><a href="/20210501-simple_front_door_regression/">Simple Front Door Regression</a></li>
<li><a href="/20210430-a_simple_instrumental_variable/">A simple Instrumental Variable</a></li>
</ul>    
    ]]></description>
<link>http://planspace.org/20211031-simple_diff_in_diff/</link>
<guid>http://planspace.org/20211031-simple_diff_in_diff/</guid>
<pubDate>Sun, 31 Oct 2021 12:00:00 -0500</pubDate>
</item>
<item>
<title>My daily routine</title>
<description><![CDATA[

<p>Especially since reading <a href="/2014/01/19/daily-rituals-is-sort-of-inspiring/" title="Daily Rituals is sort of inspiring">Daily Rituals</a>, I've thought about how
people structure their days. This year I've been working on my
routines, especially mornings. Here's how it's looking.</p>
<h3>6:00-6:30 — <a href="https://en.wikipedia.org/wiki/Spaced_repetition" title="Spaced repetition on Wikipedia">Spaced repetition</a> review with <a href="https://apps.ankiweb.net/" title="Anki App">Anki</a></h3>
<p>I review about a hundred cards per day in the <a href="https://apps.ankiweb.net/" title="Anki App">Anki</a> app. I
<a href="/20201009-anki_is_easy/" title="Anki is easy">got into this</a> about a year ago, and I think I'm going to
<a href="/20210908-spaced_repetition_is_o_log_n_sustainable/" title="Spaced repetition is O(log n) sustainable">stick with it</a>. It's pretty common to recommend daily physical
exercise, and I think of this as being a similar practice for
knowledge and memory.</p>
<h3>6:30-7:00 — Writing in journal aka “<a href="https://juliacameronlive.com/basic-tools/morning-pages/" title="Morning Pages">Morning pages</a>”</h3>
<p>This is a <a href="https://juliacameronlive.com/basic-tools/morning-pages/" title="Morning Pages">Julia Cameron thing</a> I picked up from
<a href="/20210116-pragmatic_thinking_and_learning/" title="Pragmatic Thinking and Learning, by Andy Hunt">Andy Hunt's book</a>. It's pretty good. I don't always write a full
three pages, but I make a point to always write something.</p>
<h3>7:00-7:30 — Physical exercise</h3>
<p>I alternate running about three miles outside and doing a 21-minute
“Smart Workout” with the <a href="https://7minuteworkout.jnj.com/" title="The Johnson &amp; Johnson Official 7 Minute Workout® App">J&amp;J 7-Minute Workout app</a> in the basement.
Running allows podcast listening.</p>
<p>Then I try to shower and everything by eight.</p>
<h3>Food and drink</h3>
<p>During the week, my breakfast and lunch have gotten pretty consistent.</p>
<ul>
<li>Breakfast: Granola with a little whole milk Greek yogurt</li>
<li>Lunch: Turkey and Swiss sandwich on whole grain bread with mustard</li>
</ul>
<p>Also in a typical day I drink two to four 8.4-ounce cans of Red Bull
Sugar Free.</p>
<h3>“Dailies”</h3>
<p>There are a number of little things I try to do every day. They can
change, and aren't always super strict. Here are some examples:</p>
<ul>
<li>Read today's entry from Tolstoy's <a href="https://en.wikipedia.org/wiki/A_Calendar_of_Wisdom">Calendar of Wisdom</a></li>
<li>Copy one of my old book posts into <a href="https://www.goodreads.com/user/show/34234019-aaron-schumacher">Goodreads</a> and Amazon reviews</li>
<li>Post a picture of my algae tank on Instagram</li>
<li>Greet someone I wasn't otherwise planning to talk to today</li>
<li>Do a lesson and reviews on <a href="https://www.executeprogram.com/">Execute Program</a></li>
<li>Come up with one new idea based on a random word</li>
</ul>
<p>Some of these I track in a spreadsheet.</p>
<h3>Evening — Read a book</h3>
<p>I try to read a chapter, or an hour, or at least <em>something</em> that
isn't screaming at me from a screen. This is usually a paper book
because I like them, but the main characteristic of this reading is
that it's deliberate rather than reactive. I record what I've read in
my journal.</p>
<hr>
<p>I'm not perfect in following all of this, but recently I've been
getting more consistent, and I like it. One way I think about this is
that it raises the floor for how much I do in a day. Even if I do
nothing else, if I do these things I've done something to be happy
about. And of course these things have benefits that help me do other
things too. Always room to learn and improve more too!</p>
<p>The routines here are the least important parts of my days; I hope
they support the things I value and don't conflict with them.</p>
<p>I'm very fortunate to be largely free to choose how I spend so much
time; I owe so much to Erica in particular for supporting me and
making our family's life better every day.</p>    
    ]]></description>
<link>http://planspace.org/20211030-my_daily_routine/</link>
<guid>http://planspace.org/20211030-my_daily_routine/</guid>
<pubDate>Sat, 30 Oct 2021 12:00:00 -0500</pubDate>
</item>
<item>
<title>Core RAPPOR</title>
<description><![CDATA[

<p><a href="https://arxiv.org/abs/1407.6981">RAPPOR</a> (“Randomized Aggregatable Privacy-Preserving Ordinal
Response”) is really close to the classic coin-flip
<a href="https://en.wikipedia.org/wiki/Differential_privacy#Randomized_response">randomized response</a> method, with one more layer of randomness.
It's “ordinal” in that it's every bit for itself: you can represent
histogram bins, but you can't really represent numbers in the usual
binary way. They additionally use Bloom filters to represent sets of
hashable things, which is neat too.</p>
<p><a href="https://en.wikipedia.org/wiki/Differential_privacy#Randomized_response">Randomized response</a> works like this. Did you wash your hands?</p>
<ul>
<li>25% of the time, just say “yes” regardless</li>
<li>25% of the time, just say “no” regardless</li>
<li>50% of the time, answer truthfully</li>
</ul>
<p>Those percentages correspond to \( f = 0.5 \) in the RAPPOR
<a href="https://arxiv.org/pdf/1407.6981.pdf">paper</a>'s notation. They call this the “permanent” step, because
it's done once per respondent.</p>
<!--
<pre><code class="language-python">import random</code></pre>
-->

<pre><code class="language-python">def permanent_randomized(response: bool, f=0.5):
    randomizer = random.random()  # uniform on [0, 1)
    if 0 &lt;= randomizer &lt; f/2:  # probability f/2
        return True
    elif f/2 &lt;= randomizer &lt; f:  # probability f/2
        return False
    elif f &lt;= randomizer &lt; 1:  # probability 1 - f
        return response</code></pre>

<p>If you get 59% positive responses with this method, 25% were just automatically saying “yes” and 34% were answering “yes” because they were being honest. Half of people were being honest, so double the 34% to find the overall true positive rate is 68%.</p>
<p>To let individuals report multiple times without losing privacy or being trackable, RAPPOR adds “instantaneous” randomness as well.</p>
<pre><code class="language-python">def instantaneous_randomized(response: bool, q=0.75, p=0.5):
    randomizer = random.random()  # uniform on [0, 1)
    if response is True:
        return randomizer &lt; q  # probability q
    elif response is False:
        return randomizer &lt; p  # probability p</code></pre>

<p>With the complete method, recovering the true rate gets a little more complicated; see first equation in the <a href="https://arxiv.org/pdf/1407.6981.pdf">paper</a>'s §4.</p>
<pre><code class="language-python">def extract_rate(raw_rate: float, f=0.5, q=0.75, p=0.5):
    return (raw_rate - p - f*q/2 + f*p/2) / ((1 - f)*(q - p))</code></pre>

<p>And it works!</p>
<!--
<pre><code class="language-python">random.seed(0)</code></pre>
-->

<pre><code class="language-python">true_rate = 0.68
n = 1_000_000

raw_rate = sum(
    instantaneous_randomized(
        permanent_randomized(random.random() &lt; true_rate))
    for _ in range(n)) / n

extract_rate(raw_rate)
## 0.6807759999999998</code></pre>

<p>RAPPOR is just doing this for any number of bits per respondent. Each
bit can have a direct interpretation like “this thing yes for this
respondent.”</p>
<p>RAPPOR also introduces a way of having respondents hash strings (say)
into <a href="http://en.wikipedia.org/wiki/Bloom_filter">Bloom filters</a>, which is kind of neat. They show how to have
different groups of respondents use different hash functions and then
do some regression to try to overcome noise and false positives. As
far as I can tell, you have to already know the set of things you're
going to be hashing into the Bloom filter (you won't discover an
unknown unknown) but it is more efficient and flexible than defining a
separate bit for every candidate string.</p>
<p>RAPPOR is used <a href="https://www.chromium.org/developers/design-documents/rappor">in Chrome</a>, for example, and if you have really a
lot of respondents, it seems pretty good!</p>
<!-- mathjax for formulas -->

    ]]></description>
<link>http://planspace.org/20211019-core_rappor/</link>
<guid>http://planspace.org/20211019-core_rappor/</guid>
<pubDate>Tue, 19 Oct 2021 12:00:00 -0500</pubDate>
</item>
<item>
<title>“how the world could be made a good and happy place”</title>
<description><![CDATA[

<p>The opening of <a href="https://en.wikipedia.org/wiki/The_Hitchhiker%27s_Guide_to_the_Galaxy">The Hitchhiker's Guide to the Galaxy</a> goes like this
(italics in original, bold added):</p>
<blockquote>
<p><em>Far out in the uncharted backwaters of the unfashionable end of the
Western Spiral arm of the Galaxy lies a small unregarded yellow sun.
Orbiting this at a distance of roughly ninety-eight million miles is
an utterly insignificant little blue-green planet whose apedescended
life forms are so amazingly primitive that they still think digital
watches are a pretty neat idea.</em></p>
<p><em>This planet has—or rather had—a problem, which was this: most of
the people living on it were unhappy for pretty much of the time.
Many solutions were suggested for this problem, but most of these
were largely concerned with the movements of small green pieces of
paper, which is odd because on the whole it wasn’t the small green
pieces of paper that were unhappy.</em></p>
<p><em>And so the problem remained; lots of the people were mean, and most
of them were miserable, even the ones with digital watches.</em></p>
<p><em>Many were increasingly of the opinion that they’d all made a big
mistake in coming down from the trees in the first place. And some
said that even the trees had been a bad move, and that no one should
ever have left the oceans.</em></p>
<p><em>And then, one Thursday, nearly two thousand years after one man had
been nailed to a tree for saying how great it would be to be nice to
people for a change, <strong>a girl sitting on her own in a small café in
Rickmansworth suddenly realized what it was that had been going
wrong all this time, and she finally knew how the world could be
made a good and happy place. This time it was right, it would work,
and no one would have to get nailed to anything.</strong></em></p>
<p><em>Sadly, however, before she could get to a phone to tell anyone
about it, a terrible, stupid catastrophe occurred, and the idea was
lost for ever.</em></p>
<p><em>This is not her story.</em></p>
</blockquote>
<p>I think the joke is that there is no such idea (so let's have nihilist
sci-fi fun) but I still don't like it. I bet Douglas Adams would have
written her story if he could have.</p>    
    ]]></description>
<link>http://planspace.org/20211017-how_the_world_could_be_made_a_good_and_happy_place/</link>
<guid>http://planspace.org/20211017-how_the_world_could_be_made_a_good_and_happy_place/</guid>
<pubDate>Sun, 17 Oct 2021 12:00:00 -0500</pubDate>
</item>
<item>
<title>The Case Against Education, by Caplan</title>
<description><![CDATA[

<p>Caplan thinks students aren't learning—“The vast majority are
philistines”—so we should stop trying to teach them. I agree with
Caplan on some of his premises, but I don't follow him all the way to
his libertarian recommendations.</p>
<p>Should everyone be pushed toward college as if it's the only
acceptable path? Probably not. More vocational education seems like a
fine idea. I agree there's a signaling component (possibly large) to
the personal benefit of education, and that many people don't learn as
much as it seems like they should.</p>
<p>Caplan turns his back on equality of opportunity when he recommends
letting the market decide on education. I'm not entirely comfortable
with how casually he cites <a href="https://www.splcenter.org/fighting-hate/extremist-files/individual/arthur-jensen">Jensen</a> (page 60) or ignores that
effects vary substantially by race even in work he cites (page 150).</p>
<p>I think it's inconsistent to treat education as essentially daycare
but not include the provision of that service in the accounting of
economic benefits.</p>
<p>I think the interesting thing is to consider Caplan's critiques and
think about what really effective education could look like. What
really would build human capital? What would make people wiser,
happier, and more productive?</p>
<p><img alt="cover" src="cover.jpg"></p>
<hr>
<blockquote>
<p>"Bryan Caplan, the foremost whistle-blower in the academy, argues
persuasively that learning about completely arbitrary subjects is
attractive to employers because it signals students' intelligence,
work ethic, desire to please, and conformity—even when such learning
conveys no cognitive advantage or increase in human capital." (cover
blurb from Stephen J. Ceci)</p>
</blockquote>
<hr>
<blockquote>
<p>"Learning doesn't have to be useful. Learning doesn't have to be
inspirational. When learning is neither useful nor inspirational,
though, how can we call it anything but wasteful." (page 2)</p>
</blockquote>
<hr>
<blockquote>
<p>"Few jobs require knowledge of higher mathematics, but over 80% of
high school grads suffer through geometry. Students study history
for years, but history teachers are almost the only people alive who
use history on the job." (pages 6-7)</p>
</blockquote>
<p>Geometry is higher mathematics? But more importantly, isn't history
important, at least in theory, to having informed voters for
democracy? Caplan later points out that years of history don't usually
yield informed voters in the current system anyway, but it seems
inappropriate to completely neglect civic objectives for education.</p>
<hr>
<blockquote>
<p>"...statistics and econometrics courses at elite colleges emphasize
mathematical proofs, not hands-on statistical training." (page 11)</p>
</blockquote>
<p>That cites <a href="https://ftp.iza.org/dp10535.pdf">Undergraduate Econometrics Instruction: Through Our Classes, Darkly</a>, which includes the following in its abstract:</p>
<blockquote>
<p>"Questions of research design and causality still take a back seat
in the classroom, in spite of having risen to the top of the modern
empirical agenda. This essay traces the divergent development of
econometric teaching and empirical practice, arguing for a
pedagogical paradigm shift."</p>
</blockquote>
<hr>
<blockquote>
<p>"Higher education is the only product where the consumer tries to
get as little out of it as possible." (page 26, quoting Arnold
Kling, sort of)</p>
</blockquote>
<p>The citation is <a href="https://www.econlib.org/archives/2006/08/college_custome.html">College Customers vs. Suppliers</a>, in which Kling writes:</p>
<blockquote>
<p>"I recall seeing a quote somewhere else to the effect that higher
education is the only product where the consumer tries to get as
little out of it as possible."</p>
</blockquote>
<p>So who knows where the quote comes from!</p>
<p>Kling proposes separating testing from teaching, to align students and
professors in the direction of rigor rather than easy A's. Maybe so?</p>
<hr>
<blockquote>
<p>"Basic literacy and numeracy are virtually the <em>only</em> book learning
most American adults possess." (page 40)</p>
</blockquote>
<hr>
<blockquote>
<p>"Though college grads spend at least seventeen years in school,
under a third have the level of literacy and numeracy we assume of
every college freshman." (page 43)</p>
</blockquote>
<hr>
<blockquote>
<p>"Note: Statisticians routinely rely on the approximate equality
between logged variables and percentages. However, when coefficients
are large, this approximation breaks down, so I convert results to
percentages for clarity." (page 302, endnote 8 from page 73)</p>
</blockquote>
<p>I don't think this is very clear. What it's getting at is that if
you're fitting a linear model, then a logged outcome variable makes
the additive model multiplicative in the original outcome, so a 0.02
change in the regression outcome is a 2% change in the original
outcome. Gelman has a decent <a href="https://statmodeling.stat.columbia.edu/2019/08/21/you-should-usually-log-transform-your-positive-data/">intro</a>, maybe. Or <a href="https://people.duke.edu/~rnau/411log.htm">this</a>, which
notes that "small changes in the natural log of a variable are
directly interpretable as percentage changes, to a very close
approximation" and even a nice table showing how the approximation
breaks down.</p>
<hr>
<blockquote>
<p>"Most strikingly, the standard measure of "fatalism," the Rotter
Internal-External Locus of Control Scale, is a four-question
personality test. (page 303, endnote 24 from page 75)</p>
</blockquote>
<p>This is misleading. The <a href="https://www.mccc.edu/~jenningh/Courses/documents/Rotter-locusofcontrolhandout.pdf" title="Rotter's Locus of Control Scale">Rotter</a> instrument has 29 questions. But
Caplan is citing <a href="https://www.nber.org/papers/w12006" title="The Effects of Cognitive and Noncognitive Abilities on Labor Market Outcomes and Social Behavior">a 2006 paper</a> that uses <a href="https://www.bls.gov/nls/nlsy79.htm" title="National Longitudinal Survey of Youth 1979">NLSY79</a> data, which
does indeed <a href="https://www.nlsinfo.org/content/cohorts/nlsy79/other-documentation/codebook-supplement/nlsy79-appendix-21-attitudinal-scales#rotter">include</a> a "four-item abbreviated version of the Rotter
Internal External Locus of Control Scale"—sort of.</p>
<p>NLSY79 included Rotter questions 28, 13, 15, and 25 (in that order)
and in addition to asking respondents to choose one of two statements
for each, also asked whether their choice was "much closer" or
"slightly closer" to their view. So it's an eight-question instrument
(or at least, it yields eight bits of information).</p>
<p>There are multiple tests for internal-external locus of control, some
<a href="https://www.tandfonline.com/doi/abs/10.1080/00223891.1974.10119987" title="An Abbreviated Measure of Internal-External Locus of Control">shorter</a> than 29 questions, some <a href="https://psycnet.apa.org/record/1973-21097-001" title="A locus of control scale for children">longer</a>. If you do want a
four-question instrument, consider Kovaleva's <a href="https://www.ssoar.info/ssoar/bitstream/handle/document/37119/ssoar-2012-kovaleva-The_IE-4_Construction_and_Validation.pdf" title="The IE-4: Construction and Validation of a Short Scale for the Assessment of Locus of Control">IE-4</a> (four
five-point Likert scale questions; see Table 26 on page 81).</p>
<hr>
<blockquote>
<p>"Full-length intelligence tests have a very high reliability; the
reliability of the <a href="https://en.wikipedia.org/wiki/Armed_Services_Vocational_Aptitude_Battery#Armed_Forces_Qualification_Test">AFQT</a>, for example, is .94 (ASVAB 2015). Short
intelligence tests, in contrast, have markedly lower reliability—.74
in the case of the General Social Survey's <a href="https://en.wikipedia.org/wiki/Wordsum">ten-word IQ test</a>
(Caplan and Miller 2010, p. 645)." (page 307, endnote 98 from page
93)</p>
</blockquote>
<hr>
<blockquote>
<p>"You'll never apply most of what you study, but so what? Academic
success opens doors. A dysfunctional game, but if you refuse to
play, the labor market brands you a loser." (page 108)</p>
</blockquote>
<hr>
<blockquote>
<p>"<em>Signals can affect pay even after employers know the truth.</em>
Employer learning researchers speak as if the payoff for signaling
ends as soon as employers know a worker's true worth. They should be
more circumspect. For starters, firms often give new workers
valuable on-the-job training. As a result, signaling can indirectly
boost your productivity. Step 1: Signal in school. Step 2: Land a
good job. Step 3: Learn useful job skills on the job. Step 4:
Persistently profit. If your signal modestly overstates your skill,
your imployer may soon wish they'd hired someone else. By the time
they spot their mistake, however, your new marketable skills
permanently justify higher pay." (page 112)</p>
</blockquote>
<p>Following the endnote:</p>
<blockquote>
<p>"While research on this story is sparse, Heisz and Oreopoulos 2006
report the payoff for high-ranked M.B.A. and law degrees <em>increases</em>
with experience, even correcting for individual ability, because
high-ranked degrees lead to good first jobs, which lead to even
better jobs down the line." (page 312)</p>
</blockquote>
<hr>
<blockquote>
<p>"The Higher Education Research Institute has questioned college
freshmen about their goals since the 1970s. The vast majority is
openly careerist and materialist. In 2012, almost 90% called "being
able to get a better job" a "very important" or "essential" reason
to go to college. Being "very well-off financially" (over 80%) and
"making more money" (about 75%) are almost as popular. Less than
half say the same about "developing a meaningful philosophy of
life." These results are especially striking because humans
exaggerate their idealism and downplay their selfishness. Students
probably prize worldly success even more than they admit." (page
126)</p>
</blockquote>
<hr>
<blockquote>
<p>"A cynic isn't someone who puts a price on the sacred; a cynic is
someone who puts a <em>low</em> price on the sacred." (page 126)</p>
</blockquote>
<hr>
<blockquote>
<p>"The High School Survey of Student Engagement, probably the single
best study of how high school students feel about school, reports
that 66% of high school students say they're bored in class <em>every
day</em>. Seventeen percent say they're bored in <em>every</em> class <em>every</em>
day. Only 2% claim they're never bored in class. Why so bored?
Eighty-two percent say the material isn't interesting; 41% say the
material isn't relevant. Another research team gave beepers to
middle school students to capture their feelings in real time.
During schoolwork, students were bored 36% of the time, versus 17%
for all other activities. No wonder a major Gates Foundation study
ranked boredom the <em>most</em> important reason why kids drop out of high
school." (page 135)</p>
</blockquote>
<hr>
<blockquote>
<p>"How much does your alma mater's rank matter? Research is oddly
mixed." (page 149)</p>
</blockquote>
<p>The endnote points to <a href="https://www.aeaweb.org/articles?id=10.1257/jep.23.4.95">Hoxby 2009</a>, which is about college
selectivity, not how much it matters to student outcomes. It's
interesting though; here's the abstract:</p>
<blockquote>
<p>"Over the past few decades, the average college has not become more
selective: the reverse is true, though not dramatically. People who
believe that college selectivity is increasing may be extrapolating
from the experience of a small number of colleges such as members of
the Ivy League, Stanford, Duke, and so on. These colleges have
experienced rising selectivity, but their experience turns out to be
the exception rather than the rule. Only the top 10 percent of
colleges are substantially more selective now than they were
in 1962. Moreover, at least 50 percent of colleges are substantially
less selective now than they were in 1962. To understand changing
selectivity, we must focus on how the market for college education
has re-sorted students among schools as the costs of distance and
information have fallen. In the past, students' choices were very
sensitive to the distance of a college from their home, but today,
students, especially high-aptitude students, are far more sensitive
to a college's resources and student body. It is the consequent
re-sorting of students among colleges that has, at once, caused
selectivity to rise in a small number of colleges while
simultaneously causing it to fall in other colleges. This has had
profound implications for colleges' resources, tuition, and
subsidies for students. I demonstrate that the stakes associated
with choosing a college are greater today than they were four
decades ago because very selective colleges are offering very large
per-student resources and per-student subsidies, enabling admitted
students to make massive human capital investments."</p>
</blockquote>
<hr>
<blockquote>
<p>"Their [Dale and Krueger's] most amazing discovery is that students
who <em>submit</em> lots of applications to high-quality schools enjoy
exceptional career success whether or not they attend such schools."
(page 150)</p>
</blockquote>
<p>The citation is <a href="https://www.jstor.org/stable/23799087" title="Estimating the Effects of College Characteristics over the Career Using Administrative Earnings Data">Dale and Krueger 2014</a>, which has this abstract:</p>
<blockquote>
<p>"We estimate the labor market effect of attending a highly selective
college, using the College and Beyond Survey linked to Social
Security Administration data. We extend earlier work by estimating
effects for students that entered college in 1976 over a longer time
horizon (from 1983 through 2007) and for a more recent cohort
(1989). For both cohorts, the effects of college characteristics on
earnings are sizeable (and similar in magnitude) in standard
regression models. In selection-adjusted models, these effects
generally fall to close to zero; however, these effects remain large
for certain subgroups, such as for black and Hispanic students."</p>
</blockquote>
<p>It's the last sentence that matters. Caplan only read the first half
of that one, somehow, which I think is a substantial mistake.</p>
<hr>
<blockquote>
<p>"As long as your state's best public school admits you, there's no
solid reason to pay more." (page 153)</p>
</blockquote>
<p>If you're white. I mean, <a href="https://www.jstor.org/stable/23799087" title="Estimating the Effects of College Characteristics over the Career Using Administrative Earnings Data">Dale and Krueger 2014</a> is <em>his</em>
citation—how can he ignore that?</p>
<hr>
<blockquote>
<p>"American marriage is a diploma-based caste system." (page 156)</p>
</blockquote>
<hr>
<blockquote>
<p>"Going to Harvard may not get you a better job but almost certainly
puts you in an exclusive dating pool for life." (page 157)</p>
</blockquote>
<hr>
<blockquote>
<p>"Most Ph.D. students have spent their entire lives at the top of the
class, yet half wander off before they defend their dissertations."
(pages 163-164)</p>
</blockquote>
<hr>
<blockquote>
<p>"Yet common sense insists the best way to discover useful ideas is
to search for useful ideas—not to search for whatever fascinates you
and pray it turns out to be useful." (page 175)</p>
</blockquote>
<p>He cites Niskanen 1997 in this opposition to basic research. This is
the common sense of those who don't make the discoveries that enable
the useful ideas of the future.</p>
<hr>
<blockquote>
<p>"The United States—and probably the rest of the world—is
overeducated." (page 199)</p>
</blockquote>
<p>And yet, still so apparently in need of more/better education.</p>
<hr>
<blockquote>
<p>"There really is no need for K-12 to teach history, social studies,
art, music, or foreign languages." (page 205)</p>
</blockquote>
<hr>
<blockquote>
<p>"Better retention efforts will not make Poor Students perform like
Fair Students, Fair Students like Good Students, or Good Students
like Excellent Students." (pages 211-212)</p>
</blockquote>
<hr>
<blockquote>
<p>"Instead of treating the human capital model as an accurate
description of education, they could treat it as a noble
<em>pre-_scription _for</em> education. Let's transform our schools from
time sinks to skill factories." (page 225)</p>
</blockquote>
<hr>
<blockquote>
<p>"Doing <em>any</em> job teaches you how to do a job. If this seems a low
bar, recall that almost half of dropouts and a third of high school
graduates these days aren't even <em>looking</em> for work. Acclimating
them to a any form of employment would be a step up." (page 232)</p>
</blockquote>
<hr>
<p>On page 239, Caplan quotes at some length from Malcolm X describing
how he copied a dictionary in prison. It's a bit Hirsch-like, this
advocacy for content, for volume...</p>
<hr>
<blockquote>
<p>"The straightforward story, though, is that high culture requires
extra mental effort to appreciate—and most humans resent mental
effort." (page 247)</p>
</blockquote>
<hr>
<blockquote>
<p>"Measuring effects issue by issue neatly explains education's
puzzlingly small impact on ideology and party. Since education
simultaneously increases social liberalism <em>and</em> economic
conservatism, its effect on "liberalism" is ambiguous. And while
their social liberalism makes the well-educated more Democratic,
their economic conservatism makes them more Republican, leaving
partisanship nearly untouched." (page 333, endnote 36 from page 249)</p>
</blockquote>
<hr>
<blockquote>
<p>"If a world of historical ignorance is scary, you should be scared
already, because that's where we live." (page 250)</p>
</blockquote>
<hr>
<blockquote>
<p>"When you run out of ideas, assign a random Wikipedia article. ...
Start with the Bureau of Labor Statistics' figures on "employment by
major occupational group" and "occupations with the most job
growth." When you run out of ideas, have students check out an
unfamiliar job from the Bureau of Labor Statistics' <em>Occupational
Outlook Handbook</em>." (page 256)</p>
</blockquote>
<p>I do kind of like some of this. In particular, I feel like I didn't
learn in school much of anything about what jobs exist or what people
really do in them.</p>
<hr>
<blockquote>
<p>"I'm cynical about students. The vast majority are philistines. The
best teachers in the universe couldn't inspire them with sincere and
lasting love of ideas and culture. I'm cynical about teachers. The
vast majority are uninspiring; they can't convince even <em>themselves</em>
to love ideas and culture, much less their students. I'm cynical
about "deciders"—the school officials who control what students
study. The vast majority think they've done their job as long as
students obey." (page 259)</p>
</blockquote>
<hr>
<blockquote>
<p>"I don't hate education. Rather I love education too much to accept
our Orwellian substitute." (page 260)</p>
</blockquote>
<hr>
<blockquote>
<p>"As Stanford education professor David Labaree remarks, "Motivating
volunteers to engage in human improvement is very difficult, as any
psychotherapist can confirm, but motivating conscripts is quite
another thing altogether. And it is conscripts that teachers face
every day in the classroom."" (page 260)</p>
</blockquote>
<hr>
<blockquote>
<p>"Many idealists object that the Internet provides enlightenment only
for those who seek it. They're right, but petulant to ask for more."
(page 261)</p>
</blockquote>
<hr>
<blockquote>
<p>"Most humans intrigued by abstract ideas and high culture are
working adults. Instead of lamenting youthful apathy, passionate
educators should redirect their energy to humans who are <em>ready</em> for
enlightenment. There is little money in blogging, podcasting, or
uploading lectures to YouTube. But if, like me, you love education
to the depths of your soul, such efforts are their own reward."
(page 261)</p>
</blockquote>
<p>And that's how the book ends, apart from the five dispensable
dialogues that follow.</p>    
    ]]></description>
<link>http://planspace.org/20211006-case_against_education_by_caplan/</link>
<guid>http://planspace.org/20211006-case_against_education_by_caplan/</guid>
<pubDate>Wed, 06 Oct 2021 12:00:00 -0500</pubDate>
</item>
<item>
<title>XGBoost/LightGBM/CatBoost (briefly)</title>
<description><![CDATA[

<p>There are many explainers of the popular gradient boosted tree models,
but this is short.</p>
<table width="100%" border="1">
  <tr>
    <th style="width:33%;">
      <a href="https://xgboost.readthedocs.io/">XGBoost</a></th>
    <th style="width:34%;">
      <a href="https://lightgbm.readthedocs.io/">LightGBM</a></th>
    <th style="width:33%;">
      <a href="https://catboost.ai/">CatBoost</a></th>
  </tr>
  <tr>
    <td>search missing high and low</td>
    <td>search, then assign missing</td>
    <td>specify missing high or low</td>
  </tr>
  <tr>
    <td>"normal" balanced trees</td>
    <td>leaf-first tree growth</td>
    <td>oblivious trees (tables)</td>
  </tr>
  <tr>
    <td>you handle categories</td>
    <td>smart categorical ordering</td>
    <td>permuted target coding</td>
  </tr>
  <tr>
    <td>weighted quantile sketch</td>
    <td>sample high-grad examples</td>
    <td>permuted boosting</td>
  </tr>
  <tr>
    <td>regularized objective</td>
    <td>exclusive feature bundling</td>
    <td>learns category interactions</td>
  </tr>
  <tr>
    <td><a href="https://arxiv.org/abs/1603.02754">2016 paper</a></td>
    <td><a href="https://papers.nips.cc/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html">2017 paper</a></td>
    <td><a href="https://arxiv.org/abs/1706.09516">2017 paper</a></td>
  </tr>
</table>

<p>This is close to correct, I think. It probably won't help you
understand what's going on, but if you already know it might help jog
your memory. The models all work pretty well.</p>    
    ]]></description>
<link>http://planspace.org/20211003-xgboost_lightgbm_catboost_briefly/</link>
<guid>http://planspace.org/20211003-xgboost_lightgbm_catboost_briefly/</guid>
<pubDate>Sun, 03 Oct 2021 12:00:00 -0500</pubDate>
</item>
<item>
<title>Staff Engineer, by Larson</title>
<description><![CDATA[

<p>It's all <a href="https://staffeng.com/">online for free too</a>, but I enjoyed reading a paper copy
of <a href="https://lethain.com/">Larson</a>'s <a href="https://staffeng.com/guides">Staff Engineer</a>. (Well: I skimmed some of the
interviews that pad out the page count.) I really liked the use of QR
codes with the endnote links. That could work well in Tufte-style
margins too.</p>
<p><img alt="cover" src="cover.jpg"></p>
<hr>
<blockquote>
<p>"There's a popular vision of heroic leadership that centers on
extraordinarily productive individuals whose decisions change their
company's future. Most of those narratives are intentionally
designed by public relations teams to create a good story. You're
far more likely to change your company's long-term trajectory by
growing the engineers around you than through personal heroics. The
best way to grow those around you is by creating an active practice
of mentorship and sponsorship." (page 22)</p>
</blockquote>
<hr>
<blockquote>
<p>"It might be addressing the sudden realization that your primary
database only has three months of remaining disk space, and you
can't upgrade to a larger size (in my experience, a surprisingly
frequent problem at fast-growing startups)." (page 24)</p>
</blockquote>
<hr>
<blockquote>
<p>"For mentorship and sponsorship, spend some time with Lara Hogan's
<a href="https://larahogan.me/blog/what-sponsorship-looks-like/">What Does Sponsorship Look Like?</a>, and for being glue, spend time
with Tanya Reilly's piece that bore the phrase, <a href="https://noidea.dog/glue">Being Glue</a>."
(page 34)</p>
</blockquote>
<hr>
<blockquote>
<p>"The first place to look for work that matters is exploring whether
your company is experiencing an existential risk." (page 39)</p>
</blockquote>
<hr>
<blockquote>
<p>"Foster growth" (page 40)</p>
</blockquote>
<hr>
<blockquote>
<p>"Specific statements create alignment; generic statements create the
illusion of alignment." (page 47)</p>
</blockquote>
<hr>
<blockquote>
<p>"There's no such thing as winning, only learning and earning the
chance to keep playing." (page 52)</p>
</blockquote>
<p>Reminds me of <a href="https://en.wikipedia.org/wiki/Finite_and_Infinite_Games">Finite and Infinite Games</a>.</p>
<hr>
<blockquote>
<p>"Premature processes add more friction than value and are quick to
expose themselves as ineffective." (page 43)</p>
</blockquote>
<hr>
<blockquote>
<p>"There's the old joke about <a href="https://en.wikipedia.org/wiki/Sarbanes%E2%80%93Oxley_Act">Sarbannes-Oxley</a> [sic]: it doesn't
reduce risk; it just makes it clear to blame when things go wrong."
(page 54)</p>
</blockquote>
<hr>
<blockquote>
<p>"... adopting the "define errors out of existence" approach
described in A Philosophy of Software Design." (page 54)</p>
</blockquote>
<p>At a quick look, it seems like this is something like "give your
functions defaults rather than exceptions" so if you ask for something
out of range, for example, you get the last thing rather than throwing.</p>
<hr>
<blockquote>
<p>"Genuine best practice has to be supported by research, and the best
source of research on this topic is <a href="https://itrevolution.com/accelerate-book/">Accelerate</a>." (page 56)</p>
</blockquote>
<hr>
<blockquote>
<p>"When it comes to complex systems and interdependencies, moving
quickly is just optics. It's methodical movement that gets the job
done." (page 69)</p>
</blockquote>
<hr>
<blockquote>
<p>"I think this is the most important lesson I've learned over the
past few years: the most effective leaders spend more time following
than they do leading. This idea also comes up in the idea of the
"<a href="https://www.ted.com/talks/derek_sivers_how_to_start_a_movement">the first follower creates a leader</a>," but effective leaders
don't split the world into a leader and follower dichotomy, rather
they move in and out of the leadership and follower roles with the
folks around them." (page 76)</p>
</blockquote>
<hr>
<blockquote>
<p>"There's a well-worn model of genius encapsulated in
<a href="https://wiki.c2.com/?FeynmanAlgorithm">the Feynman algorithm</a>: "1) Write down a problem. 2) Think very
hard. 3) Write down the solution." This mystical view of genius is
both unapproachable and discouraging. It's also unrealistic, but
it's hard for folks to know it's unrealistic if we don't write down
our thinking process for others to follow. By writing down the
process of finding an answer, as well as the rationale for the
answer, folks around us can being to <em>learn from</em> our decisions
rather than simply being <em>directed</em> by them" (page 85)</p>
</blockquote>
<hr>
<blockquote>
<p>"Barbara Minto, whose The Pyramid Principle is the most influential
work on effective business communication, is also a big fan of
structure: "Controlling the sequence in which you present your ideas
is the single most important act necessary to clear writing. The
clearest sequence is always to give the summarizing idea before you
give the individual ideas being summarized. I cannot emphasize this
point too much."" (page 96)</p>
</blockquote>
<hr>
<blockquote>
<p>"<a href="https://jvns.ca/blog/brag-documents/">brag document</a>" (page 106)</p>
</blockquote>
<hr>
<blockquote>
<p>"Whether your company does ad-hoc promotions or uses a calibration
process, promotions are a team activity and as Julia Grace, then of
Slack, advised me once during a job search, "Don't play team games
along, you'll lose."" (page 110)</p>
</blockquote>
<hr>
<blockquote>
<p>"Share weekly notes of your work to your team and stakeholders in a
way that other folks can get access to your notes if they're
interested" (page 128)</p>
</blockquote>
<hr>
<blockquote>
<p>"The flying wedge pattern of one senior leader joining a company and
then bringing on their previous coworkers is a well-known and
justifiably-despised pattern that relies on this built-in
referrer-as-sponsor, but it doesn't have to be toxic if done
sparingly." (page 139)</p>
</blockquote>
<hr>
<blockquote>
<p>"There are some wonderful engineering leaders creating pockets of
equitable access to Staff-plus roles, but those pockets can quickly
turn into a <a href="https://lethain.com/values-oasis/">Values Oasis</a> that can't sustain itself once the
sponsoring leader departs the company or changes roles." (page 139)</p>
</blockquote>
<hr>
<blockquote>
<p>"Back in 2012, Patrick McKenzie wrote <a href="https://www.kalzumeus.com/2012/01/23/salary-negotiation/">Salary Negotiation</a>, which
has since become the defacto [sic] guide to negotiating salaries for
software engineers." (page 145)</p>
</blockquote>
<hr>
<blockquote>
<p>"Staff-plus is all about enabling other people to do better work -
to be a force multiplier." (page 184, from Bert Fan)</p>
</blockquote>
<hr>
<blockquote>
<p>"This is not a meritocracy and your professional network is
important." (page 186, from Bert Fan)</p>
</blockquote>
<hr>
<blockquote>
<p>"To reach Staff Engineer, you have to know and do more than what you
currently know." (page 209, from Ritu Vincent)</p>
</blockquote>
<hr>
<blockquote>
<p>"A quote I love from Seneca is <em>"Luck is what happens when
preparation meets opportunity."</em>" (page 267, from Damian
Schenkelman)</p>
</blockquote>
<hr>
<blockquote>
<p>"In the quest for efficiency over effectiveness, many companies trap
their managers in a staggering amount of coordination and
bureaucracy." (page 309)</p>
</blockquote>
<hr>
<blockquote>
<p>"When we talk about designing a Staff-plus engineer interview loop,
the first thing to talk about is that absolutely no one is confident
their Staff-plus interview loop works well." (page 312)</p>
</blockquote>    
    ]]></description>
<link>http://planspace.org/20210919-staff_engineer_by_larson/</link>
<guid>http://planspace.org/20210919-staff_engineer_by_larson/</guid>
<pubDate>Sun, 19 Sep 2021 12:00:00 -0500</pubDate>
</item>
  </channel>
</rss>
