<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>plan ‚ûî space</title>
    <link>http://planspace.org/</link>
    <description>plan space from outer nine</description>
    <language>en-us</language>
    <atom:link href="http://planspace.org/rss.xml" rel="self" type="application/rss+xml" />
<item>
<title>Getting var(X/Y) can be hard</title>
<description><![CDATA[

<p>There generally isn't an analytic solution for the variance of the
ratio of two random variables (or probability distributions).
Denominators around zero are a problem, and with variance large
relative to mean, the approximation using the delta method may be
worse than you expect.</p>
<p>If you're dividing by something that can be close to zero, the results
can get big, which affects variance. Dividing two standard normal
distributions gives you a <a href="https://en.wikipedia.org/wiki/Cauchy_distribution" title="Wikipediia: Cauchy distribution">Cauchy distribution</a>, for example, and
the variance there is undefined. So if you're near zero, watch out!
Variance may not make sense, and can at least be hard to estimate.</p>
<p>The approximation of the variance of a ratio using the
<a href="https://en.wikipedia.org/wiki/Delta_method" title="Delta method">delta method</a> is:</p>
<p>\[ \text{var} \left( \frac{X}{Y} \right) \approx \frac{1}{\overline{Y}^2} \text{var} \left( X \right) + \frac{ \overline{X}^2 }{ \overline{Y}^4 } \text{var} \left( Y \right) - 2 \frac{ \overline{X} }{ \overline{Y}^3 } \text{cov} \left( X, Y \right) \]</p>
<p>See <a href="https://www.stat.cmu.edu/~hseltman/files/ratio.pdf" title="Approximations for Mean and Variance of a Ratio">Seltman</a> for a derivation. Also presented in <a href="/20211229-trustworthy_online_controlled_experiments/" title="Trustworthy Online Controlled Experiments">Kohavi et al.</a>
and <a href="http://alexdeng.github.io/public/files/WSDM2017draft.pdf" title="Trustworthy analysis of online A/B tests: Pitfalls, challenges and solutions">Deng et al.</a></p>
<p>This is an approximation based on <a href="https://en.wikipedia.org/wiki/Taylor%27s_theorem" title="Wikipedia: Taylor's theorem">Taylor series expansion</a>, and it
isn't always super close, even for seemingly simple examples like this
one in <a href="https://www.r-project.org/" title="The R Project for Statistical Computing">R</a>, with uniform distributions:</p>
<!-- set.seed(42) -->

<pre><code class="language-r">approx_var_ratio &lt;- function(x, y) {
  (1 / mean(y)^2) * var(x) +
    (mean(x)^2 / mean(y)^4) * var(y) -
    2 * (mean(x) / mean(y)^3) * cov(x, y)
}

x = runif(1000, min=14, max=26)
y = runif(1000, min= 4, max=16)

approx_var_ratio(x, y)
## 0.6286227
var(x/y)
## 1.189362</code></pre>

<p>That's no fluke of sampling. The delta-based formula is consistently
under 60% of the empirical value, for these parameters.</p>
<p>It isn't just a weirdness of the uniform distribution either. The
approximation is agnostic to distribution‚Äîwhich is another hint that
it can't be perfectly right, at least not always. Here's an example
with Gaussians, truncated so there's no risk of outliers near zero:</p>
<!--
install.packages("truncnorm")
library(truncnorm)

set.seed(42)
-->

<pre><code class="language-r">x = rtruncnorm(1000, a=14, mean=20, b=26, sd=3)
y = rtruncnorm(1000, a= 4, mean=10, b=16, sd=3)

approx_var_ratio(x, y)
## 0.3323209
var(x/y)
## 0.5294384</code></pre>

<p>That's still a substantial underestimate.</p>
<p>The approximation gets better when means are bigger relative to
variances. For example:</p>
<!-- set.seed(42) -->

<pre><code class="language-r">x = rnorm(1000, mean=200, sd=3)
y = rnorm(1000, mean=100, sd=3)

approx_var_ratio(x, y)
## 0.004369166
var(x/y)
## 0.004381438</code></pre>

<p>Now it's quite close.</p>
<p>A distribution with appreciable mass near zero has a large variance
relative to its mean, but you don't have to be near zero for the
relative spread of the distribution to affect the quality of the
approximation. Especially clear of zero, I think Monte Carlo
estimation can be better than using the formula based on the delta
method.</p>
<p>In conclusion, approximations are approximations. Do you really <em>need</em>
a ratio?</p>
<p>Stay safe out there!</p>
<!-- mathjax for formulas -->

    ]]></description>
<link>http://planspace.org/20211230-getting_var_ratio_can_be_hard/</link>
<guid>http://planspace.org/20211230-getting_var_ratio_can_be_hard/</guid>
<pubDate>Thu, 30 Dec 2021 12:00:00 -0500</pubDate>
</item>
<item>
<title>Trustworthy Online Controlled Experiments</title>
<description><![CDATA[

<p>The <a href="https://exp-platform.com/hippo/" title="HiPPO FAQ">HiPPO</a> on the cover represents the ‚ÄúHighest Paid Person's
Opinion‚Äù and the <a href="https://experimentguide.com/" title="Trustworthy Online Controlled Experiments: A Practical Guide to A/B Testing">book</a> is about making decisions in industry based
on experiments instead. It includes <a href="https://en.wikipedia.org/wiki/Twyman%27s_law">Twyman's Law</a> and much more and
is <em>the</em> text to read for online A/B tests. It does have some of the
character of class notes from a busy prof, with details left in other
papers, and the occasional misattribution (quote page 153) and error
(‚Äúdue to the increase in power that comes from the increase in power‚Äù
page 170). Indispensable, but some assembly required.</p>
<p><img alt="cover" src="cover.jpg"></p>
<hr>
<blockquote>
<p>"In statistics, this [Overall Evaluation Criterion (OEC)] is often
called the <em>Response</em> or <em>Dependent</em> variable (Mason, Gunst and Hess
1989, Box, Hunter and Hunter 2005); other synonyms are <em>Outcome</em>,
<em>Evaluation</em> and <em>Fitness Function</em> (Quarto-vonTivadar 2006).
Experiments can have multiple objectives and analysis can use a
balanced scorecard approach (Kaplan and Norton 1996), although
selecting a single metric, possibly as a weighted combination of
such objectives is highly desired and recommended (Roy 2001, 50,
405-429)." (page 7)</p>
</blockquote>
<p>The Roy <a href="https://www.wiley.com/en-us/Design+of+Experiments+Using+The+Taguchi+Approach%3A+16+Steps+to+Product+and+Process+Improvement-p-9780471361015">citation</a> is "Design of Experiments Using The Taguchi
Approach: 16 Steps to Product and Process Improvement". "Step 16" is
"Case studies", so I think Roy has confused "step" with "chapter".</p>
<hr>
<blockquote>
<p>"However, Google's tweaks to the color scheme
[<a href="https://www.nytimes.com/2009/03/01/business/01marissa.html">the 41 blues test</a>] ended up being substantially positive on
user engagement (note that Google does not report on the results of
individual changes) and led to a strong partnership between design
and experimentation moving forward." (page 16)</p>
</blockquote>
<p>This is an interesting take... They say (while saying they can't
support it with evidence) that <a href="https://www.nytimes.com/2009/03/01/business/01marissa.html">the 41 blues test</a> was good in
multiple ways, while the popular lore is mostly about how at least one
designer cited that experimentation as the <a href="http://www.zeldman.com/2009/03/20/41-shades-of-blue/">reason</a> they quit
Google. Hmm.</p>
<hr>
<blockquote>
<p>"One useful concept to keep in mind is <a href="https://tcagley.wordpress.com/2016/01/30/how-to-measure-anything-chapter-7-quantifying-the-value-of-information/">EVI</a>: Expected Value of
Information from Douglas Hubbard (2014), which captures how
additional information can help you in decision making." (page 24)</p>
</blockquote>
<hr>
<blockquote>
<p>"If we use <em>purchase indicator</em> (i.e., did the user purchase yes/no,
without regard to the purchase amount) instead of using
<em>revenue-per-user</em> as our OEC, the standard error will be smaller,
meaning that we will not need to expose the experiment to as many
users to achieve the same sensitivity." (page 32)</p>
</blockquote>
<p>This strikes me as an oversimplification... The measurements aren't on
the same scale, for one, so what does it mean to have a smaller
standard error, exactly? The two are testing different things... You
could imagine a world where the experimental condition convinces 100%
of users to make a $1 purchase, but stops the 5% of users who were
previously making $100 purchases. That's not good. I bet the authors
meant something more precise, and I wish they would have said what.</p>
<hr>
<blockquote>
<p>"In the analysis of controlled experiments, it is common to apply
the Stable Unit Treatment Value Assumption (SUTVA) (Imbens and Rubin
2015), which states that experiment units (e.g., users) do not
interfere with one another." (page 43)</p>
</blockquote>
<hr>
<blockquote>
<p>"Sample Ratio Mismatch (SRM)"</p>
</blockquote>
<hr>
<blockquote>
<p>"Zhao et al. (2016) describe how Treatment assignment was done at
Yahoo! using the <a href="https://en.wikipedia.org/wiki/Fowler%E2%80%93Noll%E2%80%93Vo_hash_function">Fowler-Noll-Vo hash function</a>, which sufficed
for single-layer randomization, but which failed to properly
distribute users in multiple concurrent experiments when the system
was generalized to overlapping experiments. Cryptographic hash
functions like MD5 are good (Kohavi et al. 2009) but slow; a
non-cryptographic function used at Microsoft is Jenkins SpookyHash
(<a href="https://www.burtleburtle.net/bob/hash/spooky.html">www.burtleburtle.net/bob/hash/spooky.html</a>)." (page 47)</p>
</blockquote>
<hr>
<blockquote>
<p>"For Bing, over 50% of US traffic is from bots, and that number is
higher than 90% in China and Russia." (page 48)</p>
</blockquote>
<hr>
<blockquote>
<p>"<em>Goal metrics</em>, also called <em>success metrics</em> or <em>true north
metrics</em>, reflect what the organization ultimately cares about."</p>
<p>"<em>Driver metrics</em>, also called <em>sign post metrics</em>, <em>surrogate
metrics</em>, <em>indirect</em> or <em>predictive metrics</em>, tend to be
shorter-term, faster-moving, and more-sensitive metrics than goal
metrics." (page 91)</p>
<p>"<em>Guardrail metrics</em> guard against violated assumptions and come in
two types: metrics that protect the business and metrics that assess
the trustworthiness and internal validity of experiment results."
(page 92)</p>
</blockquote>
<hr>
<blockquote>
<p>"Between 1945 and 1960, the federal Canadian government paid 70
cents a day per orphan to orphanages, and psychiatric hospitals
received $2.25 per day, per patient. Allegedly, up to 20,000
orphaned children were falsely certified as mentally ill so the
Catholic Church could get $2.25 per day, per patient (Wikipedia
contributors, Data dredging 2019)." (page 101)</p>
</blockquote>
<hr>
<blockquote>
<p>"... many unconstrained metrics are gameable. A metric that measures
ad revenue <em>constrained</em> to space on the page or to a measure of
quality is a much better metric to ensure a high-quality user
experience." (page 101)</p>
</blockquote>
<hr>
<blockquote>
<p>"Generally, we recommend using metrics that measure user value and
actions." (page 101)</p>
</blockquote>
<hr>
<blockquote>
<p>"Combining Key Metrics into an OEC"</p>
<p>"Given the common situation where you have multiple goal and driver
metrics, what do you do? Do you need to choose just one metric, or
do you keep more than one? Do you combine them all into single
combination metric?"</p>
<p>"While some books advocate focusing on just one metric (<em>Lean
Analytics</em> (Croll and Yoskovitz 2013) suggest the One Metric that
Matters (OMTM) and <em>The 4 Disciplines of Execution</em> (McChesney,
Covey and Huling 2012) suggest focusing on Wildly Important Goal
(WIG)), we find that motivating but an oversimplification. Except
for trivial scenarios, there is usually no single metric that
captures what a business is optimizing for. Kaplan and Norton (1996)
give a good example: imagine entering a modern jet airplane. Is
there a single metric that you should put on the pilot's dashboard?
Airspeed? Altitude? Remaining fuel? You know the pilot must have
access to these metrics and more. When you have an online business,
you will have several key goal and driver metrics, typically
measuring user engagement (e.g., active days, sessions-per-user,
clicks- per-user) and monetary value (e.g., revenue-per-user). There
is usually no simple single metric to optimize for."</p>
<p>"In practice, many organizations examine multiple key metrics, and
have a mental model of the tradeoffs they are willing to accept when
they see any particular combination. For example, they may have a
good idea about how much they are willing to lose (churn) users if
the remaining users increase their engagement and revenue to more
than compensate. Other organizations that prioritize growth may not
be willing to accept a similar tradeoff."</p>
<p>"Oftentimes, there is a mental model of the tradeoffs, and devising
a single metric ‚Äî an OEC ‚Äî that is a weighted combination of the
such objectives (Roy 2001, 50, 405-429) may be the more desired
solution. And like metrics overall, ensuring that the metrics and
the combination are not gameable is critical (see <em>Sidebar:
Gameability</em> in Chapter 6). For example, basketball scoreboards
don't keep track of shots beyond the two- and three-point lines,
only the combined score for each team, which is the OEC. FICO credit
scores combine multiple metrics into a single score ranging from 300
to 850. The ability to have a single summary score is typical in
sports and critical for business. A single metric makes the exact
definition of success clear and has a similar value to agreeing on
metrics in the first place: it aligns people in an organization
about the tradeoffs. Moreover, by having the discussion and making
the tradeoffs explicit, there is more consistency in decision making
and people can better understand the limitations of the combination
to determine when the OEC itself needs to evolve. This approach
empowers teams to make decisions without having to escalate to
management and provides an opportunity for automated searches
(parameter sweeps)."</p>
<p>"If you have multiple metrics, one possibility proposed by Roy
(2001) is to normalize each metric to a predefined range, say 0-1,
and assign each a weight. Your OEC is the weighted sum of the
normalized metrics." (pages 104-105)</p>
</blockquote>
<hr>
<blockquote>
<p>"GoodUI.org summarizes many UI patterns that win [A/B tests]
repeatedly." (page 113)</p>
</blockquote>
<hr>
<blockquote>
<p>"Experiment randomization can also act as a great instrumental
variable." (page 114)</p>
</blockquote>
<p>Hmm! I guess this would be the case if you found the experiment had
some effect on <em>X</em>, and then you were interested in further effects of
<em>X</em> on other things. See: <a href="/20210430-a_simple_instrumental_variable/">A simple Instrumental Variable</a>.</p>
<hr>
<p><a href="https://www.nber.org/papers/w17345">The Effect of Providing Peer Information on Retirement Savings Decisions</a> (ref page 119; abstract shown here)</p>
<blockquote>
<p>We conducted a field experiment in a 401(k) plan to measure the
effect of disseminating information about peer behavior on savings.
Low-saving employees received simplified plan enrollment or
contribution increase forms. A randomized subset of forms stated the
fraction of age-matched coworkers participating in the plan or
age-matched participants contributing at least 6% of pay to the
plan. We document an oppositional reaction: the presence of peer
information decreased the savings of non-participants who were
ineligible for 401(k) automatic enrollment, and higher observed peer
savings rates also decreased savings. Discouragement from upward
social comparisons seems to drive this reaction.</p>
</blockquote>
<p>Hmm! Usually peer effects are supposed to be so great...</p>
<hr>
<blockquote>
<p>"For example, Bing and Google's scaled-out human evaluation programs
are fast enough to use alongside the online controlled experiment
results to determine whether to launch the change." (page 131)</p>
</blockquote>
<hr>
<blockquote>
<p>"What customers say in a focus group setting or a survey may not
match their true preferences. A well-known example of this
phenomenon occurred when Philips Electronics ran a focus group to
gain insight into teenagers' preferences for boom box features. The
focus group attendees expressed a strong preference for yellow boom
boxes during the focus group, characterizing black boom boxes as
‚Äúconservative.‚Äù Yet when attendees exited the room and were given
the chance to take home a boom box as a reward for their
participation, most chose black (Cross and Dixit 2005)." (page 132)</p>
</blockquote>
<hr>
<blockquote>
<p>"Note that sophisticated modeling may be necessary to infer the
impact, with an online example of ITS [Interrupted Time Series]
being Bayesian Structural Time Series analysis (Charles and Melvin
2004)." (page 140)</p>
</blockquote>
<hr>
<p>"Interleaved Experiments" (page 141) are when you have two ranking
methods and you interleave their results (removing duplicates) and see
which ones get more clicks. Seems neat.</p>
<hr>
<blockquote>
<p>"More active users are simply more likely to do a broad range of
activities. Using activity as a factor is typically important."
(page 148)</p>
</blockquote>
<hr>
<p>In character for a book on RCTs, they point out
<a href="https://experimentguide.com/refuted_observational_studies/">Refuted Causal Claims from Observational Studies</a>. (pages 147-149)</p>
<hr>
<blockquote>
<p>"Indeed, the most difficult part of instrumentation is getting
engineers to instrument in the first place." (page 165)</p>
</blockquote>
<hr>
<blockquote>
<p>"The real measure of success is the number of experiments that can
be crowded into 24 hours." (quoting Thomas A. Edison, page 171)</p>
</blockquote>
<hr>
<blockquote>
<p>"The visualization tool is not just for per-experiment results but
is also useful for pivotting to <strong>per-metric results</strong> across
experiments. While innovation tends to be decentralized and
evaluated through experimentation, the global health of key metrics
is usually closely monitored by stakeholders." (page 181)</p>
</blockquote>
<hr>
<blockquote>
<p>"Assuming Treatment and Control are of equal size, the total number
of samples you need to achieve 80% power can be derived from the
power formula above, and is approximately as shown in Equation 17.8
(van Belle 2008):</p>
</blockquote>
<p>\[ n \approx \frac{16 \sigma^2 }{ \delta^2 } \]</p>
<blockquote>
<p>where, \( \sigma^2 \) is the sample variance, and \( \delta \)
is the difference between Treatment and Control." (page 189)</p>
</blockquote>
<hr>
<blockquote>
<p>"How can we ensure that Type I and Type II errors are still
reasonably controlled under multiple testing? There are many well
studied approaches; however, most approaches are either simple but
too conservative, or complex and hence less accessible. For example,
the popular Bonferroni correction, which uses a consistent but much
smaller p-value threshold (0.05 divided by the number of tests),
falls into the former category. The Benjamini-Hochberg procedure
(Hochberg and Benjamini 1995) uses varying p-value thresholds for
different tests and it falls into the latter category." (page 191)</p>
</blockquote>
<p>Benjamini‚ÄìHochberg (<a href="https://en.wikipedia.org/wiki/False_discovery_rate#BH_procedure" title="Benjamini‚ÄìHochberg procedure">wiki</a>, <a href="https://www.statisticshowto.com/benjamini-hochberg-procedure/" title="How to Run the Benjamini‚ÄìHochberg procedure">how-to</a>) doesn't seem so bad, either.
Sort of the flavor of a QQ plot, almost?</p>
<hr>
<p>Page 192 (section on "Fisher's Meta analysis") has a bunch on how to
combine p-values from multiple experiments.</p>
<hr>
<p>Pages 194-195 discuss ratio metrics and figuring out the variance of a
ratio using the <a href="https://en.wikipedia.org/wiki/Delta_method" title="Delta method">delta method</a>, as referenced in <a href="http://alexdeng.github.io/public/files/WSDM2017draft.pdf" title="Trustworthy analysis of online A/B tests: Pitfalls, challenges and solutions">Deng et al.</a>
¬ß4.2. See also <a href="https://www.stat.cmu.edu/~hseltman/files/ratio.pdf">Seltman's note</a> deriving the result. It is a little
bit of a weird formula, but the book makes it seem rather fancier than
it really is, I think, and their motivation doesn't seem to be
strictly relevant.</p>
<hr>
<p>On page 197 they mention <a href="https://exp-platform.com/Documents/2013-02-CUPED-ImprovingSensitivityOfControlledExperiments.pdf" title="Improving the Sensitivity of Online Controlled Experiments by Utilizing Pre-Experiment Data (CUPED: Controlled-experiment Using Pre-Experiment Data)">CUPED</a>.</p>
<hr>
<blockquote>
<p>"While you can always resort to bootstrap for conducting the
statistical test by finding the tail probabilities, it gets
expensive computationally as data size grows. On the other hand, if
the statistic follows a normal distribution asymptotically, you can
estimate variance cheaply. For example, the asymptotic variance for
quantile metrics is a function of the density (Lehmann and Romano
2005). By estimating density, you can estimate variance." (page 199)</p>
</blockquote>
<p>The citation is <a href="https://link.springer.com/book/10.1007/0-387-27605-X" title="Testing Statistical Hypotheses">Testing Statistical Hypotheses</a>.</p>
<hr>
<blockquote>
<p>"When conducting t-tests to compute p-values, the distribution of
p-values from repeated trials [of A/A tests] should be close to a
uniform distribution." (page 200)</p>
</blockquote>
<hr>
<blockquote>
<p>"Bing uses continuous A/A testing to identify a carry-over effect
(or residual effect), where previous experiments would impact
subsequent experiments run on the same users." (page 201)</p>
</blockquote>
<hr>
<blockquote>
<p>"We highly recommend running continuous A/A tests in parallel with
other experiments to uncover problems, including distribution
mismatches and platform anomalies." (page 201)</p>
</blockquote>
<hr>
<p>This is wild:</p>
<blockquote>
<p>"The book <em>A/B Testing: The Most Powerful Way to Turn Clicks into
Customers</em> (Siroker and Koomen 2013) suggests an incorrect procedure
for ending experiments: ‚ÄúOnce the test reaches statistical
significance, you'll have your answer,‚Äù and ‚ÄúWhen the test has
reached a statistically significance conclusion ...‚Äù (Kohavi 2014).
The statistics commonly used assume that a single test will be made
at the end of the experiment and ‚Äúpeeking‚Äù violates that assumption,
leading to many more false positives than expected using classical
hypothesis testing.</p>
<p>Early versions of Optimizely encouraged peeking and thus early
stopping, leading to many false successes. When some experimenters
started to run A/A tests, they realized this, leading to articles
such as ‚ÄúHow Optimizely (Almost) Got Me Fired‚Äù (Borden 2014). To
their credit, Optimizely worked with experts in the field, such as
Ramesh Johari, Leo Pekelis, and David Walsh, and updated their
evaluations, dubbing it ‚ÄúOptimizely's New Stats Engine‚Äù (Pekelis
2015, Pekelis, Walsh and Johari 2015). They address A/A testing in
their glossary (Optimizely 2018a)." (page 203)</p>
</blockquote>
<p>Their whole job!</p>
<hr>
<blockquote>
<p>"Always run a series of A/A tests before utlizing an A/B testing
system. Ideally, simulate a thousand A/A tests and plot the
distribution of p-values. If the distribution is far from uniform,
you have a problem. Do not trust your A/B testing system before
resolving the issue." (page 205)</p>
</blockquote>
<hr>
<p><a href="https://en.wikipedia.org/wiki/Rubin_causal_model">Rubin causal model</a>,
page 226</p>
<!-- mathjax for formulas -->

    ]]></description>
<link>http://planspace.org/20211229-trustworthy_online_controlled_experiments/</link>
<guid>http://planspace.org/20211229-trustworthy_online_controlled_experiments/</guid>
<pubDate>Wed, 29 Dec 2021 12:00:00 -0500</pubDate>
</item>
<item>
<title>Habits for 2022: ‚Äúratio‚Äù</title>
<description><![CDATA[

<p>In 2021, I worked on improving my <a href="/20211030-my_daily_routine/" title="My daily routine">daily routine</a>. I need to keep
working on it, but as 2022 approaches, I was thinking about other
changes to make. In the CGP Grey ‚Äú<a href="https://www.youtube.com/watch?v=NVGuFdX5guE&amp;ab_channel=CGPGrey" title="CGP Grey: Your Theme">theme</a>‚Äù style, I might say
‚Äúratio‚Äù as in ‚Äúratio of quality to junk.‚Äù Here are some practices I
think can support this:</p>
<ul>
<li><strong>Only check the news on Sundays.</strong> I check the news (and my phone
   generally) too much, and it doesn't really make my life better to
   check the news several times a day. Sunday is the day for news.</li>
<li><strong>Keep a book ready in Kindle app.</strong> When I <em>do</em> pick up my phone,
   there should be something worthwhile there. Read a book, or put the
   phone down.</li>
<li><strong>Skim more.</strong> I overcommit to reading things the slow way, which
   takes a lot of time and prevents me from opening books I don't want
   to commit to. I hate the phrase ‚Äúspeed reading‚Äù because I don't
   think it's real reading, but skimming in the sense of ‚Äúsurvey‚Äù in
   <a href="https://en.wikipedia.org/wiki/SQ3R" title="survey, question, read, recite and review">SQ3R</a> is a distinct thing that's worthwhile. I recently did a
   quick <a href="/20211213-practical_data_science_with_python_by_george/" title="Practical Data Science with Python, by George">book review</a> based on skimming and I thought it was fair
   and I learned a few things too. Skimming could get me connected
   with my backlog of nonfiction.</li>
</ul>    
    ]]></description>
<link>http://planspace.org/20211228-habits_for_2022_ratio/</link>
<guid>http://planspace.org/20211228-habits_for_2022_ratio/</guid>
<pubDate>Tue, 28 Dec 2021 12:00:00 -0500</pubDate>
</item>
<item>
<title>naldaramjui.com is closed</title>
<description><![CDATA[

<p>Sorry! After over ten years, I've turned off my old site for
practicing <a href="https://en.wikipedia.org/wiki/Test_of_Proficiency_in_Korean" title="Test of Proficiency in Korean">TOPIK</a> test questions.</p>
<ul>
<li>The <a href="http://www.topik.go.kr/">official TOPIK site</a> is much better than it used to be. It
   has practice questions <a href="https://www.topik.go.kr/TWSTDY/TWSTDY0010.do">every day</a>. The interface seems to be
   still largely in Korean even if you switch the language selector at
   the top, but it's not bad! You can find a lot of resources there.</li>
<li>The site ran on Google App Engine, using their oldest Python setup.
   It had been pretty stable, but in the last year or so it stopped
   handling cookies right, breaking the answer-evaluating
   functionality, and it didn't seem worth re-working to restore.</li>
<li>Some bots were abusing the simple sign-up form to sign up people
   who didn't want to be signed up, which was not cool.</li>
</ul>
<p>If you want to see what other kinds of things I'm up to, check out the
<a href="/">blog</a>! ÌôîÏù¥ÌåÖ! üòÑ</p>
<hr>
<p><img alt="His name is Î¨¥Í∏∞." src="fs.jpg"></p>    
    ]]></description>
<link>http://planspace.org/20211227-naldaramjui_is_closed/</link>
<guid>http://planspace.org/20211227-naldaramjui_is_closed/</guid>
<pubDate>Mon, 27 Dec 2021 12:00:00 -0500</pubDate>
</item>
<item>
<title>The Genetic Lottery, by Harden</title>
<description><![CDATA[

<p>Harden has something for everyone to dislike, telling the left that
environment isn't everything and telling the right that
self-determination isn't everything. I think her recommendation to use
genetic controls in statistical analysis is interesting but hard.</p>
<blockquote>
<p>"The biggest contribution of genetics to the social sciences is to
give researchers an additional set of tools to do basic research by
measuring and statistically controlling for a variable‚ÄîDNA‚Äîthat has
previously been very difficult to measure and statistically control
for." (page 192)</p>
</blockquote>
<p><a href="https://press.princeton.edu/books/hardcover/9780691190808/the-genetic-lottery" title="The Genetic Lottery: Why DNA Matters for Social Equality">The Genetic Lottery</a> has received criticism. Here's
<a href="https://www.lareviewofbooks.org/article/why-dna-is-no-key-to-social-equality-on-kathryn-paige-hardens-the-genetic-lottery" title="Why DNA Is No Key to Social Equality: On Kathryn Paige Harden‚Äôs ‚ÄúThe Genetic Lottery‚Äù">Henn et al.</a>:</p>
<blockquote>
<p>"Ultimately, [Harden's] focus on genetics as a fundamental cause of
social inequality reduces her version of social justice to
benevolent paternalism."</p>
</blockquote>
<p>I think Harden does a fair job of being clear that genetics can be a
cause, but certainly not the only cause, and not a cause that can't be
redressed. On ‚Äúbenevolent paternalism,‚Äù I think Henn et al. intend the
phrase to have negative connotation, but couldn't any attempt at
social justice (or social safety nets) be referred to in this way?</p>
<p>As in the ‚Äúequality vs. equity‚Äù <a href="https://interactioninstitute.org/illustrating-equality-vs-equity/">cartoon</a> on page 162, I take it as
a given that everyone should be able to see over the fence‚Äîthe hard
question is how high the fence that we're trying to get everybody over
is. What's the level that should be guaranteed, and what are
exceptions for extreme cases?</p>
<p>Bird has <a href="https://massivesci.com/articles/genetic-lottery-review-paige-harden-kevin-bird/" title="The Genetic Lottery is a bust for both genetics and policy">issues</a> with the presentation of the science, and I agree
I would have liked more detail and precision in the presentation, but
Harden is also trying to reach a broad audience and cover a lot of
material. I think Bird is incorrect in his accusation that Harden
doesn't engage with the history of eugenics.</p>
<p>Harden's point that genetic confounds can affect optimal policy
recommendations seems meaningful to me. If parents with lots of books
in their home have kids who learn to read, does that imply giving
everyone a stack of books is all we need to do?</p>
<p>I think Harden is right that if the well-intentioned don't engage with
genetics, their impact is muted by confounding while the
ill-intentioned advertise ‚Äúforbidden knowledge‚Äù to the benefit of
none. But I don't expect a major shift in Harden's lifetime.</p>
<p><img alt="cover" src="cover.jpg"></p>
<hr>
<blockquote>
<p>"Building a commitment to egalitarianism on our genetic uniformity
is building a house on sand." (page 19)</p>
</blockquote>
<hr>
<blockquote>
<p>"A study of what is correlated with succeeding in an education
system doesn't tell you whether that system is good, or fair, or
just." (page 60)</p>
</blockquote>
<hr>
<p>Quoting the organizers of the <a href="https://www.fragilefamilieschallenge.org/">Fragile Families Challenge</a>:</p>
<blockquote>
<p>‚ÄúIf one measures our degree of understanding by our ability to
predict, then results ... suggest that our understanding of child
development and the life course is actually quite poor.‚Äù (page 70)</p>
</blockquote>
<hr>
<blockquote>
<p>"I think we must dismantle the false distinction between
‚Äúinequalities that society is responsible for addressing‚Äù and
‚Äúinequalities that are caused by differences in biology.‚Äù" (page 91)</p>
</blockquote>
<hr>
<p><a href="https://www.nber.org/papers/w22595">Understanding and Misunderstanding Randomized Controlled Trials</a></p>
<hr>
<p>Interesting unit: <a href="https://en.wikipedia.org/wiki/Centimorgan">centimorgan</a></p>
<hr>
<p><a href="https://www.nature.com/articles/nrg2322">Heritability in the genomics era ‚Äî concepts and misconceptions</a></p>
<blockquote>
<p>"half of the additive genetic variance is between families and half
is within families"</p>
</blockquote>
<hr>
<p><a href="https://www.nature.com/articles/456018a">Personal genomes: The case of the missing heritability</a></p>
<hr>
<p>Dang, I would like to see some worked examples for how heritabilities
are calculated...</p>
<hr>
<p><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2762790/">Individual Differences in Executive Functions Are Almost Entirely Genetic in Origin</a></p>
<hr>
<blockquote>
<p>"As Dostoevsky reminded us, ‚ÄúIt takes something more than
intelligence to act intelligently.‚Äù" (page 141, referring to <em>Crime
and Punishment</em>)</p>
</blockquote>
<hr>
<p><a href="https://www.pnas.org/content/115/31/E7275">Genetic analysis of social-class mobility in five longitudinal studies</a></p>
<p>In its conclusion, a sentiment shared by Harding:</p>
<blockquote>
<p>"A long-term goal of our sociogenomic research is to use genetics to
reveal novel environmental intervention approaches to mitigating
socioeconomic disadvantage."</p>
</blockquote>
<hr>
<p><a href="https://pubmed.ncbi.nlm.nih.gov/27337136/">Genetically-mediated associations between measures of childhood character and academic achievement</a></p>
<p>In Figure 7.3 of the book, a list based on that ref:</p>
<ul>
<li>grit<ul>
<li>passion and perseverance for long-term goals</li>
</ul>
</li>
<li>growth mindset<ul>
<li>belief that intelligence is malleable</li>
</ul>
</li>
<li>intellectual curiosity<ul>
<li>desire to think about difficult or new problems</li>
</ul>
</li>
<li>mastery orientation<ul>
<li>motivation to learn for the sake of learning</li>
</ul>
</li>
<li>self-concept<ul>
<li>belief that one is smart and capable of learning</li>
</ul>
</li>
<li>test motivation<ul>
<li>trying hard on tests</li>
</ul>
</li>
</ul>
<blockquote>
<p>"The SNPs correlated with non-cognitive skills were correlated with
<em>higher</em> risk for several mental disorders, including schizophrenia,
bipolar disorder, anorexia nervosa, and obsessive-compulsive
disorder. This result warns us against viewing the genetic variants
that are associated with going further in current systems of formal
education as being inherently ‚Äúgood‚Äù things. A single genetic
variant might make it a tiny bit more likely that someone will go
further in school, but that same variant might also elevate their
risk of developing schizophrenia or another serious mental
disorder." (page 144)</p>
</blockquote>
<hr>
<blockquote>
<p>"Unfortunately, the mistaken idea that genetic influences are an
impermeable barrier to social change is also widely endorsed not
just by those who are trying to naturalize inequality, but also by
their ideological and political opponents." (page 155)</p>
</blockquote>
<hr>
<p><a href="https://pubmed.ncbi.nlm.nih.gov/27359131/">Strong genetic overlap between executive functions and intelligence</a></p>
<hr>
<blockquote>
<p>"I could quote the Bible verse from Thessalonians that was quoted to
me as a child: ‚ÄúThe one who is unwilling to work shall not eat.‚Äù"
(page 212)</p>
</blockquote>
<hr>
<blockquote>
<p>"There is no measure of so-called ‚Äúmerit‚Äù that is somehow free of
genetic influence or untethered from biology." (page 247)</p>
</blockquote>    
    ]]></description>
<link>http://planspace.org/20211224-genetic_lottery_harden/</link>
<guid>http://planspace.org/20211224-genetic_lottery_harden/</guid>
<pubDate>Fri, 24 Dec 2021 12:00:00 -0500</pubDate>
</item>
<item>
<title>Correlation can help approximate variance of a difference</title>
<description><![CDATA[

<p>Even if random variables \( \textbf{A} \) and \( \textbf{Œ¥} \) are
uncorrelated, realized data \( A \) and \( \delta \) will happen
to have some covariance. If \( B = A + \delta \), then \(
\text{var}(B) = \text{var}(A) + \text{var}(\delta) + 2\text{cov}(A,
\delta) \), and estimating any one variance by the others will be off
by the \( 2\text{cov}(A, \delta) \) term. For \( \text{var}(\delta)
\), however, the estimate \( \text{var}(B)(1 - \text{corr}(A, B)^2)
\) will usually be closer to correct, at the cost of being slightly
biased. (But don't forget that doing \( \text{var}( \delta ) \)
directly is a great first choice.)</p>
<h3>Three methods for \( \text{var}( \delta ) \), when \( B = A + \delta \)</h3>
<p>The case of \( B = A + \delta \) (so that \( \delta = B - A \)) is
central to the paired t-test, for example. The variance of \( A \)
and \( B \) could each be large, but the variance of \( \delta \)
can still be small, making it easier to reject the null for \(
\textbf{Œ¥} \), for example.</p>
<p><em>Be careful: We're trying to estimate a variance, so we're interested
in the variance of the estimate of the variance, which can be
confusing. Hopefully the language is clear enough here.</em></p>
<h4>1) Variance of Differences</h4>
<p>The obvious thing to do is to subtract and calculate the variance of
the differences: \( \text{var}( \delta ) = \text{var}( B - A ) \).
This is a good idea and what you should do. It's unbiased and has low
variance (for the estimate of the true variance of \( \textbf{Œ¥}
\)).</p>
<p>Why not do it like this? Honestly I'm not sure. Maybe you don't have
complete data, and you're looking for a \( \delta_1 - \delta_2 \)
where the means of \( A_1 \) and \( A_2 \) are assumed equal so
you're using \( B_1 - B_2 \) but you want the (smaller) variance of
\( \delta_1 - \delta_2 \)? So you'll estimate some things with
complete cases even though the main effect is estimated with all \( B
\) data? Why not still use this method with the complete cases? Maybe
you're not using simple subtraction, but some more complex regression
with multiple variables? In that case, why not use variance of the
residuals directly? Do you just want a more complicated method?</p>
<h4>2) Difference of Variances</h4>
<p>By the <a href="/20201030-the_variance_sum_law_is_interesting/">variance sum law</a>, \( \text{var}( \textbf{B} ) =
\text{var}( \textbf{A} ) + \text{var}( \textbf{Œ¥} ) \), if \(
\textbf{A} \) and \( \textbf{Œ¥} \) are uncorrelated, so \(
\text{var}( \textbf{Œ¥} ) = \text{var}( \textbf{B} ) - \text{var}(
\textbf{A} ) \). Real data is not generally perfectly uncorrelated,
however: \( \text{var}(\delta) = \text{var}(B) - \text{var}(A) -
2\text{cov}(A, \delta) \). The covariance term is zero in
expectation, so estimating \( \text{var}(\delta) = \text{var}(B) -
\text{var}(A) \) is unbiased. But the \( 2\text{cov}(A, \delta) \)
is a kind of noise term, and adds variance to the estimate of \(
\text{var}( \textbf{Œ¥} )\).</p>
<h4>3) Using Correlation</h4>
<p>It isn't obvious, but using \( \text{var}(B)(1 - \text{corr}(A, B)^2)
\) to estimate \( \text{var}( \textbf{Œ¥} ) \) is much like using
the Difference of Variances, but with a generally smaller (but always
positive) error coming from \( \text{cov}(A, \delta) \). Proceeding
in small steps:</p>
<p>\[ \text{var}(\delta) = \text{var}(B) - \text{var}(A) - 2\text{cov}(A, \delta) \]</p>
<p>\[ \text{var}(\delta) = \text{var}(B) - \left( \text{var}(A) + 2\text{cov}(A, \delta) \right) \]</p>
<p>\[ \text{var}(\delta) = \text{var}(B) \left( 1 - \frac{\text{var}(A) + 2\text{cov}(A, \delta)}{ \text{var}(B) } \right) \]</p>
<p>\[ \text{var}(\delta) = \text{var}(B) \left( 1 - \frac{\text{var}(A)^2 + 2\text{var}(A)\text{cov}(A, \delta)}{ \text{var}(A) \text{var}(B) } \right) \]</p>
<p>This has been algebraic. Now add \( \text{cov}(A, \delta)^2 \) to
the fractional term's numerator. (This is an error in the estimate, to
get to the destination.)</p>
<p>\[ \text{var}(\delta) = \text{var}(B) \left( 1 - \frac{\text{var}(A)^2 + 2\text{var}(A)\text{cov}(A, \delta) + \text{cov}(A, \delta)^2 }{ \text{var}(A) \text{var}(B) } \right) \]</p>
<p>\[ \text{var}(\delta) = \text{var}(B) \left( 1 - \frac{ ( \text{var}(A) + \text{cov}(A, \delta))^2 }{ \text{var}(A) \text{var}(B) } \right) \]</p>
<p>The bit squared in the fraction's numerator is \( \sum{(A_i - \bar{A}
)^2} + \sum{(A_i - \bar{A} )(\delta_i - \bar{\delta})} \), which is
\( \sum{(A_i - \bar{A})(A_i - \bar{A} + \delta_i - \bar{\delta})}
\). Since \( B_i = A_i + \delta_i \) and \( \bar{B} = \bar{A} +
\bar{\delta} \), that's \( \sum{(A_i - \bar{A})(B_i - \bar{B})} \),
which is \( \text{cov}(A, B) \).</p>
<p>\[ \text{var}(\delta) = \text{var}(B) \left( 1 - \frac{ \text{cov}(A, B)^2 }{ \text{var}(A) \text{var}(B) } \right) \]</p>
<p>So at last, by definition:</p>
<p>\[ \text{var}(\delta) = \text{var}(B) \left( 1 - \text{corr}(A, B)^2 \right) \]</p>
<p>QED. The error introduced is \( \text{cov}(A, \delta)^2 /
\text{var}(A) \), which will tend to be considerably smaller in
absolute value than the \( 2\text{cov}(A, \delta) \) error in the
Difference of Variances method because \( 0 \le |\text{cov}(A,
\delta)| / \text{var}(A) \ll 2 \). This error is however positive in
expectation, so the estimate is biased: it will be slightly too small.</p>
<h3>Computational demonstration</h3>
<p>Generating 1,000 datasets where each of \( A \) and \( \delta \)
have 100 values drawn at random from Gaussians with variance 9 and 16,
respectively, the three methods above generate estimates of \(
\text{var}( \textbf{Œ¥} ) \) as follows:</p>
<pre><code>| Method                  | Mean var(d) estimate | var(estimate) |
|-------------------------|----------------------|---------------|
| Variance of Differences |                15.98 |          5.10 |
| Difference of Variances |                15.95 |         10.69 |
| Using Correlation       |                15.82 |          5.12 |</code></pre>

<p>As expected, the method Using Correlation comes out more below the
true value of 16 on average, but its variance is comparable to that of
the Variance of Differences.</p>
<p>The bias is already small with just 100 samples, and gets smaller
still as samples get bigger and sample correlations tend to be
smaller.</p>
<p>Here's clumsy <a href="https://www.r-project.org/">R</a> code to do the experiment:</p>
<pre><code class="language-r">trials = 1000
samples = 100

set.seed(0)
est1 = c()
est2 = c()
est3 = c()

for (i in 1:trials) {
  A = rnorm(samples, sd=3)  # var=9
  d = rnorm(samples, sd=4)  # var=16
  B = A + d  # var=25, in population sense
  est1 = c(est1, var(B - A))  # same as var(d)
  est2 = c(est2, var(B) - var(A))
  est3 = c(est3, var(B) * (1 - cor(A, B)^2))
}

mean(est1)
## [1] 15.98799
mean(est2)
## [1] 15.9451
mean(est3)
## [1] 15.82234
var(est1)
## [1] 5.09602
var(est2)
## [1] 10.69393
var(est3)
## [1] 5.117983</code></pre>
<!-- mathjax for formulas -->

    ]]></description>
<link>http://planspace.org/20211219-correlation_can_help_approximate_variance_of_a_difference/</link>
<guid>http://planspace.org/20211219-correlation_can_help_approximate_variance_of_a_difference/</guid>
<pubDate>Sun, 19 Dec 2021 12:00:00 -0500</pubDate>
</item>
<item>
<title>Practical Data Science with Python, by George</title>
<description><![CDATA[

<p>Packt sent me a copy of this <a href="https://www.packtpub.com/product/practical-data-science-with-python/9781801071970">book</a> to review. It's 621 pages with
the best and worst characteristics of a typical data science boot
camp: very broad, necessarily shallow, frequently not quite perfect.
Even an imperfect map can tell you a lot about the territory, and it
could be the right book for you.</p>
<p>George has pulled together a lot of material, some of it good. He
includes introductory Python and command line, enough SQL to be
confused about SQL, examples with Bitcoin prices, an idiosyncratic
survey of visualization, web scraping, statistics, and the big machine
learning models, including the <a href="/20211003-xgboost_lightgbm_catboost_briefly/">big three boosted tree algorithms</a>,
which I appreciate. He includes some NLP, and even some on ethics.</p>
<p>George's own list of omissions (page 571) illustrates what he thinks
is almost in scope:</p>
<ul>
<li>Recommender systems</li>
<li>Networks and graph analysis</li>
<li>Machine learning explainability</li>
<li>Test-driven development (TDD)</li>
<li>Reinforcement learning</li>
<li>Neural networks</li>
</ul>
<p>Maybe the moral is that ‚Äúdata science‚Äù is too big a topic for one
book. Trying to pack so much in has a cost. Here's the complete
section on ‚ÄúPaired t- and z-tests‚Äù:</p>
<blockquote>
<p>One last type of t- or z-test is the paired test. This is for paired
samples, like before-and-after treatments. For example, we could
measure the blood pressure of people before and after taking a
medication to see if there is an effect. A function that can be used
for this is <code>scipy.stats.ttest_rel</code>, which can be used like this:</p>
<p><code>scipy.stats.ttest_rel(before, after)</code></p>
<p>This will return a t-statistic and p-value like with other <code>scipy</code>
t-test functions.</p>
</blockquote>
<p>If you've never heard of a paired t-test before, it's great this book
tells you about it. You can start to ask questions like: Why is this a
separate test? Does it have some advantage over a regular t-test?
Hopefully you also question some parts of the book, as when Bayesian
methods are dismissed as ‚Äúmuch more complex to implement than a
t-test.‚Äù</p>
<p>This is a map that can point you in a lot of interesting directions,
which is valuable!</p>
<p><img alt="cover" src="cover.png"></p>    
    ]]></description>
<link>http://planspace.org/20211213-practical_data_science_with_python_by_george/</link>
<guid>http://planspace.org/20211213-practical_data_science_with_python_by_george/</guid>
<pubDate>Mon, 13 Dec 2021 12:00:00 -0500</pubDate>
</item>
<item>
<title>Significant Figures: Rounding destroys information</title>
<description><![CDATA[

<p>Significant Figures often requires rounding so that the correct level
of uncertainty is conveyed. Rounding is only supposed to happen at the
‚Äúend‚Äù of calculations, and results can vary depending on what is
considered an intermediate vs. a final result. In all cases, rounding
is harmful and only necessary because of the limitations of
Significant Figures.</p>
<p>It should be clear that 4.3 with an <a href="/20211209-significant_figures_gaussian_uncertainty/">uncertainty</a> \( \sigma \times
0.1 \) is not the same as 4.34 with the same uncertainty. It's
because Significant Digits can't say 4.34 without meaning the
uncertainty is \( \sigma \times 0.01 \) that we round to 4.3, if the
uncertainty is \( \sigma \times 0.1 \). But this has a cost: good
information is dropped.</p>
<p>Consider adding 1 + 1.4 + 1.4. If we do it in one go, we get 3.8 and
then round to 4 for significance. But what if this is done in two
steps? Maybe one team does 1 + 1.4, correctly reports their result as
2, and then a second team builds on that, adding 1.4 to get 3. The
rounding that Significant Figures requires degrades the quality of
results.</p>
<p>At some point it isn't worth tracking every digit in a result, but
Significant Figures often encourages dropping too many. It may even
give people the incorrect idea that if our uncertainty suggests two
significant figures, we can't have three figures in our best guess at
what the value is. We absolutely can, but this requires a system more
expressive than Significant Figures to report.</p>
<!-- mathjax for formulas -->

    ]]></description>
<link>http://planspace.org/20211212-significant_figures_rounding_destroys_information/</link>
<guid>http://planspace.org/20211212-significant_figures_rounding_destroys_information/</guid>
<pubDate>Sun, 12 Dec 2021 12:00:00 -0500</pubDate>
</item>
<item>
<title>‚ÄúShow your work‚Äù means ‚ÄúWrite a proof‚Äù</title>
<description><![CDATA[

<p>At any level of mathematics, ‚Äúshowing your work‚Äù should not be a
chore, but a chance to communicate an argument for correctness: to
write a proof. ‚ÄúWork‚Äù as ‚Äúproof‚Äù keeps mathematics relevant,
authentic, and interesting.</p>
<p>Many school math exercises can be answered with calculators. But how
do we know the calculator is correct? The difference between inductive
(‚Äúthe calculator has always been right‚Äù) and deductive (‚Äúthis answer
is proven correct‚Äù) is important. The answer itself isn't important;
it's the demonstration that the answer follows from the question.</p>
<p>High school geometry is sometimes pointed to as the first place
students encounter the idea of proofs. This need not be. Arithmetic is
a process of proof, using theorems of single-digit operations and
place-based algorithms to build new results. In this way, 2+2=4 is
used to demonstrate that 22+22=44, and so on. These could be called
constructive proofs.</p>
<p>‚ÄúWork‚Äù as ‚Äúproof‚Äù opens up the world of math. ‚ÄúChecking your work‚Äù is
finding a second proof. For the advanced student who might have been
bored with ‚Äúshowing work,‚Äù there is an invitation to find further
proofs, understand algorithms more deeply, and be creative, rather
than being limited to mechanical processes.</p>
<p>Computational fluency is not without value, but math is about much
more. We waste opportunities to deliver on teaching mathematical ways
of thinking if students don't realize that they're constructing
logical arguments in all their math classes.</p>    
    ]]></description>
<link>http://planspace.org/20211212-show_your_work_means_write_a_proof/</link>
<guid>http://planspace.org/20211212-show_your_work_means_write_a_proof/</guid>
<pubDate>Sun, 12 Dec 2021 12:00:00 -0500</pubDate>
</item>
<item>
<title>Bernoulli's Fallacy, by Clayton</title>
<description><![CDATA[

<p><a href="https://aubreyclayton.com/bernoulli"><em>Bernoulli's Fallacy</em></a> is that the likelihood of data given a
hypothesis is enough to make inferences about that hypothesis (or
others). Clayton covers historical and modern aspects of frequentist
statistics, and lays crises of replication at the feet of significance
testing. I find it largely compelling, though perhaps it neglects
problematic contributions of non-statistical pressures in modern
academic life. I'm generally on board re: Bayesian methods.</p>
<p><img alt="cover" src="cover.jpg"></p>
<hr>
<blockquote>
<p>"...statistical methods are a means of accounting for the epistemic
role of measurement error and uncertainty..." (page x)</p>
</blockquote>
<hr>
<blockquote>
<p>"...an effect‚Äîif it <em>is</em> found‚Äîis likely overstated and unlikely to
be replicable, a paradox known as the ‚Äú<a href="https://en.wikipedia.org/wiki/Winner%27s_curse">winner's curse</a>.‚Äù" (page
xi)</p>
</blockquote>
<hr>
<blockquote>
<p>The common theme is that there would be no need to continually treat
the symptoms of statistical misuse if the underlying disease were
addressed.</p>
<p>I offer the following for consideration:</p>
<p>Hypothesizing after the results of an experiment are known does not
necessarily present a problem and in fact is the way that most
hypotheses are ever constructed.</p>
<p>No penalty need be paid, or correction made, for testing multiple
hypotheses at once using the same data.</p>
<p>The conditions causing an experiment to be terminated are largely
immaterial to the inferences drawn from it. In particular, an
experimenter is free to keep conducting trials until achieving a
desired result, with no harm to the resulting inferences.</p>
<p>No special care is required to avoid ‚Äúoverfitting‚Äù a model to the
data, and validating the model against a separate set of test data
is generally a waste.</p>
<p>No corrections need to be made to statistical estimators (such as
the sample variance as an estimate of population variance) to ensure
they are ‚Äúunbiased.‚Äù In fact, by doing so the quality of those
estimators may be made worse.</p>
<p>It is impossible to ‚Äúmeasure‚Äù a probability by experimentation.
Furthermore, all statements that begin ‚ÄúThe probability is ...‚Äù
commit a category mistake. There is no such thing as ‚Äúobjective‚Äù
probability.</p>
<p>Extremely improbably events are not necessarily noteworthy or reason
to call into question whatever assumed hypotheses implied they were
improbable in the first place.</p>
<p>Statistical methods requiring an assumption of a particular
distribution (for example, the normal distribution) for the error in
measurement are perfectly valid whether or not the data ‚Äúactually
is‚Äù normally distributed.</p>
<p>It makes no sense to talk about whether data ‚Äúactually is‚Äù normally
distributed or could have been sampled from a normally distributed
population, or any other such consideration.</p>
<p>There is no need to memorize a complex menagerie of different tests
or estimators to apply to different kinds of problems with different
distributional assumptions. Fundamentally, all statistical problems
are the same.</p>
<p>‚ÄúRejecting‚Äù or ‚Äúaccepting‚Äù a hypothesis is not the proper function
of statistics and is, in fact, dangerously misleading and
destructive.</p>
<p>The point of statistical inference is not to produce the right
answers with high frequency, but rather to <em>always</em> produce the
inferences best supported by the data at hand when combined with
existing background knowledge and assumptions.</p>
<p>Science is largely not a process of falsifying claims definitively,
but rather assigning them probabilities and updating those
probabilities in light of observation. This process is endless. No
proposition apart from a logical contradiction should ever get
assigned probability 0, and nothing short of a logical tautology
should get probability 1.</p>
<p>The more unexpected, surprising, or contrary to established theory a
proposition seems, the more impressive the evidence must be before
that proposition is taken seriously.</p>
</blockquote>
<hr>
<p>Heavily influenced by <a href="https://www.cambridge.org/core/books/probability-theory/9CA08E224FF30123304E6D8935CF1A99">Probability Theory: The Logic of Science</a> by
Edwin Jaynes.</p>
<hr>
<blockquote>
<p>"We can, for example, set <em>s</em> = 0.01 and by 99 percent sure.
Bernoulli called this ‚Äúmoral certainty,‚Äù as distinct from absolute
certainty of the kind only logical deduction can provide." (page 8)</p>
</blockquote>
<hr>
<blockquote>
<p>"<em>statistics is both much easier and much harder than we have been
led to believe.</em>" (page 17, italics in original)</p>
</blockquote>
<hr>
<blockquote>
<p>"Aristotle's <em>Rhetoric</em> described ‚Äúthe Probably‚Äù (in Greek, <em>eikos</em>,
from <em>eoika</em> meaning ‚Äúto seem‚Äù) as ‚Äúthat which happens generally but
not invariably.‚Äù The context for this was his classification of the
arguments one could use in a courtroom or legislative debate, where
perfect logical deductions may not be available. He called this form
of argument an <em>enthymeme</em>, to be distinguished from the purely
logical form of argument known as the <em>syllogism</em>, which links
together a set of assumed premises to reach deductive
conclusions..." (page 22)</p>
</blockquote>
<hr>
<blockquote>
<p>"Hume's general point
[in <em>An Enquiry Concerning Human Understanding</em>], later referred to
as the <em>problem of induction</em>, was that we have no way of <em>knowing</em>
experience is a guide for valid conclusions about the future because
if we did, that claim could be based only on past experience." (page
35)</p>
</blockquote>
<hr>
<p>I kind of like the names Clayton uses in his tables of calculations:
"Prior probability" is normal, then "Sampling probability" is used for
the likelihood of the data, and then he multiplies them together to
get a "Pathway probability."</p>
<hr>
<blockquote>
<p>"Whether Bayes himself believed he had disproved Hume we have no way
of knowing. Some historians such as Stephen Stigler at the
University of Chicago have suggested that since Bayes did not find
the counterexample sufficiently convincing because it relied on some
assumptions he could not justify, he delayed publishing his results.
When presenting Bayes's results to the world, Price did not shy away
from emphasizing their philosophical and religious significance.
Contemporary reprints of the essay show Price intended the title to
be ‚ÄúA Method of Calculating the Exact Probability of All Conclusions
founded on Induction.‚Äù In his publication, he added this preamble:
‚ÄúThe purpose I mean is, to shew what reason we have for believing
that there are in the constitution of things fixt laws according to
which things happen, and that, therefore, the frame of the world
must be the effect of the wisdom and power of an intelligent cause;
and thus to confirm the argument taken from final causes for the
existence of the Deity.‚Äù That is, somewhere in the calculation of
probabilities for Bayes's rule, Price thought he saw evidence for
God." (page 41)</p>
</blockquote>
<hr>
<blockquote>
<p>"... logical deduction is just a special case of reasoning with
probabilities, in which all the probability values are zeros or
ones." (page 53)</p>
</blockquote>
<hr>
<blockquote>
<p>"Jaynes's essential point bears repeating: <em>probability is about
information.</em>" (page 68, italics in original)</p>
</blockquote>
<hr>
<blockquote>
<p>"Base rate neglect and the prosecutor's fallacy are the same thing,
and both are examples of Bernoulli's Fallacy." (page 103)</p>
</blockquote>
<hr>
<blockquote>
<p>"... a new general trend of collecting data in service to the social
good. John Graunt, haberdasher by day and demographer by night, had
made a breakthrough in London in 1662 when he used weekly mortality
records to design an early warning system to detect outbreaks of
bubonic plague in the city. Even though the system was never
actually deployed, it opened people's eyes to the rich possibilities
of data gathering and its usefulness to the state. By the 1740s,
prominent thinkers such as the German philosopher Gottfried
Achenwall had taken to calling this kind of data <em>statistics</em>
(<em>statistik</em> in German), the root of which is the Latin word
<em>statisticum</em> meaning ‚Äúof the state.‚Äù" (page 109)</p>
</blockquote>
<hr>
<blockquote>
<p>"His [Quetelet's] goal, perhaps antagonized by the Baron de
Keverberg's skepticism, was to investigate analytically all the ways
people <em>were</em> the same or different and to create a theory of
<em>social physics</em>, a set of laws governing society that could be an
equivalent of Kepler's laws of planetary motion and other immutable
principles of the hard sciences." (page 113)</p>
</blockquote>
<p>This reminds me of <a href="/20200714-foundation_trilogy/">psychohistory</a>.</p>
<hr>
<blockquote>
<p>"George P√≥lya gave it the lofty name the <em>central limit theorem</em>"
(page 120)</p>
</blockquote>
<p>Huh!</p>
<hr>
<blockquote>
<p>"He [Quetelet] would later be harshly ridiculed for his love of the
normal distribution by statisticians like Francis Edgeworth, who
wrote in 1922: ‚ÄúThe theory [of errors] is to be distinguished from
the doctrine, the false doctrine, that generally, wherever there is
a curve with a single apex representing a group of statistics ...
that the curve must be of the ‚Äònormal‚Äô species. The doctrine has
been nick-named ‚ÄòQuetelismus,‚Äô on the ground that Quetelet
exaggerated the prevalence of the normal law.‚Äù" (page 122)</p>
</blockquote>
<hr>
<p>Interesting/weird idea from Galton: "statistics by intercomparison."
If you can only order people on some characteristic (say
intelligence), then do that and then assume it's quantitatively
normal. Sort of like QQ plots. Sort of. (page 136)</p>
<hr>
<p>On page 142 it seems to be saying that Pearson's chi-squared is for
general testing of whether data comes from a certain distribution...
Is that right? Does this just mean binning out data and comparing
counts to expected? Maybe that's it?</p>
<hr>
<blockquote>
<p>"For an experimental scientist without advanced mathematical
training, the book
[Fisher's Statistical Methods for Research Workers] was a godsend.
All such a person had to do was find the procedure corresponding to
their problem and follow the instructions." (page 153)</p>
</blockquote>
<hr>
<blockquote>
<p>"He [Fisher] proved what he called the <a href="https://en.wikipedia.org/wiki/Fisher%27s_fundamental_theorem_of_natural_selection">fundamental theorem</a> of
natural selection: ‚ÄúThe rate of increase in fitness of any organism
at any time is equal to its genetic variance in fitness at that
time.‚Äù"</p>
</blockquote>
<p>Is this in conflict with Fisher as eugenicist? It seems to be
pro-diversity? At least some kinds of diversity...</p>
<hr>
<p>Interesting comparison between choosing one- or two-sided testing, and
Bayesian priors: you're not really bringing zero information to the
problem.</p>
<hr>
<p>Huh - there really is a <a href="https://www.socscistatistics.com/tests/what_stats_test_wizard.aspx">Social Science Statistics online wizard</a>.</p>
<hr>
<blockquote>
<p>"There is no coherent theory to orthodox statistics, only a loose
amalgam of half-baked ideas held together by suggestive naming,
catchy slogans, and folk superstition." (page 196)</p>
</blockquote>
<hr>
<blockquote>
<p>"As Fisher wrote in <em>Statistical Methods for Research Workers</em>, ‚ÄúNo
human mind is capable of grasping in its entirety the meaning of any
considerable quantity of numerical data. We want to be able to
express all the relevant information contained in the mass by means
of comparatively few numerical values. This is a purely practical
need which the science of statistics is able to some extent to
meet." (page 233)</p>
</blockquote>
<hr>
<p><a href="http://www.psych.ualberta.ca/~rozeboom/files/1960_The_fallacy_of_the_null_hypothesis_significance_test.pdf">The Fallacy of the Null-Hypothesis Significance Test</a></p>
<hr>
<p><a href="https://www.press.umich.edu/186351/cult_of_statistical_significance">The Cult of Statistical Significance</a></p>
<hr>
<blockquote>
<p>"Harold Jeffreys first proposed the idea of Bayes factors in his
<em>Theory of Probability</em>." (page 262)</p>
</blockquote>
<hr>
<p>Daryl Bem (who published in support of psi) amusingly wrote (quoted
page 264):</p>
<blockquote>
<p>To compensate for this remoteness from our participants, let us at
least become intimately familiar with the record of their behavior:
the data. Examine them from every angle. Analyze the sexes
separately. Make up new composite indexes. If a datum suggests a new
hypothesis, try to find further evidence for it elsewhere in the
data. If you see dim traces of interesting patterns, try to
reorganize the data to bring them into bolder relief. If there are
participants you don't like, or trials, observers, or interviewers
who gave you anomalous results, place them aside temporarily and see
if any coherent patterns emerge. Go on a fishing expedition for
something‚Äîanything‚Äîinteresting.</p>
</blockquote>
<p>That's from <a href="https://psychology.yale.edu/sites/default/files/bemempirical.pdf">Writing the Empirical Journal Article</a>.</p>
<hr>
<p><a href="https://amstat.tandfonline.com/doi/full/10.1080/00031305.2016.1154108#.YbT7Nn1KiVK">The ASA Statement on p-Values: Context, Process, and Purpose</a></p>
<hr>
<p><a href="https://www.tandfonline.com/toc/utas20/73/sup1">Statistical Inference in the 21st Century: A World Beyond p &lt; 0.05</a></p>
<hr>
<blockquote>
<p>"Significance testing was always based on a classification of
results into significant/insignificant without regard to effect size
or importance; no attempts to rehabilitate it now can change that
fundamental aspect nor repair the damage significance testing has
already caused. This yes/no binary has well and truly mixed things
up." (page 275)</p>
</blockquote>
<hr>
<blockquote>
<p>"The better, more complete interpretation of probability is that it
measures the <em>plausibility</em> of a proposition given some assumed
<em>information</em>. This extends the notion of deductive reasoning‚Äîin
which a proposition is derivable as a logical consequence of a set
of premises‚Äîto situations of incomplete information, where the
proposition is made more or less plausible, depending on what is
assumed to be known." (page 283)</p>
</blockquote>
<hr>
<blockquote>
<p>"All probability is conditional." (page 284)</p>
</blockquote>
<hr>
<blockquote>
<p>"Once we jettison the bureaucracy of frequentist statistics, we can
spend more time doing actual science." (page 287)</p>
</blockquote>
<hr>
<blockquote>
<p>"Getting rid of the useless concepts (significance testing,
estimators, sufficient and ancillary statistics, stochastic
processes) will amount to cutting out probably 90 percent of the
standard statistics curriculum. It might even mean giving up on
statistics as a separate academic discipline altogether, but that's
alright. Probability as a topic should rightfully split time between
its parents, math and philosophy, the way logic does. Bayesian
statistical inference contains exactly one theorem of importance
anyway, and its practical techniques can be taught in a single
semester-long course in applied math. There needn't be a whole
university department dedicated to it, any more than there needs to
be a department of the quadratic formula." (page 287)</p>
</blockquote>
<hr>
<blockquote>
<p>"We should no more be teaching <em>p</em>-values in statistics courses than
we should be teaching phrenology in medical schools." (page 293)</p>
</blockquote>
<hr>
<blockquote>
<p>"Joseph Berkson called this the ‚Äúinterocular traumatic test‚Äù; you
know what the data means when the conclusion hits you right between
the eyes." (page 297)</p>
</blockquote>
<p>That's quoted from "Bayesian statistical inference for psychological
research."</p>
<p>That source cites "J. Berkson, personal communication, July 14, 1958"
and goes on:</p>
<blockquote>
<p>"The interocular traumatic test is simple, commands general
agreement, and is often applicable; well-conducted experiments often
come out that way. But the enthusiast's interocular trauma may be
the skeptic's random error. A little arithmetic to verify the extent
of the trauma can yield great peace of mind for little cost." (page
217)</p>
</blockquote>
<hr>
<blockquote>
<p>"The results of experiments, particularly surprising or
controversial ones, can be trusted noly if the experiments are known
to be sound; however, as is often the case, an experiment is <em>known</em>
to be sound only if it produces the results we expect. So it would
seem that no experiment can ever convince us of something
surprising. This situation was anticipated by the ancient Greek
philosopher Sextus Empiricus. In a skepticism of induction that
predated David Hume's by 1,500 years, he wrote: ‚ÄúIf they shall judge
the intellects by the senses, and the senses by the intellect, this
involves circular reasoning inasmuch as it is required that the
intellects should be judged first in order that the sense may be
judged, and the senses be first scrutinized in order that the
intellects may be tested [hence] we possess no means by which to
judge objects.‚Äù" (page 301)</p>
</blockquote>    
    ]]></description>
<link>http://planspace.org/20211211-bernoullis_fallacy_by_clayton/</link>
<guid>http://planspace.org/20211211-bernoullis_fallacy_by_clayton/</guid>
<pubDate>Sat, 11 Dec 2021 12:00:00 -0500</pubDate>
</item>
<item>
<title>Significant Figures: Addition is too precise</title>
<description><![CDATA[

<p>The usual <a href="https://en.wikipedia.org/wiki/Significance_arithmetic#Addition_and_subtraction_using_significance_arithmetic" title="Wikipedia: Addition and subtraction using significance arithmetic">rules</a> for adding and subtracting numbers with
significant digits often propagate uncertainly approximately
correctly, but always give results more precise than they actually are
because they don't (and can't, generally) follow the
<a href="/20201030-the_variance_sum_law_is_interesting/" title="The Variance Sum Law is Interesting">Variance Sum Law</a>.</p>
<p>Say a <a href="/20211209-significant_figures_gaussian_uncertainty/" title="Significant Figures: Gaussian uncertainty, œÉ=2.5eN">number</a> in Significant Figures with rightmost significant
digit \( D \times 10^N \) has uncertainty with standard deviation
\( \sigma \times 10^N \), and assume errors are always uncorrelated.</p>
<p>So the number 12.3, with three significant figures, has uncertainty
\( \sigma \times 0.1 \), and 2.48 has \( \sigma \times 0.01 \).
Adding them gives 14.8, which has the same uncertainty as 12.3. By the
<a href="/20201030-the_variance_sum_law_is_interesting/" title="The Variance Sum Law is Interesting">Variance Sum Law</a>, the true uncertainty is \( \sigma \times 0.1005
\), but that's pretty close to \( \sigma \times 0.1 \). In this
way, the usual rule for adding with significant figures is often
reasonable-seeming.</p>
<p>With many numbers of the same precision, however, the usual rules are
more problematic. If you add 1.2 + 3.4 + 5.6 + 7.8, the result 18.0
implies \( \sigma \times 0.1 \), but in fact uncertainty has doubled
to \( \sigma \times 0.2 \). Significant Figures has no way to convey
this, because it only communicates in powers of ten.</p>
<p>Adding and subtracting 100 numbers with the same precision, then,
should give a result with exactly one fewer significant figures. With
25 numbers the standard deviation could ‚Äúround up‚Äù to the next power
of ten, arguably. It may not be common to add so many numbers with
significant figures, but even with just a few, Sig Figs is a course
approximation of correct propagation of uncertainty.</p>
<!-- mathjax for formulas -->

    ]]></description>
<link>http://planspace.org/20211211-significant_figures_addition_is_too_precise/</link>
<guid>http://planspace.org/20211211-significant_figures_addition_is_too_precise/</guid>
<pubDate>Sat, 11 Dec 2021 12:00:00 -0500</pubDate>
</item>
<item>
<title>Significant Figures: Gaussian uncertainty, œÉ=2.5eN</title>
<description><![CDATA[

<p>Significant Figures (or Significant Digits) is a tool for an important
task: tracking precision. Ironically, the meaning of a number with
Significant Figures is often not defined precisely. I propose that if
a number's rightmost significant digit is \( D \times 10^N \), this
means uncertainty is Gaussian with standard deviation \( \sigma = 2.5
\times 10^N \).</p>
<p><img alt="" src="gaussian.png"></p>
<p>The final digit is ‚Äú<a href="https://chem.libretexts.org/Courses/University_of_British_Columbia/CHEM_100%3A_Foundations_of_Chemistry/02%3A_Measurement_and_Problem_Solving/2.03%3A_Significant_Figures_-_Writing_Numbers_to_Reflect_Precision">significant but not certain</a>‚Äù ‚Äî which shouldn't
mean we know <em>nothing</em> about what the digit is likely to be, and
shouldn't mean that there's no chance the neighboring digit could be
wrong. A purely digit-based interpretation is unnatural.</p>
<p><img alt="" src="uniform.png"></p>
<p>A particular instrument (or other source) may generate measurements
with a different distribution of uncertainty. In such cases, that
information should be captured specifically, and Significant Figures
alone is not sufficient. For measurements where there isn't more
specific information, a Gaussian distribution is a good choice.</p>
<p>Even when given a precise interpretation, Significant Figures is a
system tied to base ten numbers and manual measurement that asks a
single number to convey both value and uncertainty. There's only so
much one number can mean. Sig Figs is better than not tracking
precision at all, but better yet is to be explicit about precision.</p>
<hr>
<p>I would love to find other sources or interpretations that agree or
disagree with this definition. So far I haven't found anything
explicit enough to really compare one way or another. References (and
feedback of any kind) are especially welcome here!</p>
<p>Visualization code is <a href="https://github.com/ajschumacher/sigfigs">on GitHub</a>.</p>
<!-- mathjax for formulas -->

    ]]></description>
<link>http://planspace.org/20211209-significant_figures_gaussian_uncertainty/</link>
<guid>http://planspace.org/20211209-significant_figures_gaussian_uncertainty/</guid>
<pubDate>Thu, 09 Dec 2021 12:00:00 -0500</pubDate>
</item>
<item>
<title>Try HoloViz for Python plotting</title>
<description><![CDATA[

<p>Yang <a href="https://sophiamyang.github.io/DS/visualization/holoviz/holoviz.html" title="Visualization and Interactive Dashboard in Python">inspired</a> me to check out <a href="https://holoviz.org/" title="High-level tools to simplify visualization in Python">HoloViz</a>, which makes it easy,
among other things, to do interactive scatterplots in Jupyter
Notebooks‚Äîand stick them in any HTML!</p>
<p><link rel="stylesheet" href="https://unpkg.com/@holoviz/panel@0.12.5/dist/css/alerts.css" type="text/css">
<link rel="stylesheet" href="https://unpkg.com/@holoviz/panel@0.12.5/dist/css/card.css" type="text/css">
<link rel="stylesheet" href="https://unpkg.com/@holoviz/panel@0.12.5/dist/css/widgets.css" type="text/css">
<link rel="stylesheet" href="https://unpkg.com/@holoviz/panel@0.12.5/dist/css/markdown.css" type="text/css">
<link rel="stylesheet" href="https://unpkg.com/@holoviz/panel@0.12.5/dist/css/json.css" type="text/css">
<link rel="stylesheet" href="https://unpkg.com/@holoviz/panel@0.12.5/dist/css/loading.css" type="text/css">
<link rel="stylesheet" href="https://unpkg.com/@holoviz/panel@0.12.5/dist/css/dataframe.css" type="text/css"></p>
<style>
                    .bk.pn-loading.arcs:before {
                      background-image: url("data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiBzdHlsZT0ibWFyZ2luOiBhdXRvOyBiYWNrZ3JvdW5kOiBub25lOyBkaXNwbGF5OiBibG9jazsgc2hhcGUtcmVuZGVyaW5nOiBhdXRvOyIgdmlld0JveD0iMCAwIDEwMCAxMDAiIHByZXNlcnZlQXNwZWN0UmF0aW89InhNaWRZTWlkIj4gIDxjaXJjbGUgY3g9IjUwIiBjeT0iNTAiIHI9IjMyIiBzdHJva2Utd2lkdGg9IjgiIHN0cm9rZT0iI2MzYzNjMyIgc3Ryb2tlLWRhc2hhcnJheT0iNTAuMjY1NDgyNDU3NDM2NjkgNTAuMjY1NDgyNDU3NDM2NjkiIGZpbGw9Im5vbmUiIHN0cm9rZS1saW5lY2FwPSJyb3VuZCI+ICAgIDxhbmltYXRlVHJhbnNmb3JtIGF0dHJpYnV0ZU5hbWU9InRyYW5zZm9ybSIgdHlwZT0icm90YXRlIiByZXBlYXRDb3VudD0iaW5kZWZpbml0ZSIgZHVyPSIxcyIga2V5VGltZXM9IjA7MSIgdmFsdWVzPSIwIDUwIDUwOzM2MCA1MCA1MCI+PC9hbmltYXRlVHJhbnNmb3JtPiAgPC9jaXJjbGU+PC9zdmc+");
                      max-height: 400px;
                    }
</style>

<div class="bk-root" id="7b0543dc-c84e-416d-896c-1741cbecf45a" data-root-id="3831"></div>

<p>I've wanted this to be standard functionality for a long time; six
years ago I briefly started a <a href="/20150119-gog_a_separate_layer_for_visualization/" title="gog: a separate layer for visualization">project</a> that included it in its
proof of concept, as in this screenshot:</p>
<p><img alt="gog screenshot" src="gogi_iris.png"></p>
<p>The interactive example above is the hvPlot <a href="https://hvplot.holoviz.org/reference/pandas/scatter.html" title="Scatter">Scatter example</a> saved
to HTML as in <a href="https://hvplot.holoviz.org/user_guide/Viewing.html#saving-plots" title="Saving plots">Saving plots</a> (code is <a href="https://github.com/ajschumacher/try_holoviz">on GitHub</a>) and then
copy-pasted into my blog Markdown file. The interactive plots
unfortunately don't show up in GitHub's Jupyter Notebook preview.</p>
<p><a href="https://hvplot.holoviz.org/">hvPlot</a> is just one corner of HoloViz and related packages, but the
interactive scatterplot functionality alone is enough for me to be a
big fan.</p>    
    ]]></description>
<link>http://planspace.org/20211126-try_holoviz_for_python_plotting/</link>
<guid>http://planspace.org/20211126-try_holoviz_for_python_plotting/</guid>
<pubDate>Fri, 26 Nov 2021 12:00:00 -0500</pubDate>
</item>
<item>
<title>The Essence of Progress and Poverty, by Henry George</title>
<description><![CDATA[

<p>This is a 2020 version of the 1928 abridging (endorsed by
<a href="https://en.wikipedia.org/wiki/John_Dewey" title="John Dewey (Wikipedia)">John Dewey</a>) of the 1879 original. It's enough to
<a href="https://www.henrygeorge.org/catsup.htm" title="‚ÄúSeeing the Cat‚Äù has long been a slang term for achieving an understanding of Henry George's ideas.">‚Äúsee the cat‚Äù</a>, I think, but I'm not yet convinced <a href="https://en.wikipedia.org/wiki/Georgism" title="Georgism (Wikipedia)">Georgism</a> is
<a href="/20211017-how_the_world_could_be_made_a_good_and_happy_place/" title="(a simple solution to solve all problems)">‚Äúhow the world could be made a good and happy place‚Äù</a>. The single
tax on land seems like a decent idea, but I'm not sure it's sufficient
for Star Trek style post-scarcity utopia (<a href="/20210401-trekonomics_by_saadia/" title="Trekonomics, by Saadia">a</a>, <a href="/20201109-economics_of_star_trek_by_webb/" title="The Economics of Star Trek, by Webb">b</a>).</p>
<p>It's nice to encounter an idea that's <em>trying</em> to be a big solution. I
don't know a good reason not to try it, apart from that those who are
currently own land wouldn't like it.</p>
<p><img alt="cover" src="cover.jpg"></p>
<hr>
<blockquote>
<p>"It has always been to the newer countries‚Äîthat is, to the countries
where material progress is yet in its earlier stages‚Äîthat laborers
have emigrated in search of higher wages, and capital has flowed in
search of higher interest. It is in the older countries‚Äîthat is to
say, the countries where material progress has reached later
stages‚Äîthat widespread destitution is found in the midst of the
greatest abundance. Go into a new community where Anglo-Saxon vigor
is just beginning the race of progress; where the machinery of
production and exchange is yet rude and inefficient; where the
increment of wealth is not yet great enough to enable any class to
live in ease and luxury; where the best house is but a cabin of logs
or a cloth and paper shanty; and the richest man is forced to daily
work‚Äîand though you will find an absence of wealth and all its
concomitants, you will find no beggars." (page 4)</p>
</blockquote>
<p>Couple things.</p>
<ul>
<li>"Anglo-Saxon vigor"? How deep is this racist-imperialist thinking?
   Does he think only white people are people?</li>
<li>Was this ever true, that net migration was from richer to poorer
   countries? Is it true now? Certainly the stereotype is that people
   prefer to move from poorer to richer countries "in search of higher
   wages".</li>
<li>Is it really true that the situation he describes is somehow a cure
   for poverty, or is it that George is wrong, or perhaps that people
   who would be poor cannot survive in such settings, so they aren't
   observed, or some combination?</li>
</ul>
<hr>
<blockquote>
<p>"To educate men who must be condemned to poverty, is but to make
them restive; to base on a state of most glaring social inequality
political institutions under which men are theoretically equal, is
to stand a pyramid on its apex." (page 7)</p>
</blockquote>
<p>This makes me think of antagonism toward "elites"... Not a perfect
connection though.</p>
<hr>
<blockquote>
<p>"The reason why, in spite of the increase of productive power, wages
constantly tend to a minimum which will give but a bare living, is
that, with increase in productive power, rent tends to even greater
increase, thus producing a constant tendency to the forcing down of
wages." (page 26)</p>
</blockquote>
<p>But it also isn't that employers keep throwing more money at
employees, and then landlords respond by increasing rents. If rent was
suddenly free, wouldn't employers lower wages in response to the
changed cost of living? Currently we see employers adjusting salaries
of remote employees based on cost of living in various markets, which
is largely driven by cost of housing.</p>
<hr>
<blockquote>
<p>"To extirpate poverty, to make wages what justice demands they
should be, the full earnings of the laborer, we must therefore
substitute for the individual ownership of land a common ownership.</p>
<p>"The right of ownership that springs from labor excludes the
possibility of any other right of ownership." (page 32)</p>
</blockquote>
<hr>
<blockquote>
<p>"It [tax upon land values] is the taking by the community, for the
use of the community, of that value which is the creation of the
community." (page 37)</p>
</blockquote>
<hr>
<blockquote>
<p>"Every productive enterprise, besides its return to those who
undertake it, yields collateral advantages to others." (page 43)</p>
</blockquote>
<p>This is tautological or incorrect.</p>
<hr>
<blockquote>
<p>"It seems to me that in a condition of society in which no one need
fear poverty, no one would desire great wealth‚Äîat least, no one
would take the trouble to strive and to strain for it as men do
now." (page 47)</p>
</blockquote>
<hr>
<blockquote>
<p>"Shortsighted is the philosophy which counts on selfishness as the
master motive of human action." (page 57)</p>
</blockquote>    
    ]]></description>
<link>http://planspace.org/20211121-essence_of_progress_and_poverty/</link>
<guid>http://planspace.org/20211121-essence_of_progress_and_poverty/</guid>
<pubDate>Sun, 21 Nov 2021 12:00:00 -0500</pubDate>
</item>
<item>
<title>Come work at Meta! (Facebook/Instagram/WhatsApp/etc.)</title>
<description><![CDATA[

<p>People have started asking me to refer them for jobs at <a href="https://meta.com/">Meta</a>,
which is great because Meta is <a href="https://www.facebookcareers.com/jobs">hiring</a>! I'd be happy to refer you;
send <a href="/aaron/">me</a> the following:</p>
<ol>
<li>Your full name</li>
<li>Your email address</li>
<li>Exactly one of:<ul>
<li>Your resume in PDF format</li>
<li>A link to your LinkedIn profile</li>
<li>Tell me to put in your email address and you'll be prompted to
  provide information yourself. (I've never tried this.)</li>
</ul>
</li>
<li>One to three job links from the <a href="https://www.facebookcareers.com/jobs">official jobs site</a><ul>
<li>Not from LinkedIn, not from Indeed, etc. They have to be from
  the official jobs site to work in the system.</li>
<li>Ensure you meet the minimum requirements listed for each, and
  that the locations listed (including remote if applicable) are
  aligned with your preferences.</li>
<li>If you're interested in working remotely, mention that to me.</li>
<li>If you're interested in a position based in an EU or UK office,
  mention that to me.</li>
</ul>
</li>
<li>Are you either of: a current university student or graduated from
   university within the last nine months? (yes/no)</li>
<li>How many years of industry experience do you have (not counting
   internships)? (one number)</li>
<li>Do you live in either of: the EU or UK? (yes/no)</li>
<li>Optional things<ul>
<li>LinkedIn profile link</li>
<li>GitHub profile link</li>
<li>Portfolio website link</li>
<li>Brief note (one paragraph or less) on why you should be hired</li>
</ul>
</li>
</ol>
<p>Once I have the above, I can put you into the system. After that, I
won't hear anything before you do. I can't guarantee any particular
next steps, but I wish you the best of luck and hope you do well!</p>    
    ]]></description>
<link>http://planspace.org/20211109-come_work_at_meta/</link>
<guid>http://planspace.org/20211109-come_work_at_meta/</guid>
<pubDate>Tue, 09 Nov 2021 12:00:00 -0500</pubDate>
</item>
<item>
<title>Parallel coordinate plots for visualizing functions</title>
<description><![CDATA[

<p>While thinking about the <a href="https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Arcsine_transformation">arcsin transform</a> for <a href="/20211106-visualizing_cohens_h/">Cohen's <em>h</em></a>, I
described the function as ‚Äústretching‚Äù its inputs, and compared it to
the <a href="https://en.wikipedia.org/wiki/Logit">logit function</a>. I wondered whether a
<a href="https://en.wikipedia.org/wiki/Parallel_coordinates">parallel coordinates plot</a> could visualize the ‚Äústretching‚Äù more
clearly than the traditional Cartesian plot. I think it kind of works!</p>
<p><img alt="parallel coordinates plots" src="parallel.png"></p>
<p>This left-to-right function input-to-output view is pleasing in some
ways. I think it succeeds in showing how the function is ‚Äústretching‚Äù
the input, as desired. It's hard to show very many input/output pairs
though, and it would be hard to show non-monotonic functions well, or
show multiple functions on the same axes. For the logit, I think it's
less clear in implying the function is unbounded than with the
Cartesian version.</p>
<p><img alt="traditional cartesian plots" src="cartesian.png"></p>
<p>The usual way of plotting functions feels a little drier. ‚ÄúStretching‚Äù
is greater where the plot is more vertical; I feel this less
immediately here than with the parallel coordinates version. Overall
this is still a great way to visualize functions, of course. I
wouldn't mind the occasional parallel coordinates plot as an
alternative visualization though!</p>
<hr>
<p>Code is <a href="https://github.com/ajschumacher/parallel_coordinate_functions">on GitHub</a>.</p>    
    ]]></description>
<link>http://planspace.org/20211107-parallel_coordinate_plots_for_visualizing_functions/</link>
<guid>http://planspace.org/20211107-parallel_coordinate_plots_for_visualizing_functions/</guid>
<pubDate>Sun, 07 Nov 2021 12:00:00 -0500</pubDate>
</item>
<item>
<title>Visualizing Cohen's h</title>
<description><![CDATA[

<p><a href="https://en.wikipedia.org/wiki/Cohen%27s_h">Cohen's <em>h</em></a> is an effect size for differences between two
proportions. It ranges from 0 to \( \pi \) (when nondirectional) and
adjusts for whether the proportions are ‚Äúextreme‚Äù (near 0 or 1) so
that equal Cohen's <em>h</em> means equal statistical detectability (Cohen
1988).</p>
<p><img alt="Cohen's h" src="cohens_h.png"></p>
<p>The heuristic values of 0.2 for ‚Äúsmall,‚Äù 0.5 for ‚Äúmedium,‚Äù and 0.8 for
‚Äúlarge‚Äù are analogous to the 0.05 confidence level: often taken more
seriously than they should be.</p>
<p>Cohen's <em>h</em> is the difference between the two proportions'
<a href="https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Arcsine_transformation">arcsin transforms</a>.</p>
<p><img alt="arcsin transformation" src="arcsin_transform.png"></p>
<p>The arcsin transform stretches out values near 0 and 1 more than
values around 0.5. It is bounded though, so for example if your
baseline proportion is 0.95, there is no proportion higher than that
which will register as a ‚Äúmedium‚Äù effect size.</p>
<p>You could replace the arcsin transform with <a href="https://en.wikipedia.org/wiki/Logit">log odds</a>; the logit
would allow any size difference anywhere, at least in principle. But I
don't think it would have any useful connection to variance, as the
<a href="https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Arcsine_transformation">arcsin transformation</a> does.</p>
<hr>
<p>Code is <a href="https://github.com/ajschumacher/cohens_h">on GitHub</a>.</p>
<!-- mathjax for formulas -->

    ]]></description>
<link>http://planspace.org/20211106-visualizing_cohens_h/</link>
<guid>http://planspace.org/20211106-visualizing_cohens_h/</guid>
<pubDate>Sat, 06 Nov 2021 12:00:00 -0500</pubDate>
</item>
<item>
<title>Simple diff-in-diff</title>
<description><![CDATA[

<p>Things are always changing. <a href="https://en.wikipedia.org/wiki/Difference_in_differences">Difference in differences</a> tries to
identify how much of a change is due to some treatment by comparing to
a control that's also changing over time.</p>
<p><img alt="diff-in-diff diagram" src="diff_in_diff.jpg"></p>
<p>This is often presented graphically with response value against time.</p>
<p><img alt="diff-in-diff graph" src="diff_in_diff_graph.jpg"></p>
<p>A noiseless design matrix illustrates how this becomes a diff-in-diff
regression.</p>
<pre><code>| treatment base | control base | common diff | treatment diff | response |
|----------------|--------------|-------------|----------------|----------|
| 1              | 0            | 0           | 0              |  3       |
| 0              | 1            | 0           | 0              |  2       |
| 1              | 0            | 1           | 1              | 12       |
| 0              | 1            | 1           | 0              |  7       |
</code></pre>

<p>Real data will have noise; here's a simulation in <a href="https://www.r-project.org/">R</a>.</p>
<pre><code class="language-r">treatment_base = c(rep(1, 100), rep(0, 100), rep(1, 100), rep(0, 100))
control_base   = c(rep(0, 100), rep(1, 100), rep(0, 100), rep(1, 100))
common_diff    = c(rep(0, 100), rep(0, 100), rep(1, 100), rep(1, 100))
treatment_diff = c(rep(0, 100), rep(0, 100), rep(1, 100), rep(0, 100))
response = c(rnorm(100, mean=3),
             rnorm(100, mean=2),
             rnorm(100, mean=3) + rnorm(100, mean=5) + rnorm(100, mean=4),
             rnorm(100, mean=2) + rnorm(100, mean=5))
summary(lm(response ~ treatment_base + control_base + common_diff + treatment_diff + 0))
##                Estimate Std. Error t value Pr(&gt;|t|)
## treatment_base   3.0747     0.1367   22.50   &lt;2e-16 ***
## control_base     1.9798     0.1367   14.49   &lt;2e-16 ***
## common_diff      5.0990     0.1933   26.38   &lt;2e-16 ***
## treatment_diff   3.9453     0.2733   14.44   &lt;2e-16 ***</code></pre>

<p>Diff-in-diff can get <a href="https://andrewcbaker.netlify.app/2019/09/25/difference-in-differences-methodology/">more complicated</a>, but the simple version
isn't too bad.</p>
<hr>
<p>Thanks to Dr. Erica Blom for valuable discussion and references on
this topic.</p>
<hr>
<h3>See also</h3>
<ul>
<li>Four ‚Äúback door‚Äù regression situations:
   <a href="/20200912-what_should_be_in_your_regression/">What should be in your regression?</a></li>
<li><a href="/20210501-simple_front_door_regression/">Simple Front Door Regression</a></li>
<li><a href="/20210430-a_simple_instrumental_variable/">A simple Instrumental Variable</a></li>
</ul>    
    ]]></description>
<link>http://planspace.org/20211031-simple_diff_in_diff/</link>
<guid>http://planspace.org/20211031-simple_diff_in_diff/</guid>
<pubDate>Sun, 31 Oct 2021 12:00:00 -0500</pubDate>
</item>
<item>
<title>My daily routine</title>
<description><![CDATA[

<p>Especially since reading <a href="/2014/01/19/daily-rituals-is-sort-of-inspiring/" title="Daily Rituals is sort of inspiring">Daily Rituals</a>, I've thought about how
people structure their days. This year I've been working on my
routines, especially mornings. Here's how it's looking.</p>
<h3>6:00-6:30 ‚Äî <a href="https://en.wikipedia.org/wiki/Spaced_repetition" title="Spaced repetition on Wikipedia">Spaced repetition</a> review with <a href="https://apps.ankiweb.net/" title="Anki App">Anki</a></h3>
<p>I review about a hundred cards per day in the <a href="https://apps.ankiweb.net/" title="Anki App">Anki</a> app. I
<a href="/20201009-anki_is_easy/" title="Anki is easy">got into this</a> about a year ago, and I think I'm going to
<a href="/20210908-spaced_repetition_is_o_log_n_sustainable/" title="Spaced repetition is O(log n) sustainable">stick with it</a>. It's pretty common to recommend daily physical
exercise, and I think of this as being a similar practice for
knowledge and memory.</p>
<h3>6:30-7:00 ‚Äî Writing in journal aka ‚Äú<a href="https://juliacameronlive.com/basic-tools/morning-pages/" title="Morning Pages">Morning pages</a>‚Äù</h3>
<p>This is a <a href="https://juliacameronlive.com/basic-tools/morning-pages/" title="Morning Pages">Julia Cameron thing</a> I picked up from
<a href="/20210116-pragmatic_thinking_and_learning/" title="Pragmatic Thinking and Learning, by Andy Hunt">Andy Hunt's book</a>. It's pretty good. I don't always write a full
three pages, but I make a point to always write something.</p>
<h3>7:00-7:30 ‚Äî Physical exercise</h3>
<p>I alternate running about three miles outside and doing a 21-minute
‚ÄúSmart Workout‚Äù with the <a href="https://7minuteworkout.jnj.com/" title="The Johnson &amp; Johnson Official 7 Minute Workout¬Æ App">J&amp;J 7-Minute Workout app</a> in the basement.
Running allows podcast listening.</p>
<p>Then I try to shower and everything by eight.</p>
<h3>Food and drink</h3>
<p>During the week, my breakfast and lunch have gotten pretty consistent.</p>
<ul>
<li>Breakfast: Granola with a little whole milk Greek yogurt</li>
<li>Lunch: Turkey and Swiss sandwich on whole grain bread with mustard</li>
</ul>
<p>Also in a typical day I drink two to four 8.4-ounce cans of Red Bull
Sugar Free.</p>
<h3>‚ÄúDailies‚Äù</h3>
<p>There are a number of little things I try to do every day. They can
change, and aren't always super strict. Here are some examples:</p>
<ul>
<li>Read today's entry from Tolstoy's <a href="https://en.wikipedia.org/wiki/A_Calendar_of_Wisdom">Calendar of Wisdom</a></li>
<li>Copy one of my old book posts into <a href="https://www.goodreads.com/user/show/34234019-aaron-schumacher">Goodreads</a> and Amazon reviews</li>
<li>Post a picture of my algae tank on Instagram</li>
<li>Greet someone I wasn't otherwise planning to talk to today</li>
<li>Do a lesson and reviews on <a href="https://www.executeprogram.com/">Execute Program</a></li>
<li>Come up with one new idea based on a random word</li>
</ul>
<p>Some of these I track in a spreadsheet.</p>
<h3>Evening ‚Äî Read a book</h3>
<p>I try to read a chapter, or an hour, or at least <em>something</em> that
isn't screaming at me from a screen. This is usually a paper book
because I like them, but the main characteristic of this reading is
that it's deliberate rather than reactive. I record what I've read in
my journal.</p>
<hr>
<p>I'm not perfect in following all of this, but recently I've been
getting more consistent, and I like it. One way I think about this is
that it raises the floor for how much I do in a day. Even if I do
nothing else, if I do these things I've done something to be happy
about. And of course these things have benefits that help me do other
things too. Always room to learn and improve more too!</p>
<p>The routines here are the least important parts of my days; I hope
they support the things I value and don't conflict with them.</p>
<p>I'm very fortunate to be largely free to choose how I spend so much
time; I owe so much to Erica in particular for supporting me and
making our family's life better every day.</p>    
    ]]></description>
<link>http://planspace.org/20211030-my_daily_routine/</link>
<guid>http://planspace.org/20211030-my_daily_routine/</guid>
<pubDate>Sat, 30 Oct 2021 12:00:00 -0500</pubDate>
</item>
<item>
<title>Core RAPPOR</title>
<description><![CDATA[

<p><a href="https://arxiv.org/abs/1407.6981">RAPPOR</a> (‚ÄúRandomized Aggregatable Privacy-Preserving Ordinal
Response‚Äù) is really close to the classic coin-flip
<a href="https://en.wikipedia.org/wiki/Differential_privacy#Randomized_response">randomized response</a> method, with one more layer of randomness.
It's ‚Äúordinal‚Äù in that it's every bit for itself: you can represent
histogram bins, but you can't really represent numbers in the usual
binary way. They additionally use Bloom filters to represent sets of
hashable things, which is neat too.</p>
<p><a href="https://en.wikipedia.org/wiki/Differential_privacy#Randomized_response">Randomized response</a> works like this. Did you wash your hands?</p>
<ul>
<li>25% of the time, just say ‚Äúyes‚Äù regardless</li>
<li>25% of the time, just say ‚Äúno‚Äù regardless</li>
<li>50% of the time, answer truthfully</li>
</ul>
<p>Those percentages correspond to \( f = 0.5 \) in the RAPPOR
<a href="https://arxiv.org/pdf/1407.6981.pdf">paper</a>'s notation. They call this the ‚Äúpermanent‚Äù step, because
it's done once per respondent.</p>
<!--
<pre><code class="language-python">import random</code></pre>
-->

<pre><code class="language-python">def permanent_randomized(response: bool, f=0.5):
    randomizer = random.random()  # uniform on [0, 1)
    if 0 &lt;= randomizer &lt; f/2:  # probability f/2
        return True
    elif f/2 &lt;= randomizer &lt; f:  # probability f/2
        return False
    elif f &lt;= randomizer &lt; 1:  # probability 1 - f
        return response</code></pre>

<p>If you get 59% positive responses with this method, 25% were just automatically saying ‚Äúyes‚Äù and 34% were answering ‚Äúyes‚Äù because they were being honest. Half of people were being honest, so double the 34% to find the overall true positive rate is 68%.</p>
<p>To let individuals report multiple times without losing privacy or being trackable, RAPPOR adds ‚Äúinstantaneous‚Äù randomness as well.</p>
<pre><code class="language-python">def instantaneous_randomized(response: bool, q=0.75, p=0.5):
    randomizer = random.random()  # uniform on [0, 1)
    if response is True:
        return randomizer &lt; q  # probability q
    elif response is False:
        return randomizer &lt; p  # probability p</code></pre>

<p>With the complete method, recovering the true rate gets a little more complicated; see first equation in the <a href="https://arxiv.org/pdf/1407.6981.pdf">paper</a>'s ¬ß4.</p>
<pre><code class="language-python">def extract_rate(raw_rate: float, f=0.5, q=0.75, p=0.5):
    return (raw_rate - p - f*q/2 + f*p/2) / ((1 - f)*(q - p))</code></pre>

<p>And it works!</p>
<!--
<pre><code class="language-python">random.seed(0)</code></pre>
-->

<pre><code class="language-python">true_rate = 0.68
n = 1_000_000

raw_rate = sum(
    instantaneous_randomized(
        permanent_randomized(random.random() &lt; true_rate))
    for _ in range(n)) / n

extract_rate(raw_rate)
## 0.6807759999999998</code></pre>

<p>RAPPOR is just doing this for any number of bits per respondent. Each
bit can have a direct interpretation like ‚Äúthis thing yes for this
respondent.‚Äù</p>
<p>RAPPOR also introduces a way of having respondents hash strings (say)
into <a href="http://en.wikipedia.org/wiki/Bloom_filter">Bloom filters</a>, which is kind of neat. They show how to have
different groups of respondents use different hash functions and then
do some regression to try to overcome noise and false positives. As
far as I can tell, you have to already know the set of things you're
going to be hashing into the Bloom filter (you won't discover an
unknown unknown) but it is more efficient and flexible than defining a
separate bit for every candidate string.</p>
<p>RAPPOR is used <a href="https://www.chromium.org/developers/design-documents/rappor">in Chrome</a>, for example, and if you have really a
lot of respondents, it seems pretty good!</p>
<!-- mathjax for formulas -->

    ]]></description>
<link>http://planspace.org/20211019-core_rappor/</link>
<guid>http://planspace.org/20211019-core_rappor/</guid>
<pubDate>Tue, 19 Oct 2021 12:00:00 -0500</pubDate>
</item>
  </channel>
</rss>
