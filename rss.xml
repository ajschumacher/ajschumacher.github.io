<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>plan âž” space</title>
    <link>http://planspace.org/</link>
    <description>plan space from outer nine</description>
    <language>en-us</language>
    <atom:link href="http://planspace.org/rss.xml" rel="self" type="application/rss+xml" />
<item>
<title>mergic</title>
<description><![CDATA[

<p><em>A lightning talk at the <a href="http://www.meetup.com/PyDataNYC/events/222329250/">May meeting</a> (<a href="http://www.bloomberg.com/event-registration/?id=39288">registration</a>) of the <a href="http://www.meetup.com/PyDataNYC/">PyData NYC meetup group</a>, introducing <a href="https://github.com/ajschumacher/mergic">mergic</a>.</em></p>
<hr>
<p></p><center>
planspace.org
<p>@planarrowspace
</p></center>
<hr>
<p>Hi! I'm Aaron. This is my blog and <a href="https://twitter.com/planarrowspace">my twitter handle</a>. You can get from one to the other. <a href="big.html">This presentation</a> and a corresponding write-up (you're reading it) are on my blog (which you're on).</p>
<hr>
<p><img height="1000%" title="ermahgerd mergic" src="ermahgerd.png"></p>
<hr>
<p>Down to business!</p>
<hr>
<pre><code>Lukas Lacko             F Pennetta
Leonardo Mayer          S Williams
Marcos Baghdatis        C Wozniacki
Santiago Giraldo        E Bouchard
Juan Monaco             N.Djokovic
Dmitry Tursunov         S.Giraldo
Dudi Sela               Y-H.Lu
Fabio Fognini           T.Robredo
...                     ...</code></pre>

<hr>
<p>The problem often looks like this: You have either two columns with slightly different versions of identifiers, or one long list of things that you need to resolve to common names. These problems are fundamentally the same.</p>
<p>Do you see the match here? (It's Santiago!)</p>
<hr>
<p>workflow support for reproducible deduplication and merging</p>
<hr>
<p>This is what <code>mergic</code> is for. <code>mergic</code> is a simple tool designed to make it less painful when you need to merge things that don't yet merge.</p>
<hr>
<p><img width="1000%" title="big data" src="big_data.png"></p>
<hr>
<p>A quick disclaimer!</p>
<p>This is John Langford's slide, about what big data is. He says that small data is data for which O(n<sup>2</sup>) algorithms are feasible. Currently <code>mergic</code> is strictly for this kind of "artisanal" data, where we want to ensure that our matching is correct but want to reduce the amount of human work to ensure that. And we are about to get very O(n<sup>2</sup>).</p>
<hr>
<pre><code>Santiago Giraldo,Leonardo Mayer
Santiago Giraldo,Dudi Sela
Santiago Giraldo,Juan Monaco
Santiago Giraldo,S Williams
Santiago Giraldo,C Wozniacki
Santiago Giraldo,S.Giraldo
Santiago Giraldo,Marcos Baghdatis
Santiago Giraldo,Y-H.Lu
...</code></pre>

<hr>
<p>So we make all possible pairs of identifiers! This is annoying for a computer, and awful for humans. The computer can calculate a lot of pairwise distances, but I don't want to look at all the pairs.</p>
<p>Do you see the match here? (It's Santiago again!)</p>
<hr>
<pre><code>INFO:dedupe.training:1.0
name : stanislas wawrinka

name : stanislas wawrinka

Do these records refer to the same thing?
(y)es / (n)o / (u)nsure / (f)inished

O.o?</code></pre>

<hr>
<p>The is a "screen shot" of the <a href="https://github.com/datamade/csvdedupe">csvdedupe</a> interface, which is based on the Python <a href="https://github.com/datamade/dedupe">dedupe</a> project, which is very cool. It could be exactly what you want for larger amounts of more complex data. There's even work on getting learnable edit distances implemented now, which would be great to see. But for very simple data sets, <code>dedupe</code> can be overkill. Also, you don't get much sense of the big picture of your data set, and it's still very pair-oriented.</p>
<hr>
<pre><code>Karolina Pliskova,K Pliskova</code></pre>

<hr>
<p>Aside from being a drag to look at, there's a bigger problem with verifying equality on a pairwise basis.</p>
<p>Do these two records refer to the same person? (Tennis fans may see where I'm going with this.)</p>
<hr>
<pre><code>Kristyna Pliskova,K Pliskova</code></pre>

<hr>
<p>Karolina has a twin sister, and Kristyna also plays professional tennis! This may well not be obvious if you only look at pairs individually. What matters is the set of names that are transitively judged as equal.</p>
<hr>
<p>sets &gt; pairs</p>
<hr>
<p>Both perceptually and logically, it's better to think in sets than in a bunch of individual pairs.</p>
<hr>
<p><img width="1000%" title="Open Refine" src="open_refine.png"></p>
<hr>
<p><a href="http://openrefine.org/">Open Refine</a> is quite good. Their interface shows you some useful diagnostics, and you can see sets of things. There's even some idea of repeatable transformations. But there's so much functionality wrapped up in a mostly graphical interface that it's hard to make it part of an easily repeatable workflow. And while there are a bunch of built-in distance functions, I'm not sure whether it's possible to use a custom distance function in Open Refine.</p>
<hr>
<ul>
<li>simple</li>
<li>customizable</li>
<li>reproducible</li>
</ul>
<hr>
<p>So the goals of <code>mergic</code> are to be:</p>
<ul>
<li>simple, meaning largely text-based and obvious</li>
<li>customizable, meaning you can easily use a custom distance function</li>
<li>reproducible, meaning everything you do can be done again automatically</li>
</ul>
<hr>
<p>demo</p>
<hr>
<p>Here's a quick run-through of the <code>mergic</code> workflow. It's similar to the one in the <a href="https://github.com/ajschumacher/mergic">README</a>.</p>
<pre><code class="language-bash">pew new pydata</code></pre>

<p>I'll start by making a new <a href="https://virtualenv.pypa.io/">virtual environment</a> using <a href="https://github.com/berdario/pew">pew</a>.</p>
<pre><code class="language-bash">pip install mergic</code></pre>

<p><code>mergic</code> is very new (version 0.0.4) and it currently installs with no extra dependencies.</p>
<pre><code class="language-bash">mergic -h</code></pre>

<p><code>mergic</code> runs includes a command-line script based on <a href="https://docs.python.org/2/library/argparse.html">argparse</a> that uses a default string distance function.</p>
<pre><code>usage: mergic [-h] {calc,make,check,diff,apply,table} ...

positional arguments:
  {calc,make,check,diff,apply,table}
    calc                calculate all partitions of data
    make                make a JSON partition from data
    check               check validity of JSON partition
    diff                diff two JSON partitions
    apply               apply a patch to a JSON partition
    table               make merge table from JSON partition

optional arguments:
  -h, --help            show this help message and exit</code></pre>

<p>The command line script has a number of sub-commands that expose its functionality.</p>
<pre><code class="language-bash">head -4 RLdata500.csv</code></pre>

<p>We'll try <code>mergic</code> out with an example data set from <a href="http://www.r-project.org/">R</a>'s <a href="http://journal.r-project.org/archive/2010-2/RJournal_2010-2_Sariyar+Borg.pdf">RecordLinkage</a> package.</p>
<pre><code>CARSTEN,,MEIER,,1949,7,22
GERD,,BAUER,,1968,7,27
ROBERT,,HARTMANN,,1930,4,30
STEFAN,,WOLFF,,1957,9,2</code></pre>

<p>The data is fabricated name and birth date from a hypothetical German hospital. It has a number of columns, but for <code>mergic</code> we'll just treat the rows of CSV as single strings.</p>
<pre><code class="language-bash">mergic calc RLdata500.csv</code></pre>

<p>The <code>calc</code> subcommand calculates all the pairwise distances and provides diagnostics about possible groupings that could be produced.</p>
<pre><code>num groups, max group, num pairs, cutoff
----------------------------------------
       500,         1,         0, -0.982456140351
       497,         2,         3, 0.0175438596491</code></pre>

<p>With a cutoff lower than any actual encountered string distance, every item stays separate, the maximum group size is one, and there are no pairs within those groups to evaluate.</p>
<pre><code>         2,       499,    124251, 0.416666666667
         1,       500,    124750, 0.418181818182</code></pre>

<p>On the other extreme, we could group every item together in a giant mega-group.</p>
<pre><code>       451,         2,        49, 0.111111111111
       450,         2,        50, 0.115384615385
       449,         3,        52, 0.125</code></pre>

<p><code>mergic</code> gives you a choice about how big the groups it will produce will be. In this case, there's a cutoff of about 0.12 that will produce 50 groups of two items, which looks promising.</p>
<pre><code class="language-bash">mergic make RLdata500.csv 0.12</code></pre>

<p>We can make a grouping with that cutoff, and the result is a JSON-formatted partition.</p>
<pre><code class="language-json">{
    "MATTHIAS,,HAAS,,1955,7,8": [
        "MATTHIAS,,HAAS,,1955,7,8",
        "MATTHIAS,,HAAS,,1955,8,8"
    ],
    "HELGA,ELFRIEDE,BERGER,,1989,1,18": [
        "HELGA,ELFRIEDE,BERGER,,1989,1,18",
        "HELGA,ELFRIEDE,BERGER,,1989,1,28"
    ],</code></pre>

<p>In this example, the partition at a cutoff of 0.12 happens to be exactly right and we correctly group everything. (This says something about how realistic this example data set is, something about your tool of choice if it can't easily get perfect performance on this example data set, and also something about information leakage.)</p>
<pre><code class="language-json">{
    "MATTHIAS,,HAAS,,1955,7,8": [
        "MATTHIAS,,HAAS,,1955,8,8"
    ],
    "HELGA,ELFRIEDE,BERGER,,1989,1,18": [
        "MATTHIAS,,HAAS,,1955,7,8",
        "HELGA,ELFRIEDE,BERGER,,1989,1,18",
        "HELGA,ELFRIEDE,BERGER,,1989,1,28"
    ],</code></pre>

<p>The above would be a strange change to make, but you could make such a change and save your changed version as a new file.</p>
<pre><code class="language-bash">mergic diff base.json edited.json &gt; diff.json
mergic apply base.json diff.json</code></pre>

<p><code>mergic</code> includes functionality for creating and applying diffs that compare two partitions. You can preserve just the changes that you make by hand, which provides a record of the changes that had a human in the loop versus the changes that were computer-generated.</p>
<pre><code class="language-bash">mergic table edited.json</code></pre>

<p>To actually accomplish the desired merge or deduplication after creating a good grouping in JSON, <code>mergic</code> will generate a two-column merge table in CSV that can be used with most any data system.</p>
<pre><code>"HANS,,SCHAEFER,,2003,6,22","HANS,,SCHAEFER,,2003,6,22"
"HARTMHUT,,HOFFMSNN,,1929,12,29","HARTMHUT,,HOFFMSNN,,1929,12,29"
"HARTMUT,,HOFFMANN,,1929,12,29","HARTMHUT,,HOFFMSNN,,1929,12,29"</code></pre>

<p>These merge tables are awful to work with by hand, which is why <code>mergic</code> leaves their generation as a final step after humans work with the more understandable JSON groupings.</p>
<hr>
<p><img width="1000%" title="custom distance function documentation on GitHub" src="custom_distance.png"></p>
<hr>
<p>It's easy to write a script with a custom distance function and immediately use it with all the workflow support of the <code>mergic</code> script.</p>
<p>Often, a custom distance function makes or breaks your effort. It's worth thinking about and experimenting with, and <code>mergic</code> makes it easy!</p>
<hr>
<p><img width="1000%" title="New York Open Statistical Programming Meetup; Practical Mergic: How to Join Anything" src="open_stats_prog_meetup.png"></p>
<hr>
<p>If you're interested in this kind of thing, I'll be doing <a href="http://www.meetup.com/nyhackr/events/222328498/">a longer talk</a> at the <a href="http://www.meetup.com/nyhackr/">New York City Open Statistical Programming Meetup</a> next week Wednesday.</p>
<hr>
<p><img width="1000%" title="Open Data Science Conference" src="open_data_sci_con.png"></p>
<hr>
<p>I also hope to see you at <a href="http://opendatascicon.com/">Open Data Science Con</a> in Boston!</p>
<hr>
<p>Thanks!</p>
<hr>
<p>Thank you!</p>
<hr>
<p></p><center>
planspace.org
<p>@planarrowspace
</p></center>
<hr>
<p>This is just me again.</p>    
    ]]></description>
<link>http://planspace.org/20150514-mergic/</link>
<guid>http://planspace.org/20150514-mergic/</guid>
<pubDate>Thu, 14 May 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>Working with Captured Variables in Python Closures</title>
<description><![CDATA[

<p>You can make <a href="http://en.wikipedia.org/wiki/Closure_%28computer_programming%29">closures</a> in Python like this:</p>
<pre><code class="language-python">def add_some(x=0):
    def adder(y):
        return x+y
    return adder

add3 = add_some(3)

print(add3(7))
## 10</code></pre>

<p>The <code>adder</code> function "remembers" <code>x</code>.</p>
<p>But this will fail:</p>
<pre><code class="language-python">def a_counter(count=0):
    def counter():
        count += 1
        return count
    return counter

the_counter = a_counter()

print(the_counter())
## UnboundLocalError: local variable 'count' referenced before assignment</code></pre>

<p>You can't assign to <code>count</code> in there!</p>
<p>In Python 3, you can resolve this with <code>nonlocal</code>:</p>
<pre><code class="language-python">def a_counter(count=0):
    def counter():
        nonlocal count
        count += 1
        return count
    return counter

the_counter = a_counter()

print(the_counter())
## 1

print(the_counter())
## 2

print(the_counter())
## 3</code></pre>

<p>Great! I'd rather not have to specify <code>nonlocal</code>, but I'm happy to know of another feature unique to Python 3.</p>
<p>In Python 2, you can't get assignment of captured variables inside your closure, but you can mutate a captured variable. <code>&#175;\_(&#12484;)_/&#175;</code></p>
<pre><code class="language-python">def a_counter(count={'val': 0}):
    def counter():
        count['val'] += 1
        return count['val']
    return counter

the_counter = a_counter()

print(the_counter())
## 1

print(the_counter())
## 2

print(the_counter())
## 3</code></pre>

<p>Hooray for closures!</p>    
    ]]></description>
<link>http://planspace.org/20150425-working_with_captured_variables_in_python_closures/</link>
<guid>http://planspace.org/20150425-working_with_captured_variables_in_python_closures/</guid>
<pubDate>Sat, 25 Apr 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>Two Highlights from NY R Conference 2015</title>
<description><![CDATA[

<p>Here are the two things I thought were most interesting at <a href="http://www.rstats.nyc/">NY R</a> on Saturday April 25, 2015:</p>
<hr>
<p><img alt="MRAN" src="mran.png"></p>
<p><a href="https://twitter.com/revojoe">Joe Rickert</a> showed <a href="http://www.revolutionanalytics.com/">Revolution Analytics</a>' <a href="http://mran.revolutionanalytics.com/">Managed R Archive Network</a> (MRAN). It snapshots all of <a href="http://cran.r-project.org/">CRAN</a> every day so that you can finally do a real lock-in of the versions for all your R dependencies, using the <a href="http://cran.r-project.org/web/packages/checkpoint/index.html">checkpoint</a> package. This was <a href="http://blog.revolutionanalytics.com/2014/10/introducing-rrt.html">announced</a> a while ago, but it was new to me. I love any sort of data store that gives you <code>as-of</code>, even if it's just by snapshotting.</p>
<hr>
<p><a href="https://twitter.com/wesmckinn">Wes McKinney</a> spoke about data frame design and how such tooling should develop further. Here's a transcription of a key <a href="https://twitter.com/planarrowspace/status/591990689635905536">slide</a>:</p>
<ul>
<li><strong>The Great Data Tool Decoupling&#8482;</strong><ul>
<li>Thesis: over time, user interfaces, data storage, and execution engines will decouple and specialize</li>
<li>In fact, you should really want this to happen<ul>
<li>Share systems among languages</li>
<li>Reduce fragmentation and "lock-in"</li>
<li>Shift developer focus to usability</li>
</ul>
</li>
<li>Prediction: we'll be there by 2025; sooner if we all get our act together</li>
</ul>
</li>
</ul>
<p>I've thought about this kind of decoupling for data visualization (see <a href="http://planspace.org/20150119-gog_a_separate_layer_for_visualization/">gog</a>) and I think it's a cool direction in general.</p>
<hr>
<p>There were certainly other interesting and good things as well!</p>    
    ]]></description>
<link>http://planspace.org/20150425-two_highlights_from_ny_r_conference_2015/</link>
<guid>http://planspace.org/20150425-two_highlights_from_ny_r_conference_2015/</guid>
<pubDate>Sat, 25 Apr 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>Forward Selection with statsmodels</title>
<description><![CDATA[

<p>Python's <a href="http://statsmodels.sourceforge.net/stable/">statsmodels</a> doesn't have a built-in method for choosing a linear model by <a href="http://en.wikipedia.org/wiki/Stepwise_regression">forward selection</a>. Luckily, it isn't impossible to write yourself. So <a href="https://github.com/trevor-smith">Trevor</a> and I sat down and hacked out the following. It tries to optimize <a href="http://en.wikipedia.org/wiki/Coefficient_of_determination#Adjusted_R2">adjusted R-squared</a> by adding features that help the most one at a time until the score goes down or you run out of features.</p>
<pre><code class="language-python">import statsmodels.formula.api as smf

def forward_selected(data, response):
    """Linear model designed by forward selection.

    Parameters:
    -----------
    data : pandas DataFrame with all possible predictors and response

    response: string, name of response column in data

    Returns:
    --------
    model: an "optimal" fitted statsmodels linear model
           with an intercept
           selected by forward selection
           evaluated by adjusted R-squared
    """
    remaining = set(data.columns)
    remaining.remove(response)
    selected = []
    current_score, best_new_score = 0.0, 0.0
    while remaining and current_score == best_new_score:
        scores_with_candidates = []
        for candidate in remaining:
            formula = "{} ~ {} + 1".format(response,
                                           ' + '.join(selected + [candidate]))
            score = smf.ols(formula, data).fit().rsquared_adj
            scores_with_candidates.append((score, candidate))
        scores_with_candidates.sort()
        best_new_score, best_candidate = scores_with_candidates.pop()
        if current_score &lt; best_new_score:
            remaining.remove(best_candidate)
            selected.append(best_candidate)
            current_score = best_new_score
    formula = "{} ~ {} + 1".format(response,
                                   ' + '.join(selected))
    model = smf.ols(formula, data).fit()
    return model</code></pre>

<p>There isn't just one way to design this kind of thing. You could select on some other evaluation metric. You could use internal cross-validation. You might not want to do use forward selection at all. But hey!</p>
<p>Here's how ours can be applied to a classic data set on <a href="http://data.princeton.edu/wws509/datasets/#salary">Discrimination in Salaries</a>:</p>
<pre><code class="language-python">import pandas as pd

url = "http://data.princeton.edu/wws509/datasets/salary.dat"
data = pd.read_csv(url, sep='\\s+')

model = forward_selected(data, 'sl')

print model.model.formula
# sl ~ rk + yr + 1

print model.rsquared_adj
# 0.835190760538</code></pre>    
    ]]></description>
<link>http://planspace.org/20150423-forward_selection_with_statsmodels/</link>
<guid>http://planspace.org/20150423-forward_selection_with_statsmodels/</guid>
<pubDate>Thu, 23 Apr 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>R Squared Can Be Negative</title>
<description><![CDATA[

<p>Let's do a little linear regression in Python with <a href="http://scikit-learn.org/">scikit-learn</a>:</p>
<pre><code class="language-python">import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.cross_validation import train_test_split

X, y = np.random.randn(100, 20), np.random.randn(100)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)</code></pre>

<p>It is a property of <a href="http://en.wikipedia.org/wiki/Ordinary_least_squares">ordinary least squares regression</a> that for the training data we fit on, the <a href="http://en.wikipedia.org/wiki/Coefficient_of_determination">coefficient of determination R<sup>2</sup></a> and the square of the <a href="http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient">correlation coefficient</a> r<sup>2</sup> of the model's predictions with the actual data are equal.</p>
<pre><code class="language-python"># coefficient of determination R^2
print model.score(X_train, y_train)
## 0.203942898079

# squared correlation coefficient r^2
print np.corrcoef(model.predict(X_train), y_train)[0, 1]**2
## 0.203942898079</code></pre>

<p>This does not hold for new data, and if our model is sufficiently bad the coefficient of determination can be negative. The squared correlation coefficient is never negative but can be quite low.</p>
<pre><code class="language-python"># coefficient of determination R^2
print model.score(X_test,  y_test)
## -0.277742673311

# squared correlation coefficient r^2
print np.corrcoef(model.predict(X_test), y_test)[0, 1]**2
## 0.0266856746214</code></pre>

<p>These declines in performance worsen with <a href="http://en.wikipedia.org/wiki/Overfitting">overfitting</a>.</p>    
    ]]></description>
<link>http://planspace.org/20150417-negative_r_squared/</link>
<guid>http://planspace.org/20150417-negative_r_squared/</guid>
<pubDate>Fri, 17 Apr 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>RStudio in a Web Browser</title>
<description><![CDATA[

<p>The most annoying part of <code>R</code> workshops is installing software and downloading necessary files. With RStudio Server, workshop participants can skip all that entirely.</p>
<p>Here's what RStudio looks like running locally. To attain the setup shown, you need to install <a href="http://www.r-project.org/">R</a>, install <a href="http://www.rstudio.com/">RStudio</a>, install necessary R <a href="http://cran.r-project.org/">packages</a>, separately download necessary code and data, and navigate to the correct working directory.</p>
<p><img alt="RStudio running locally" src="local.png"></p>
<p>Here's what RStudio looks like in a browser. To attain the setup shown, you go to a URL and log in.</p>
<p><img alt="RStudio in a browser" src="web.png"></p>
<p>In a workshop, it's very nice to be able to start doing things with <code>R</code> without messing with setup. Somebody does have to set up the environment in advance though.</p>
<p>Setting up RStudio Server is very easy, especially if you're already familiar with Amazon Web Services (<a href="http://aws.amazon.com/">AWS</a>):</p>
<ul>
<li>Spin up an AWS Elastic Compute Cloud (<a href="http://aws.amazon.com/ec2/">EC2</a>) machine using <a href="http://www.louisaslett.com/RStudio_AMI/">Louis Aslett's RStudio Amazon Machine Image (AMI)</a>.<ul>
<li><a href="http://www.louisaslett.com/RStudio_AMI/">Louis's page</a> has good concise directions on how to do this&#8212;follow his directions!</li>
</ul>
</li>
<li>Secure Shell (SSH) into your EC2 machine and set up the environment as you want it to be.<ul>
<li>Install globally required packages while running R as root (<code>sudo R</code>).</li>
<li>Put necessary files (code, data, etc.) as desired in <code>/home/rstudio</code> (the prototypical user).</li>
<li>Run <a href="https://gist.github.com/ajschumacher/12f7484d06cacd4b4cd3">build_logins.sh</a>, a script developed by <a href="https://twitter.com/joshdata">Josh Tauberer</a>, to create the desired number of user accounts. (See documentation in the script itself.)</li>
</ul>
</li>
<li>You can optionally give your EC2 machine an AWS <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html">elastic IP address</a>, which will let you start and stop the machine (avoiding the cost of keeping it running when you aren't using it) but still know in advance what address the machine will be accessible at.<ul>
<li>Be sure your EC2 machine and elastic IP are both &#8220;EC2 classic&#8221; or both Virtual Private Cloud (VPC). (Some instance types are only available in VPC!)</li>
</ul>
</li>
</ul>
<p>For an in-person workshop, it can be effective to distribute login information on slips of paper like this:</p>
<p><img alt="workshop login information" src="paper.png"></p>
<p>I've led an <a href="/20150220-data_science_isnt_magic/">introductory workshop</a> with ten active participants on one EC2 m3.2xlarge, which runs at 56 cents per hour. For a larger group, I ran two m3.2xlarge machines and three c4.8xlarge machines ($1.856/hour). That used all my five elastic IP addresses and provided 124 vCPUs and 240 gigs of RAM. We had 94 active users and could probably have supported quite a few more without problems.</p>
<p>RStudio in the cloud is a great fit for workshops because it eliminates install and setup pain for participants. It lets you decouple the install process from other workshop activities. The experience of using RStudio in a browser is nice enough that it makes me wonder whether anyone offers cloud RStudio as a service&#8212;is that a business that should exist? Workshops certainly aren't the only place where RStudio Server could make sense.</p>
<p>There are also other options for setting up your server. You don't need to use Louis's AMIs; you don't need to use AWS at all. The <a href="http://www.bioconductor.org/">Bioconductor Project</a> maintains a <a href="http://www.bioconductor.org/help/bioconductor-cloud-ami/">Cloud AMI</a> that comes with a lot of other pre-installed R packages, for example. (When I last checked it was based on an older version of Ubuntu.) For total control, you can install <a href="http://www.rstudio.com/products/rstudio/download-server/">RStudio Server</a> from scratch on whatever type of system you like.</p>    
    ]]></description>
<link>http://planspace.org/20150221-rstudio_in_a_web_browser/</link>
<guid>http://planspace.org/20150221-rstudio_in_a_web_browser/</guid>
<pubDate>Sat, 21 Feb 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>Data Science isn't Magic</title>
<description><![CDATA[

<p>This is the flow for a <a href="http://dc.opendataday.org/">DC Open Data Day</a> 2015 workshop. It may not make sense out of context. For a fun summary in tweets (with photos!) you might check out <a href="https://storify.com/planarrowspace/data-science-isn-t-magic-workshop-at-dc-open-data">a storification of it</a>.</p>
<p>View this page as <a href="big.html">slides</a> to make this site's base URL appear quite large so that people can find this easily.</p>
<hr>
<p>planspace.org</p>
<hr>
<h3>Workshop Outline</h3>
<ul>
<li>Intro / Disclaimer</li>
<li><a href="80_percent_definitions/">80% Definitions</a> (<a href="80_percent_definitions/big.html">slides</a>)</li>
<li><a href="osemn/">Data Science is OSEMN</a> (<a href="osemn/big.html">slides</a>)</li>
<li><a href="problem/">What's the Problem?</a> (<a href="problem/big.html">slides</a>)</li>
<li>Find NYC attendance data<ul>
<li><a href="http://schools.nyc.gov/">schools.nyc.gov</a></li>
<li><a href="http://schools.nyc.gov/AboutUs/">About Us</a></li>
<li><a href="http://schools.nyc.gov/AboutUs/schools/">Our Schools</a></li>
<li><a href="http://schools.nyc.gov/AboutUs/schools/data/">Data About Schools</a></li>
<li><a href="http://schools.nyc.gov/AboutUs/schools/data/Attendance.htm">Daily Attendance Rates</a></li>
<li><a href="http://schools.nyc.gov/aboutus/data/attendancexml/">XML</a></li>
<li><a href="http://schools.nyc.gov/AboutUs/schools/data/attendancexml/">http://schools.nyc.gov/AboutUs/schools/data/attendancexml/</a></li>
</ul>
</li>
<li>Data Science <a href="tools/">Tools</a> (<a href="tools/big.html">slides</a>)</li>
<li>Connect to RStudio in the cloud<ul>
<li>Backup plan:<ul>
<li>Install the appropriate <code>R</code> distribution for your system from this <a href="http://watson.nci.nih.gov/cran_mirror/">mirror</a>.</li>
<li>Install the <a href="http://www.rstudio.com/ide/download/desktop">RStudio IDE</a> for <code>R</code>. The RStudio site should suggest an appropriate package for your system.</li>
<li>Download and unzip the <a href="https://github.com/ajschumacher/odddsim/archive/master.zip">files</a> we're using. (They're <a href="https://github.com/ajschumacher/odddsim">on GitHub</a>, so you can clone if you prefer.)</li>
</ul>
</li>
</ul>
</li>
<li>Working with <code>R</code><ul>
<li>Working with one day of data (<code>01-day_attendance.R</code>)</li>
<li>Selecting usable data points (<code>02-select_totals.R</code>)</li>
<li>The relationship between temperature and attendance (<code>03-merge_and_plot.R</code>)</li>
</ul>
</li>
<li>Bonus: Introducing the DC voter file</li>
</ul>
<h3>Additional Resources</h3>
<ul>
<li>For learning <code>R</code>:<ul>
<li><a href="http://tryr.codeschool.com/">Try R</a> is an interactive web site that guides you through <code>R</code> functionality in your web browser.</li>
<li>The <a href="https://raw.githubusercontent.com/ajschumacher/gadsdc/master/02-R/walking_intro.Rmd">walking introduction to R</a> is an <code>R</code> script that you can open and work through in RStudio.</li>
</ul>
</li>
<li>For more fun data:<ul>
<li><a href="http://blogs.worldbank.org/opendata/accessing-world-bank-data-apis-python-r-ruby-stata">Accessing the World Bank Data APIs in Python, R, Ruby &amp; Stata</a></li>
<li>The <a href="https://github.com/ajschumacher/dc_voter_reg">DC voter registration file</a></li>
</ul>
</li>
</ul>    
    ]]></description>
<link>http://planspace.org/20150220-data_science_isnt_magic/</link>
<guid>http://planspace.org/20150220-data_science_isnt_magic/</guid>
<pubDate>Fri, 20 Feb 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>To Migrate from GitHub to BitBucket</title>
<description><![CDATA[

<p><a href="https://github.com/">GitHub</a> has great design and unlimited collaborators but no free private repos. <a href="https://bitbucket.org/">BitBucket</a> has unlimited free private repos but a limit of five collaborators on them. Assuming you want your repos private so that very few people have access to them, BitBucket is perfect.</p>
<p>BitBucket makes migrating from GitHub ridiculously easy. They have a <a href="https://blog.bitbucket.org/2013/04/02/finnovation-brings-you-a-heap-of-new-features/">one-click integration</a> and everything. But it's harder to migrate issues.</p>
<p>Joe Workman <a href="http://joeworkman.tumblr.com/post/40133335482/migrating-from-github-to-bitbucket">made</a> <a href="http://www.bytebucket.org/joeworkman/git2bit">git2bit</a>, which magically puts issues from a GitHub repository into a BitBucket repository. The systems aren't exactly the same, but it makes mostly intelligent choices for how to convert.</p>
<p>You already have Ruby installed, so it should be this easy to install <code>git2bit</code>:</p>
<pre><code class="language-bash">gem install git2bit</code></pre>

<p>Alas, <code>git2bit</code> depends on both <a href="https://rubygems.org/gems/bitbucket_rest_api">bitbucket_rest_api</a> and <a href="https://rubygems.org/gems/github_api">github_api</a>, taking their most recent versions by default, and these two gems now require conflicting versions of <a href="https://rubygems.org/gems/hashie">hashie</a>.</p>
<p>This is what I did:</p>
<pre><code class="language-bash">gem install hashie -v 2.0.5
gem install github_api -v 0.11.3
gem install bitbucket_rest_api -v 0.1.5
gem install git2bit -v 1.0.2</code></pre>

<p>At last! Use <code>git2bit --help</code> to see the fairly clear options.</p>
<p>It seems the default is to migrate only open issues, but if you use the <code>--closed</code> flag it will migrate both open and closed issues, which is what I wanted. Be aware that there's no checking for duplication, so if you run <code>git2bit</code> twice you will get double the issues on the BitBucket side. It's easy to make an empty repo to test that things are working as intended.</p>
<p>Other minutiae:</p>
<p>If you want to have a different identity for some repos (the ones on BitBucket, say) this is easy to set on a per-repo basis. These commands will alter a repo's <code>.git/config</code>:</p>
<pre><code class="language-bash">git config user.name "Your Name Here"
git config user.email your@email.com</code></pre>

<p>On BitBucket you'll want to <a href="https://confluence.atlassian.com/display/BITBUCKET/Add+an+SSH+key+to+an+account">Add an SSH key to an account</a> and <a href="https://confluence.atlassian.com/display/BITBUCKET/Use+the+SSH+protocol+with+Bitbucket">Use the SSH protocol with Bitbucket</a>.</p>
<p>To point a current local repo up to BitBucket:</p>
<pre><code class="language-bash">git remote remove origin
git remote add origin git@bitbucket.org:accountname/reponame.git</code></pre>

<p>And if you didn't use the one-click migration but are instead doing your initial push to BitBucket from your local repo:</p>
<pre><code class="language-bash">git push -u origin --all
git push -u origin --tags</code></pre>

<p>That's just copied from BitBucket's directions when you create a new repo, which I like for being more complete than GitHub's.</p>    
    ]]></description>
<link>http://planspace.org/20150126-to_migrate_from_github_to_bitbucket/</link>
<guid>http://planspace.org/20150126-to_migrate_from_github_to_bitbucket/</guid>
<pubDate>Mon, 26 Jan 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>Monads by Diagram</title>
<description><![CDATA[

<p>Let's get some notation out of the way:</p>
<p><img alt="intput -&gt; function -&gt; output" src="function_notation.jpg"></p>
<p>A &#8220;function&#8221; is represented by a rectangle. Its arguments (&#8220;input&#8221;) appear above it, and its result (&#8220;output&#8221;) appears to its right.</p>
<p><img alt="value and monadic value" src="values.jpg"></p>
<p>We have some idea of &#8220;values&#8221; which can be worked with. We introduce the idea of a &#8220;monadic value&#8221; which can be thought of as &#8220;wrapping&#8221; our more familiar values.</p>
<p><img alt="unit" src="unit.jpg"></p>
<p>There's a function called &#8220;unit&#8221; which takes a value and returns a monadic value. Now we can make monadic values from our usual values.</p>
<p><img alt="bind" src="bind.jpg"></p>
<p>There's a function called &#8220;bind&#8221; which takes a function and a monadic value. The argument function takes a value and returns a monadic value. The argument monadic value gets unwrapped by &#8220;bind&#8221; and its value becomes the argument to the passed function. Now we can do things to our monadic values.</p>
<p>That's it. Monads are defined. They should obey some reasonable rules, but that&#8217;s it.</p>
<p><em>And yet you want more?</em></p>
<p><img alt="map" src="map.jpg"></p>
<p>Nobody wants to write functions that take a normal value and return a monadic value. The &#8220;map&#8221; function takes a normal function that knows nothing about monads and returns a function that takes and returns monadic values. Just use the resulting function on your monadic values! So convenient!</p>
<p><img alt="join" src="join.jpg"></p>
<p>The &#8220;join&#8221; function unwraps a doubly-monadic value. You want this.</p>
<p><img alt="bind via map and join" src="bind_via_map_and_join.jpg"></p>
<p>We can combine the &#8220;map&#8221; and &#8220;join&#8221; functions to get the &#8220;bind&#8221; function. Pretty neat!</p>
<p><em>And yet you want a real explanation?</em></p>
<p>For a lucid formal exposition, read <a href="http://homepages.inf.ed.ac.uk/wadler/papers/marktoberdorf/baastad.pdf">Monads for Functional Programming</a>. The above is my attempt to express the key definitions there, without the mathematical Haskell notation.</p>
<p>For a plain-talking explanation with practical examples in friendly Ruby, <a href="http://codon.com/refactoring-ruby-with-monads">read</a> or (preferably) <a href="https://www.youtube.com/watch?v=uTR__8RvgvM">watch</a> Refactoring Ruby with Monads. <a href="https://twitter.com/tomstuart">Tom Stuart</a> uses <code>from_value</code> for &#8220;unit&#8221;, <code>and_then</code> for &#8220;bind&#8221;, and <code>within</code> for calling &#8220;map&#8221; and applying the result.</p>
<p>For a great overview of even more kinds of things and much better illustration than I can manage, read <a href="http://adit.io/posts/2013-04-17-functors,_applicatives,_and_monads_in_pictures.html">Functors, Applicatives, And Monads In Pictures</a>. This follows the Haskell in using <code>fmap</code> for &#8220;map&#8221;.</p>
<p>And <a href="http://james-iry.blogspot.com/2009/05/brief-incomplete-and-mostly-wrong.html">don't forget</a>:</p>
<blockquote>
<p>&#8220;a monad is a monoid in the category of endofunctors, what's the problem?&#8221;</p>
</blockquote>    
    ]]></description>
<link>http://planspace.org/20150125-monads_by_diagram/</link>
<guid>http://planspace.org/20150125-monads_by_diagram/</guid>
<pubDate>Sun, 25 Jan 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>Use pew, not virtualenvwrapper, for Python virtualenvs</title>
<description><![CDATA[

<p>Have you noticed how good <a href="http://docs.python-guide.org/">The Hitchhiker&#8217;s Guide to Python</a> is? The documentation there on <a href="http://docs.python-guide.org/en/latest/dev/virtualenvs/">virtualenv</a> is exemplary.</p>
<p>Of course nobody wants to use virtualenvs directly, which is why virtualenvwrapper exists. But as <a href="http://datagrok.org/">Lamb</a> pointed out, <a href="https://gist.github.com/datagrok/2199506">Virtualenv's <code>bin/activate</code> is Doing It Wrong</a>, and virtualenvwrapper doesn't do it right either. Also, it's annoying to run env'ed Python via <a href="http://en.wikipedia.org/wiki/Cron">cron</a>.</p>
<p>The solution is to use <a href="https://github.com/berdario/pew">pew</a> instead. The Python Environment Wrapper (pew) handles your environment elegantly. This includes not mangling your <code>PS1</code>, though you can still display <code>VIRTUAL_ENV</code> information in your prompt if you want, and however you want. Also, suddenly all your virtualenv tasks are pew sub-commands, which means all you have to remember is <code>pew</code>. Typing <code>pew</code> alone shows all the pew commands, with helpful descriptions.</p>
<p>I no longer set the <code>WORKON_HOME</code> environment variable. This means pew will use the default <code>~/.local/share/virtualenvs</code>, which is a good choice. It's consistent with <a href="https://www.python.org/dev/peps/pep-0370/">PEP 370</a>, and it means you don't rely on <code>WORKON_HOME</code> to find your virtualenvs, which is nice for cron.</p>
<p>The location of <code>pew</code> is <code>/usr/local/bin</code>. For convenience, I suggest adding a <code>PATH</code> line to your crontab. The default <code>PATH</code> is <code>/usr/bin:/bin</code>, so the following adds one entry:</p>
<pre><code>PATH=/usr/local/bin:/usr/bin:/bin</code></pre>

<p>With that done, you can easily run any Python script you want in any virtualenv you want, via cron, with concise syntax:</p>
<pre><code>* * * * * pew in my_env my_script.py</code></pre>

<p>The example assumes <code>my_script.py</code> is in your home directory, executable, and with the standard <code>#!/usr/bin/env python</code> shebang.</p>
<p>I find this to be a very nice setup: <code>pew workon my_env</code> and develop <code>my_script.py</code> however I want, and <code>pew in my_env my_script.py</code> to run in that virtualenv from cron or anywhere else.</p>
<hr>
<p>Related tools: <a href="https://virtualenv.pypa.io/">virtualenv</a>, <a href="https://virtualenvwrapper.readthedocs.org/">virtualenvwrapper</a>, <a href="https://github.com/sashahart/vex">vex</a>, <a href="https://github.com/zimbatm/direnv">direnv</a>, probably more...</p>
<p>Thanks to <a href="https://twitter.com/jessicagarson">Jessica</a>, <a href="https://twitter.com/necaris">Rami</a>, and <a href="https://twitter.com/reconbot">Francis</a> for thoughts on this.</p>    
    ]]></description>
<link>http://planspace.org/20150120-use_pew_not_virtualenvwrapper_for_python_virtualenvs/</link>
<guid>http://planspace.org/20150120-use_pew_not_virtualenvwrapper_for_python_virtualenvs/</guid>
<pubDate>Tue, 20 Jan 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>gog: a separate layer for visualization</title>
<description><![CDATA[

<p><em>A short presentation at the DC <a href="http://www.meetup.com/TrackMaven-Monthly-Challenge/">Monthly Challenge</a> on <a href="http://www.meetup.com/TrackMaven-Monthly-Challenge/events/219314544/">Monday January 19, 2015</a>.</em></p>
<hr>
<p><img alt="gog" src="gog.png"></p>
<hr>
<p><code>gog</code> is a name for a simple idea I've been exploring about making it easy to quickly see data from whatever computing environment you happen to be in.</p>
<p>First I'll talk about <em>quickly</em>, then I'll talk about <em>from whatever</em>, and then I'll show some <code>gog</code> prototype work.</p>
<p>There are two kinds of <em>quickly</em> that I care about. One is getting images produced quickly, and another is working with or exploring them quickly.</p>
<hr>
<pre><code class="language-r">plot(data)</code></pre>

<hr>
<p>This is <code>R</code>. <code>R</code> can produce plots quickly. The <code>plot</code> function is generic and will dispatch to a method that knows how to plot whatever you're plotting. This is very nice for the user.</p>
<hr>
<p><img alt="one-dimensional R plot" src="plot_r_1.png"></p>
<hr>
<p>If you plot a single vector, you get an image like this with the index in the horizontal direction.</p>
<hr>
<p><img alt="two-dimensional R plot" src="plot_r_2.png"></p>
<hr>
<p>If you plot a data frame with two fields, you get a familiar scatterplot.</p>
<hr>
<p><img alt="three-dimensional R plot" src="plot_r_3.png"></p>
<hr>
<p>If you plot a data frame with more than two fields, you get a scatterplot matrix.</p>
<hr>
<p><img alt="linear model diagnostic R plots" src="plot_r_4.png"></p>
<hr>
<p>And if you plot a linear model object, you get four diagnostic plots.</p>
<p>All of these plots were created with <code>plot(data)</code>. There are a lot of defaults chosen in order to get something on the screen. You might also want to see another view, but the pure convenience of being able to <code>plot(data)</code> so easily is really wonderful.</p>
<p>The only way to reduce the friction of graphing even further would be to have plots automatically generated in the background based on whatever data you have on your system. I <a href="https://twitter.com/planarrowspace/status/555893329460994048">think</a> that would be a fun project, in fact.</p>
<p>Once you have an image, you want to be able to work with it quickly and easily. This means interaction with the image itself, and this is a place that <code>R</code> is not particularly strong, at least with base graphics.</p>
<hr>
<p><img alt="paper plot" src="paper_plot.png"></p>
<hr>
<p>A lot of statistical graphics behave essentially like paper. I like paper a lot, but computers can do more. We should expect, at a minimum, to be able to point to a data element and find out more about it. There are tons of other direct manipulation ways to interact with graphics (like brushing) that we should be able to have easily at our disposal.</p>
<p>Most of the world agrees that the way to get interactivity is to use web frontend things, and I'm inclined to think so too. It does often seem that the time to produce a visualization has a dramatic inverse relationship with the amount of interactivity supported, however.</p>
<p>In summary:</p>
<ul>
<li>Interactivity lets us quickly work with plots.</li>
<li>A simple user API lets us quickly make plots.</li>
</ul>
<p>What about <em>from whatever</em>?</p>
<hr>
<p><img alt="diagram of R graphics ecosystem" src="graphics_r.png"></p>
<hr>
<p><code>R</code> can make a lot of different graphics. Parts of the ecosystem are organized in clever ways that make things convenient for <code>R</code> users, and there's just a ton available. It seems fine to make our graphics in <code>R</code>. But what about other languages?</p>
<hr>
<p><img alt="diagram of R, Python, and Julia graphics ecosystem" src="graphics_multi.png"></p>
<hr>
<p>The current status quo is mostly to not share graphics capabilities between languages. So if Python want to have <code>ggplot</code>, <a href="https://yhathq.com/">somebody</a> has to <a href="http://ggplot.yhathq.com/">port</a> it over, which takes a bunch of work. And newer languages can be at a disadvantage purely because they don't have as much tooling for visualization, despite other strengths.</p>
<p>What I'm suggesting is that we might be able to pull the graphics parts out of our data languages.</p>
<hr>
<p><img alt="diagram of gog graphics ecosystem" src="graphics_gog.png"></p>
<hr>
<p>So this is the idea. You make the control surface inside various languages quite small and easy, so that everybody can connect into a sort of shared library of visualization pieces.</p>
<hr>
<p><img alt="cover of The Grammar of Graphics" src="gg_cover.jpg"></p>
<hr>
<p>As an aside:</p>
<p>The grammar of graphics seems to be a good idea, and the <a href="http://www.amazon.com/The-Grammar-Graphics-Statistics-Computing/dp/0387245448">book</a> is also quite good. A lot of what's good about Tableau is good because of the grammar of graphics.</p>
<p>However, the design that should be influenced by the grammar of graphics is not the part that <code>gog</code> is directly concerned with.</p>
<hr>
<pre><code class="language-r">ggplot(data) +
  aes(x=thing) +
  geom_histogram() +
  # etc.</code></pre>

<hr>
<p>To make a comparison to R's <code>ggplot2</code>, notice that you always start by passing data in. Then <code>ggplot2</code> let's you specify everything about your plot&#8212;and you <em>have</em> to specify a good deal before you get any plot at all.</p>
<hr>
<pre><code class="language-r">gog(data)
# etc. happens elsewhere</code></pre>

<hr>
<p>What <code>gog</code> suggests is to standardize the data passing, but then the way that the plot gets made is none of <code>gog</code>'s business. It's probably a good idea to start with some default plot and then let users work with it further.</p>
<p>So what is all this nonsense, and how could it work?</p>
<hr>
<p><img alt="gogd console" src="gogd.png"></p>
<hr>
<p>The connecting component is a <code>gog</code> HTTP server which runs on port 4808 and just accepts POST requests at <code>/data</code> and rebroadcasts it to anything listening to a websocket also at <code>/data</code>. The assumption is that we're passing a JSON array of objects.</p>
<p>I have <a href="https://github.com/ajschumacher/gogd">a gog server</a> written in Clojure, but a server can be implemented and hosted however you want, as long as everything connects.</p>
<hr>
<p><img alt="charted.co welcome screen" src="charted.png"></p>
<hr>
<p>People have already made some visualizations that are suitable for adapting to use with <code>gog</code>. As an example, I took the code from <a href="http://www.charted.co/">charted.co</a>, which was designed to load data from CSV files, and adapted it to also allow input from <code>gog</code>.</p>
<hr>
<pre><code class="language-python"># pip install gogpy
from gogpy import gog
import numpy as np
import pandas as pd
gog(pd.DataFrame(np.random.sample(10)))</code></pre>

<hr>
<p>The <code>gogpy</code> package is ridiculously small, but can still be <code>pip</code> installed from <a href="https://pypi.python.org/pypi">PyPI</a> for your convenience. Currently the <code>gog</code> function will only take a <code>pandas</code> <code>DataFrame</code>.</p>
<hr>
<p><img alt="charted.co graph" src="charted_demo.png"></p>
<hr>
<p>Thanks to the work that went into making <code>charted.co</code>, we have a graph already.</p>
<hr>
<pre><code class="language-python">import time
data = pd.DataFrame(np.random.sample(100))
for i in range(91):
    gog(data[i:i+10])
    time.sleep(2)</code></pre>

<hr>
<p>Live updates are the only thing happening&#8212;it just depends on when you send the data.</p>
<hr>
<pre><code class="language-r"># install_github("ajschumacher/gogr")
library("gogr")
data &lt;- data.frame(0-runif(100))
for (i in 1:91) {
    gog(data[i:(i+9), ,drop=F])
    Sys.sleep(2)
}</code></pre>

<hr>
<p>The <code>R</code> package <a href="https://github.com/ajschumacher/gogr">gogr</a> isn't on <a href="http://cran.r-project.org/">CRAN</a> but can be installed with <a href="https://github.com/hadley/devtools">devtools</a>. It's trivially easy to use any visualization that supports <code>gog</code> from any environment that supports <code>gog</code>.</p>
<hr>
<pre><code class="language-r">rstudio::viewer("http://localhost:8000")
library("gogr")
data &lt;- iris
names(data)[1:2] &lt;- c("x", "y")
gog(data)</code></pre>

<hr>
<p>It turns out that the <a href="http://www.rstudio.com/">RStudio</a> viewer pane is really just <a href="https://www.webkit.org/">WebKit</a> and it's easy to plug in <code>gog</code> visualizations.</p>
<hr>
<p><img alt="visualizing iris sepal length vs. sepal width" src="gogi_iris.png"></p>
<hr>
<p>I made this little visualization, a simple scatterplot with <a href="http://d3js.org/">D3</a>. It's very crude at the moment, but it's enough to show sepal length versus sepal width, and to show information about data points as we explore around. This could obviously be much improved with such luxuries as axes, and replace the renaming of columns to &#8217;x&#8217; and &#8217;y&#8217; with defaulting and interaction. And using &#8220;title&#8221; elements for tooltips has only the advantage of expediency.</p>
<p>But even in its current state this visualization allows for fun data manipulation experiments with animated updates which would be much more work to achieve with other techniques. (See <a href="demo.R">demo.R</a> for the complete <code>R</code> demo source.)</p>
<hr>
<pre><code class="language-python"># access meetup API and extract data, then:
data = pd.DataFrame({'x': rsvped, 'y': ids, 'name': names})
gog(data)</code></pre>

<hr>
<p>Of course nobody cares about irises - let's pull some data about humans we know, using the Meetup API. (See <a href="demo.py">demo.py</a> for complete Python demo source.)</p>
<hr>
<p><img alt="visualizing RSVP time vs. member ID" src="gogi_meetup.png"></p>
<hr>
<p>Even though we collected data with Python, we can visualize it wherever we want, including back inside the RStudio interface.</p>
<p>I haven't tried it, but it strikes me that it would be pretty straightforward to cobble together an RStudio-style Python IDE, at least with a REPL and <code>gog</code> visualization container, using existing components.</p>
<hr>
<p>Thank you!</p>
<hr>
<p>This is all very preliminary and there are no doubt a lot of complications and other things to consider. The question of how to manage possibly many <code>gog</code> visualization frontends or have any sort of dispatch system is not at all addressed, for instance. And there are a number of more or less related projects, which I've started to collect at <a href="https://github.com/ajschumacher/gog">ajschumacher/gog</a>. Any and all feedback, suggestions, or anything else appreciated!</p>    
    ]]></description>
<link>http://planspace.org/20150119-gog_a_separate_layer_for_visualization/</link>
<guid>http://planspace.org/20150119-gog_a_separate_layer_for_visualization/</guid>
<pubDate>Mon, 19 Jan 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>A Great Python Book explains Hash Tables</title>
<description><![CDATA[

<p><a href="http://mitpress.mit.edu/books/introduction-computation-and-programming-using-python-0">Introduction to Computation and Programming Using Python</a> is the book you should get if you think you want to learn Python. It packs in tons of sophisticated content and makes it accessible, not dumbed down.</p>
<p><a href="http://mitpress.mit.edu/books/introduction-computation-and-programming-using-python-0"><img alt="Introduction to Computation and Programming Using Python" src="cover.jpg"></a></p>
<p>This book seems to be something of a hidden gem. It isn't what comes up first when you search for Python resources. It doesn't have a lot of flash or promotion behind it. Of course it does have the weight of its publisher, The MIT Press. And there's the blurb on the back cover from Hal Abelson, coauthor of <a href="https://mitpress.mit.edu/sicp/">Structure and Interpretation of Computer Programs</a>. So there are clear signs of the book's quality.</p>
<p>One example of what's good about this book is the explanation of <em>hashing</em>. This is a very important topic, but it has a little bit of complexity to it and so it's often almost entirely omitted when teaching people to code. This is the depth sometimes given in introductory Python courses:</p>
<pre><code class="language-python"># A Python dictionary is a set of key-value pairs
my_dict = {'a_key': 'a_value', 'another_key': 'another_value'}
# which allows access to the values via the keys (lookup)
my_dict['another_key']  # 'another_value'
# This data structure is also known as a `map` or `hash-map`</code></pre>

<p>That's true, and you can start to use the language functionality with that, but it doesn't help you understand the underlying key idea. Below is part of the treatment from <em>Introduction to Computation and Programming Using Python</em>, in section 10.3 <em>Hash Tables</em>:</p>
<section style="font-family: sans-serif">

<p>... [Python] dictionaries use a technique called hashing to do the lookup in time that is nearly independent of the size of the dictionary. The basic idea behind a <strong>hash table</strong> is simple. We convert the key to an integer, and then use that integer to index into a list, which can be done in constant time. In principle, values of any immutable type can be easily converted to an integer. After all, we know that the internal representation of each object is a sequence of bits, and any sequence of bits can be viewed as representing an integer.  For example, the internal representation of <code>'abc'</code> is the string of bits 011000010110001001100011, which can be viewed as a representation of the decimal integer 6,382,179. Of course, if we want to use the internal represntation of strings as indices into a list, the list is going to have to be pretty darn long.</p>

<p>What about situation swhere the keys are already integers? Imagine, for the moment, that we are implementing a dictionary all of whose keys are U.S. Social Security numbers. (A United States Social Security number is a nine-digit integer.) If we represented the dictionary by a list with 10<sup>9</sup> elements and used Social Security numbers to index into the list, we could do lookups in constant time. Of course, if the dictionary contained entires for only ten thousand (10<sup>4</sup>) people, this would waste quite a lot of space.</p>

<p>Which gets us to the subject of hash functions. A <strong>hash function</strong> maps a large space of inputs (e.g., all natural numbers) to a smaller space of outputs (e.g., the natural numbers between 0 and 5000). Hash functions can be used to convert a large space of keys to a smaller space of integer indices.</p>

<p>Since the space of possible outputs is smaller than she space of possible inputs, a shash function is a <strong>many-to-one mapping</strong>, i.e., multiple different inputs may be mapped to the same output. When two inputs are mapped to the same output, it is called a <strong>collision</strong>&#8212;a topic which we will return to shortly. A good hash function produces a <strong>uniform distribution</strong>, i.e., every output in the range is equally probable, which minimizes the probability of collisions.</p>

<p>Designing good hash functions is surprisingly challenging. The problem is that one wants the outputs to be uniformly distributed given the expected distribution of inputs. Suppose, for example, that one hashed surnames by performing some calculation on the first three letters. In the Netherlands, where roughly 5% of surnames begin with &#8220;van&#8221; and another 5% with &#8220;de,&#8221; the distribution would be far from uniform.</p>

<p>Figure 10.6 uses a simple hash function (recall that <code>i%j</code> returns the remainder when the integer <code>i</code> is divided by the integer <code>j</code>) to implement a dictionary with integers as keys.</p>

<p>The basic idea is to represent an instance of class <code>intDict</code> by a list of <strong>hash buckets</strong>, where each bucket is a list of key/value pairs. By making each bucket a list, we handle collisions by storing all of the values that hash to the same bucket in the list.</p>

<p>The hash table works as follows: The instance variable <code>buckets</code> is initialized to a list of <code>numBuckets</code> empty lists. To store or look up an entry with key <code>dictKey</code>, we use the hash function <code>%</code> to convert <code>dictKey</code> into an integer, and use that integer to index into <code>buckets</code> to find the hash bucket associated with <code>dictKey</code>. We then search that bucket (which is a list) linearly to see if there is an entry with the key <code>dictKey</code>. If we are doing a lookup and there is an entry with the key, we simply return the value stored with that key. If there is no entry with that key, we return <code>None</code>. If a value is to be stored, then we either replace the value in the existing entry, if one was found, or append a new entry to the bucket if none was found.</p>

<p>There are many other ways to handle collisions, some considerably more efficient than using lists. But this is probably the simplest mechanism, and it works fine if the hash table is big enough and the hash function provides a good enough approximation to a uniform distribution.</p>

<p>Notice that the <code>__str__</code> method produces a representation of a dictionary that is unrelated to the order in which elements were added to it, but is instead ordered by the values to which the keys happen to hash. This explains why we can't predict the order of the keys in an object of type <code>dict</code>.</p>

<p><strong>Figure 10.6 Implementing dictionaries using hashing</strong></p>

<pre><code class="language-python">class intDict(object):
    """A dictionary with integer keys"""

    def __init__(self, numBuckets):
        """Create an empty dictionary"""
        self.buckets = []
        self.numBuckets = numBuckets
        for i in range(numBuckets):
            self.buckets.append([])

    def addEntry(self, dictKey, dictVal):
        """Assumes dictKey an int. Adds an entry."""
        hashBucket = self.buckets[dictKey%self.numBuckets]
        for i in range(len(hashBucket)):
            if hashBucket[i][0] == dictKey:
                hashBucket[i] = (dictKey, dictVal)
                return
        hashBucket.append((dictKey, dictVal))

    def getValue(self, dictKey):
        """Assumes dictKey an int. Returns entry associated
           with the key dictKey"""
        hashBucket = self.buckets[dictKey%self.numBuckets]
        for e in hashBucket:
            if e[0] == dictKey:
                return e[1]
        return None

    def __str__(self):
        result = '{'
        for b in self.buckets:
            for e in b:
                result = result + str(e[0]) + ':' + str(e[1]) + ','
        return result[:-1] + '}' #result[:-1] omits the last comma</code></pre>

</section>

<p>The text goes on to show a further example and explanation of how the <code>intDict</code> works.</p>
<p>The selection above does depend on knowing something about what &#8220;constant time&#8221; means, and possibly &#8220;immutable.&#8221; But if you were reading the book, you would have learned these already.</p>
<p>The organization and exposition is really extraordinary. Even in just the small hashing section above, there&#8217;s a sequence of three or four concrete examples to motivate the thinking, and then it leads into an understandable implementation. This is phenomenal educational writing.</p>
<p>This is <a href="http://mitpress.mit.edu/books/introduction-computation-and-programming-using-python-0">the book</a> for people who want to learn.</p>    
    ]]></description>
<link>http://planspace.org/20150111-a_great_python_book_explains_hash_tables/</link>
<guid>http://planspace.org/20150111-a_great_python_book_explains_hash_tables/</guid>
<pubDate>Sun, 11 Jan 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>Learn to Code with Emacs</title>
<description><![CDATA[

<p>To <a href="/20141231-two_problems_with_learn-to-code_sites/">learn to code</a> effectively, use:</p>
<ul>
<li>tools that are actually used for coding</li>
<li>materials that are reusable as references</li>
</ul>
<p>Here's one way to do it:</p>
<ol>
<li><a href="http://emacs.link/">Install Emacs</a>.</li>
<li>Do the built-in Emacs Tutorial, accessible from the start screen.</li>
<li>Learn about the Info system (C-h i, select Info).</li>
<li>Work through the Introduction to Programming Emacs Lisp.</li>
</ol>
<p>These steps will all make sense if you do them in order.</p>
<p>Here's what you get, if you follow the steps above:</p>
<ul>
<li>a powerful editor that you can use with any programming language</li>
<li>programming skills together with computer science concepts</li>
<li>the ability to use that programming language to enhance that editor</li>
</ul>
<p>This is a powerful combination, and one that blends seamlessly into further learning and productive work.</p>    
    ]]></description>
<link>http://planspace.org/20150101-learn_to_code_with_emacs/</link>
<guid>http://planspace.org/20150101-learn_to_code_with_emacs/</guid>
<pubDate>Thu, 01 Jan 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>Two Problems with Learn-to-Code Sites</title>
<description><![CDATA[

<p>Lots of people want to learn to code, and this is good. But lots of the web sites that want to teach people to code are bad.</p>
<p>The popular learn-to-code sites generally teach a popular language, which is not necessarily a bad thing.</p>
<p>But there are two fundamental problems with the approach of many popular sites:</p>
<ol>
<li>They teach all inside a browser. This removes setup difficulty by providing a custom toy environment but leaves the student with no independent working setup of their own.</li>
<li>They take a one pass adventure game guided tour approach. This enforces a progression and generally makes it difficult or impossible to scan through and refer back to the material.</li>
</ol>
<p>There is definitely value in making it easy for people to try something they otherwise wouldn't try. After trying, a different approach may be better for facilitating learning.</p>
<p>To learn to code effectively, use:</p>
<ol>
<li>tools that are actually used for coding</li>
<li>materials that are reusable as references</li>
</ol>
<p>This means that you should:</p>
<ol>
<li>use an editor and language implementation installed on your own machine</li>
<li>have, read, and work from one or more good documents, guides, or books</li>
</ol>
<p>Books are a really good idea. This should be obvious, but it seems, especially when it comes to technology, that people want to imagine you can "learn by doing" everything, starting from nothing and then just checking Stack Overflow occasionally. No. Read a book. It's a mistake in an in-person class to think that you can get by without reading a book. You should read a book. It's also a mistake to think you can get by with just online activities. Get a book.</p>
<p>The "toy environment" and "guided tour" design problems of most online learn-to-code systems are fundamental problems. It additionally seems to be, too frequently, that the materials presented in these systems are less well organized and produced than one would expect from a book. Further, the interactive exercises that are ostensibly a major advantage of online systems can totally break the experience if they have the tiniest blocking error in design or implementation. And finally projects, which are an excellent way to learn, too often feel inane and constricting rather than interesting and eye-opening when forced into the frameworks of automated systems.</p>    
    ]]></description>
<link>http://planspace.org/20141231-two_problems_with_learn-to-code_sites/</link>
<guid>http://planspace.org/20141231-two_problems_with_learn-to-code_sites/</guid>
<pubDate>Wed, 31 Dec 2014 12:00:00 -0500</pubDate>
</item>
<item>
<title>Use Info in Emacs</title>
<description><![CDATA[

<p>There is a documentation system accessible from <a href="http://www.gnu.org/software/emacs/">Emacs</a> called <a href="https://www.gnu.org/software/texinfo/">Info</a>. Here's advice from a parenthetical in the <a href="http://www.gnu.org/software/emacs/manual/html_mono/eintr.html#Note-for-Novices">Note for Novices</a> of <a href="www.gnu.org/software/emacs/manual/html_mono/eintr.html">An Introduction to Programming in Emacs Lisp</a>:</p>
<blockquote>
<p>"To learn about Info, type C-h i and then select Info."</p>
</blockquote>
<p>Despite using Emacs, off and on, for many years, and also more recently hearing <a href="http://sachachua.com/">Sacha Chua</a> recommend Info in her <a href="http://sachachua.com/blog/category/podcast/emacs-chat-podcast/">Emacs chats</a>, I had never really used the Info system. But something on <a href="http://www.johndcook.com/blog/">John Cook</a>'s blog pointed me to the <a href="www.gnu.org/software/emacs/manual/html_mono/eintr.html">intro to Emacs Lisp</a> and I started reading it in a web browser until I got to that part:</p>
<blockquote>
<p>"To learn about Info, type C-h i and then select Info."</p>
</blockquote>
<p>At that point I did. I went into Emacs and learned about Info, and then I continued working through the <a href="www.gnu.org/software/emacs/manual/html_mono/eintr.html">intro to Emacs Lisp</a> inside Emacs, which was a much better way to read it than inside a browser.</p>
<p>I wish I'd come to all this sooner.</p>
<p>If you'd like to try Emacs, you should find it <a href="http://planspace.org/20141207-make_it_easy_to_install_emacs/">easy to install</a> using <a href="http://emacs.link/">emacs.link</a>. Once you have Emacs open on your computer, do the built-in tutorial. Then:</p>
<blockquote>
<p>"To learn about Info, type C-h i and then select Info."</p>
</blockquote>
<p>And then check out the Introduction to Emacs Lisp, also in the Info system.</p>
<p>Have fun!</p>    
    ]]></description>
<link>http://planspace.org/20141230-use_info_in_emacs/</link>
<guid>http://planspace.org/20141230-use_info_in_emacs/</guid>
<pubDate>Tue, 30 Dec 2014 12:00:00 -0500</pubDate>
</item>
<item>
<title>Calendar Plots should be Easy</title>
<description><![CDATA[

<p>Popularized by Mike Bostock's <a href="http://bl.ocks.org/mbostock/4063318">example</a> and the GitHub <a href="https://github.com/blog/1360-introducing-contributions">contribution calendar</a>, calendar plots have some recognition now.</p>
<p><a href="http://bit.ly/NYCsubway"><img alt="" src="calendar.png"></a></p>
<p>Calendar plots can be very good for showing daily data over time when weekly, monthly, and yearly structure are relevant. While color is not ideal for conveying numeric data, patterns can be made quickly discernible and interaction can add table lookup functionality.</p>
<p>I've made plots like <a href="http://bit.ly/NYCsubway">this</a> myself, but it has required fiddling with HTML, CSS, and JavaScript in typical <a href="http://d3js.org/">D3</a> fashion. There are some tools for making it easier to build these plots, like the <a href="http://kamisama.github.io/cal-heatmap/v2/">Cal-Heatmap</a> JavaScript plugin and the Google Charts <a href="https://developers.google.com/chart/interactive/docs/gallery/calendar">Calendar Chart</a>, but even with these it's still an HTML/JavaScript affair.</p>
<p>I want to be able to do something like this in <a href="http://www.r-project.org/">R</a> and get an interactive calendar plot:</p>
<pre><code class="language-r"># this package does not (yet) exist
library(plotcal)
plot.cal(values, dates)</code></pre>

<p>I want to be able to do something like this in <a href="https://www.python.org/">Python</a> and get an interactive calendar plot:</p>
<pre><code class="language-python"># this package does not (yet) exist
from plot_cal import plot_cal
plot_cal(values, dates)</code></pre>

<p>Such functionality could also interact more closely with <code>data.frame</code>s/<code>DataFrame</code>s, tuples, maps/dicts, etc. That would be fine.</p>
<p>Somebody should make such functions, or extend some existing packages to include them. Or if such things already exist, somebody should just tell me about it. Either way, I will say thank you.</p>    
    ]]></description>
<link>http://planspace.org/20141229-calendar_plots_should_be_easy/</link>
<guid>http://planspace.org/20141229-calendar_plots_should_be_easy/</guid>
<pubDate>Mon, 29 Dec 2014 12:00:00 -0500</pubDate>
</item>
<item>
<title>How does R calculate histogram break points?</title>
<description><![CDATA[

<p>Break points make (or break) your histogram. <a href="http://www.r-project.org/">R</a>'s default algorithm for calculating histogram break points is a little interesting. Tracing it includes an unexpected dip into R's <a href="http://en.wikipedia.org/wiki/C_%28programming_language%29">C</a> implementation.</p>
<pre><code class="language-r"># set seed so "random" numbers are reproducible
set.seed(1)
# generate 100 random normal (mean 0, variance 1) numbers
x &lt;- rnorm(100)
# calculate histogram data and plot it as a side effect
h &lt;- hist(x, col="cornflowerblue")</code></pre>

<p><img alt="" src="histogram.png"></p>
<p>The <code>hist</code> function calculates and returns a histogram representation from data. That calculation includes, by default, choosing the break points for the histogram. In the example shown, there are ten bars (or bins, or cells) with eleven break points (every 0.5 from -2.5 to 2.5). With break points in hand, <code>hist</code> counts the values in each bin. The histogram representation is then shown on screen by <code>plot.histogram</code>.</p>
<p>(By default, bin counts include values less than or equal to the bin's right break point and strictly greater than the bin's left break point, except for the leftmost bin, which includes its left break point.)</p>
<p>The choice of break points can make a big difference in how the histogram looks. Badly chosen break points can obscure or misrepresent the character of the data. R's default behavior is not particularly good with the simple data set of the integers 1 to 5 (as pointed out by <a href="https://twitter.com/hadleywickham">Wickham</a>).</p>
<pre><code class="language-r">hist(1:5, col="cornflowerblue")</code></pre>

<p><img alt="" src="bad5.png"></p>
<p>A manual choice like the following would better show the evenly distributed numbers.</p>
<pre><code class="language-r">hist(1:5, breaks=0.5:5.5, col="cornflowerblue")</code></pre>

<p><img alt="" src="good5.png"></p>
<p>It might be even better, arguably, to use more bins to show that not all values are covered.</p>
<pre><code class="language-r">hist(1:5, breaks=seq(0.55, 5.55, 0.1), col="cornflowerblue")</code></pre>

<p><img alt="" src="better5.png"></p>
<p>In any event, break points matter. When exploring data it's probably best to experiment with multiple choices of break points. But in practice, the defaults provided by R get seen a lot.</p>
<p>So how does R choose break points?</p>
<p>By default, inside of <code>hist</code> a two-stage process will decide the break points used to calculate a histogram:</p>
<ol>
<li>
<p>The function <code>nclass.Sturges</code> receives the data and returns a recommended number of bars for the histogram. The documentation says that <a href="http://en.wikipedia.org/wiki/Histogram#Number_of_bins_and_width">Sturges' formula</a> is "implicitly basing bin sizes on the range of the data" but it's just based on the number of values, as <code>ceiling(log2(length(x)) + 1)</code>. This is really fairly dull.</p>
</li>
<li>
<p>Then the data and the recommended number of bars gets passed to <code>pretty</code> (usually <code>pretty.default</code>), which tries to "Compute a sequence of about n+1 equally spaced &#8216;round&#8217; values which cover the range of the values in x. The values are chosen so that they are 1, 2 or 5 times a power of 10." This ends up calling into some parts of R implemented in C, which I'll describe a little below.</p>
</li>
</ol>
<p>Note: In what follows I'll link to a <a href="https://github.com/wch/r-source/tree/34c4d5dd3493863f6665f907dbad9bf1d800c0d4">mirror</a> of the R <a href="https://svn.r-project.org/R/">sources</a> because <a href="https://github.com/">GitHub</a> has a nice, familiar interface. I'll point to the most recent version of files without specifying line numbers. You'll want to search within the files to what I'm talking about. To see exactly what I saw go to commit <a href="https://github.com/wch/r-source/tree/34c4d5dd3493863f6665f907dbad9bf1d800c0d4">34c4d5dd</a>.</p>
<p>The <a href="https://github.com/wch/r-source/blob/trunk/src/library/grDevices/R/calc.R">source</a> for <code>nclass.Sturges</code> is trivial R, but the <code>pretty</code> <a href="https://github.com/wch/r-source/blob/trunk/src/library/base/R/pretty.R">source</a> turns out to get into C. I hadn't looked into any of R's C implementation before; here's how it seems to fit together:</p>
<p>The source for <code>pretty.default</code> is straight R until:</p>
<pre><code class="language-r">z &lt;- .Internal(pretty( # ... cut</code></pre>

<p>This <code>.Internal</code> thing is a call to something written in C. The file <a href="https://github.com/wch/r-source/blob/trunk/src/main/names.c">names.c</a> can be useful for figuring out where things go next. We find this line:</p>
<pre><code class="language-c">{"pretty",    do_pretty,  0,  11, 7,  {PP_FUNCALL, PREC_FN,   0}},</code></pre>

<p>So it goes to a C function called <code>do_pretty</code>. That can be found in <a href="https://github.com/wch/r-source/blob/trunk/src/main/util.c">util.c</a>. This is a lot of very Lisp-looking C, and mostly for handling the arguments that get passed in. For example:</p>
<pre><code class="language-c">    int n = asInteger(CAR(args)); args = CDR(args);</code></pre>

<p>That's kind of neat, but the actual work is done somewhere else again. The body of <code>do_pretty</code> calls a function <code>R_pretty</code> like this:</p>
<pre><code class="language-c">    R_pretty(&amp;l, &amp;u, &amp;n, min_n, shrink, REAL(hi), eps, 1);</code></pre>

<p>The call is interesting because it doesn't even use a return value; <code>R_pretty</code> modifies its first three arguments in place. Gross.</p>
<p>The function <code>R_pretty</code> is in its own file, <a href="https://github.com/wch/r-source/blob/trunk/src/appl/pretty.c">pretty.c</a>, and finally the break points are made to be "nice even numbers" and there's a result.</p>
<p>I was surprised by where the code complexity of this process is.</p>    
    ]]></description>
<link>http://planspace.org/20141225-how_does_r_calculate_histogram_break_points/</link>
<guid>http://planspace.org/20141225-how_does_r_calculate_histogram_break_points/</guid>
<pubDate>Thu, 25 Dec 2014 12:00:00 -0500</pubDate>
</item>
<item>
<title>DC Voter Registration Data</title>
<description><![CDATA[

<p><em>This is an expanded account; see <a href="https://github.com/ajschumacher/dc_voter_reg">dc_voter_reg</a> on GitHub for the data and concise documentation.</em></p>
<p><img alt="" src="cd.jpg"></p>
<p>DC voter registration data is public, but not very easy to get.</p>
<p>On Thursday, December 18, 2014, I took a printed copy of a <a href="http://www.dcboee.org/pdf_files/Data_Request_Form.pdf">PDF form</a> that I found on the <a href="http://www.dcboee.org/">DC Board of Elections web site</a> to 441 4th Street NW, suite 250 north, Washington, DC, 20001. I had to show ID, have my bag x-rayed, and go through a metal detector to get into the building.</p>
<p>I brought my checkbook so I could pay $2 for a CD-ROM of voter registration data. The clerk informed me that the data is updated daily. She burned my CD while I waited. She told me there are no rules on how the data can be used. I take it to be public domain.</p>
<p>On the CD were two files:</p>
<ul>
<li>The data was in a Microsoft Access database file called <code>DC VH EXPORT.MDB</code> (154 MB). I bought a PC laptop running Windows 8, purchased and installed Microsoft Access, opened the file and exported the data as text. It comes out as 130 megabytes of uncompressed text. I was able to get the column headers out and stick them on top of the file so that it can be read as nice CSV. Zipped (using the Windows built-in) the result is the 20 MB <a href="https://github.com/ajschumacher/dc_voter_reg/blob/master/20141218-dc_voters.csv.zip">20141218-dc_voters.csv.zip</a>.</li>
<li>The provided documentation file was a plain text <code>Read Me.txt</code>. I renamed this to <a href="https://github.com/ajschumacher/dc_voter_reg/blob/master/20141218-dc_voters.txt">20141218-dc_voters.txt</a>. It seems slightly out of date with respect to the data in that the later columns corresponding to elections are not the same in the documentation and in the data. These column names are interpretable as MMYYYY-T, I believe, where MM is numeric month, YYYY is four-digit year, and T is a letter corresponding to election type. I think G is General and P is primary.</li>
</ul>
<p>You can also get voter registration data on a per-ward basis in Microsoft Access or Excel formats. They will email it to you, but you have to fax in the form to make this request. I wonder if they would accommodate daily requests for every ward's data. It seems like it would be much less annoying to just put the data online automatically; it isn't even very heavy. If we dream big, maybe the data could live in <a href="http://dat-data.com/">dat</a>? Any option that doesn't require making a physical visit to their office during their fairly narrow business hours would be an improvement. Any format that doesn't require purchasing proprietary software would be an improvement.</p>
<p>Another thing you can get is images (PDFs?) of signatures for nominating petitions and ballot measures. You pay just $2 per CD-ROM, but these are only available during ten-day "challenge periods." So keep your eyes peeled! I haven't seen what these things look like.</p>
<p>At least you can get nice maps like this (or for individual wards) for just $10 each:</p>
<p><img alt="" src="map.jpg"></p>    
    ]]></description>
<link>http://planspace.org/20141220-dc_voter_registration_data/</link>
<guid>http://planspace.org/20141220-dc_voter_registration_data/</guid>
<pubDate>Sat, 20 Dec 2014 12:00:00 -0500</pubDate>
</item>
<item>
<title>Please stop saying â€œBig Dataâ€</title>
<description><![CDATA[

<p><em>A short talk for <a href="http://www.air.org/">AIR</a>.</em></p>
<p>I thought about calling this &#8220;Size and Complexity in Real Data Systems,&#8221; but that would have been so boring.</p>
<hr>
<p><img alt="" src="alan_kay.jpg"></p>
<hr>
<p>This is <a href="http://en.wikipedia.org/wiki/Alan_Kay">Alan Kay</a>. He worked at Xerox PARC, he said &#8220;The best way to predict the future is to invent it,&#8221; and he said this, at a <a href="https://www.youtube.com/watch?v=gTAghAJcO1o">talk</a> earlier this year:</p>
<blockquote>
<p>Big data is a way that a lot of people are trying to make money today. And it's a favorite of marketing people, because it's in the wind. Everybody has heard the phrase &#8220;big data.&#8221; Not everybody knows what it means. And so it's the perfect context for doing things that people can say, &#8220;Well this is an application of big data and this is an application of big data.&#8221; But in fact, the interesting future's not about data at all&#8212;it's about meaning.</p>
</blockquote>
<p>The objection here, which is an objection of many, is an objection to torturing the phrase &#8220;Big Data&#8221; until it means everything and nothing.</p>
<p>You can also read an article that Deloitte published last Wednesday on the Wall Street Journal's web site called <a href="http://deloitte.wsj.com/cio/2014/12/10/should-we-stop-using-the-term-big-data/">Should We Stop Using the Term &#8216;Big Data&#8217;?</a>.</p>
<p>The answer is yes. Let's all stop saying &#8220;big data&#8221; when we just mean &#8220;data.&#8221;</p>
<hr>
<p><img alt="" src="excel.png"></p>
<hr>
<p>Some people think big data is data they can't put in Excel. The problem with this is that you shouldn't be putting <em>any</em> data in Excel. There are many reasons Excel is bad. If you care about getting the right answer, you shouldn't use Excel [1]. But it is also true that more and more, we've moved from the era of &#8220;shouldn't use Excel&#8221; to the era of &#8220;can't possibly use Excel.&#8221;</p>
<p>The reasons you can't use Excel these days can include volume, velocity, variety, and veracity, those <a href="http://www.ibmbigdatahub.com/infographic/four-vs-big-data">V words</a> that buzz along with &#8220;big data.&#8221; These Vs are trying to get people to realize that their ideas about how to work with data are inadequate. The ideas were inadequate before as well, but now they're really clearly inadequate. It is more comfortable to think that the problem is only with these new-seeming Vs.</p>
<p>[1]: <em>See, for example, <a href="http://www.businessweek.com/articles/2013-04-18/faq-reinhart-rogoff-and-the-excel-error-that-changed-history">Growth in a Time of Debt</a>.</em></p>
<hr>
<p><img alt="" src="data_flow.jpg"></p>
<hr>
<p>Working with data is always about building systems. This includes systems where the output is an analysis result. You need to build systems that are testable, reusable, extensible, and composable. Ideally, systems that are elegant. You can't do that with Excel, or with most commercial statistical offerings. To steal some other peoples' language, <a href="http://www.wsj.com/articles/SB10001424053111903480904576512250915629460">software is eating the world</a> and you need to <a href="http://www.rushkoff.com/program-or-be-programmed/">program or be programmed</a>.</p>
<p>What should you learn, if all you know is Excel or something similarly unfortunate like Stata or SAS? If you think you should learn <a href="http://www.r-project.org/">R</a> or <a href="http://clojure.org/">Clojure</a> or <a href="http://www.scala-lang.org/">Scala</a>, you might be right. Otherwise, learn <a href="https://www.python.org/">Python</a>.</p>
<hr>
<p><img alt="" src="big_data.png"></p>
<hr>
<p>This is a <a href="http://people.cs.umass.edu/~mcgregor/stocworkshop/langford.pdf">slide</a> from <a href="http://en.wikipedia.org/wiki/John_Langford_%28computer_scientist%29">John Langford</a>, who has worked on large scale machine learning problems at Yahoo! and Microsoft. This is what technical people tend to mean by &#8220;big data.&#8221;</p>
<p>It's not quite the same thing as being &#8220;O(n<sup>2</sup>) algorithm feasible,&#8221; but to put it differently, let's say that some small data you could theoretically work with in Excel.</p>
<p>Medium data might fit on one computer, but to work with it you'll probably need someone who knows what &#8220;O(n<sup>2</sup>)&#8221; means. She started programming years before you, and you probably want her working on your small data too.</p>
<p>Very few people have big data by the definition here, and that's okay [2].</p>
<p>Second recommendation: if you do have really big data, learn <a href="https://spark.apache.org/">Spark</a>.</p>
<p>[2]: <em>Everybody can get access to somebody else's big data, often public, but very few people are producing their own big data.</em></p>
<hr>
<p><img alt="" src="unreasonable_data.png"></p>
<hr>
<p>Is more data better? This question is distinct from the question of whether our data is big. Do we want more data at all?</p>
<p>In 2009, some folks at Google come out with <a href="http://static.googleusercontent.com/media/research.google.com/en/us/pubs/archive/35179.pdf">The Unreasonable Effectiveness of Data</a>. They seem to be saying that there is at least one case in which the answer is yes, we want more data.</p>
<hr>
<p><img alt="" src="scaling_corpora.png"></p>
<hr>
<p>A good illustration of their point is a <a href="http://dl.acm.org/citation.cfm?id=1073017">2001 Microsoft paper</a>. The problem here is called &#8220;confusion set disambiguation,&#8221; which sounds fancy but just means that the model has to decide whether to fill in a blank in a sentence with T-O to, T-O-O too, or T-W-O two. So the data is text, and the point is that the amount of data seems to be more important than the choice of algorithm. There are three things to say here.</p>
<p>First, is this big data? Their complete data set was a billion words. Around five gigabytes. This is not an example of big data; that fits on a memory stick.</p>
<p>Second, how are we measuring effectiveness? This is an important thing to take from this example. The Y axis is prediction accuracy on a test set. The model sees some training data, and then we ask it to predict for separate test data, and we see how often it's right. This is a predictive framing of the problem, which is powerful. It's the scientific method: the model is correct to the extent it makes correct predictions. But it doesn't make much sense to try to interpret these models in any way that would please a traditional linguist.</p>
<p>Finally, should we be surprised that having more data helps for this problem?</p>
<p>The problem is a natural language problem. Language is complicated. The problem has been simplified quite a lot, to just confusion set disambiguation, but there are a lot of words, and all these models are high variance, meaning roughly that they have a ton of coefficients [3]. So even if you have a lot of data, you may still have relatively little (or even none) for some cases.</p>
<p>So it makes sense that these models should do better when they get to study more text. And text is easy to get from the internet, so there's no reason not to throw more data at the problem.</p>
<p>Not all problems are like this.</p>
<p>[3]: <em><a href="http://xavier.amatriain.net/">Xavier Amatriain</a> (formerly of Netflix) has a good <a href="http://technocalifornia.blogspot.com/2012/07/more-data-or-better-models.html">explanation</a> of this which inspired this section.</em></p>
<hr>
<p><img alt="" src="big_complex.png"></p>
<hr>
<p>I've tried to separate bigness from other kinds of data complexity. Where in this space can we operate?</p>
<hr>
<p><img alt="" src="big_complex_possible.png"></p>
<hr>
<p>I don't know if this is a perfect sketch, but there's some truth to it. Simple problems are common and now relatively easy to scale with the big name technologies that people like to sell.</p>
<p>A criticism of this graph is that it's a little circular, in that I'm kind of implicitly saying that things that can be scaled are simple. Maybe so. But there are problems that don't scale even in theory, and there are also problems that have no current good solution at any scale, and the number one way to make a complex problem scale is to transform it into a simple problem.</p>
<p>The amazing thing that you encounter in practice though is how much work there is still to be done here in the lower left-hand corner of this graph, and the tendency of organizations to take their easy problems and make them more complicated.</p>
<hr>
<p>TSDL</p>
<hr>
<p>Here's an example that I didn't work on; I only witnessed it from a distance. TSDL stands for Teacher Student Data Linkage. So say you're a company like AIR and you have to decide, for a state, who's a good teacher based on how their students did. (That's hard, by the way.)</p>
<p>You need, of course, some Teacher Student Data Linkage data. Certainly the school districts in the state have this, right?</p>
<p>Well, no. In the case of at least one large urban school district, they did not have usable information of this type. What they did have was two systems that each had some overlapping information. This was arguably already too many systems.</p>
<p>So how do they satisfy the need that the state and the company have for the linkage data? They build a third system. This third system gets data from the two existing systems, and then presents an interface for everyone in the district to go in by hand and identify their students.</p>
<p>This is a lot of work to get something that they probably should have had to begin with. But at least now that they have it, all their systems can benefit, right?</p>
<p>The data that was collected, to the best of my knowledge, was never fed back into the rest of the systems.</p>
<p>This is an example of the importance of smart engineering. For whatever reason, be it funding, vision, technical knowledge... systems are not good. They are not delivering the data that should really be expected of them, so when there's finally a clear and present need for that data, a lot of work goes into getting it,too often yielding dividends for nobody else.</p>
<p>It's not easy to set up and instrument systems, and especially hard to anticipate future needs. And the fragmentation of the American education system does not help with data integration. But if we want to benefit from better uses of data, we need to invest in better systems, not band-aid systems.</p>
<hr>
<p>Arlington Big Data Roundtable</p>
<hr>
<p>Here's an example that I did work on. The Arlington Big Data Roundtable was put on by Arlington Public Schools, and the team I was on won, and it got some <a href="http://www.washingtonpost.com/local/education/arlington-schools-announce-key-findings-from-big-data-competition/2014/09/10/fff0ee3a-3903-11e4-8601-97ba88884ffd_story.html">press</a>, which was nice.</p>
<p>What did we do? They gave us a bunch of data, and we predicted which students would drop out in the following year. Based on the historical data we had, and the success metric that we chose, we were pretty good at it. You can predict dropouts. This should be surprising to no one. So what's to say about this?</p>
<p>Let's start with the size of the data. The data in the last example wasn't big, by the way, and that was a large urban school district. Arlington has something like 20,000 students. So the only &#8220;big&#8221; here has to be some other kind of complexity.</p>
<p>You may have heard that 80% of a data scientist's time is spent munging data, leaving only 20% for analysis. It's interesting that in a data-obsessed profession this entirely made-up statistic gets quoted so much. But it feels true!</p>
<p>How is this example like the last example? We were handed data and worked entirely separately from the real host data systems. So all the work we did to make the data usable, who benefits from that? Nobody. They can't just take that and plug it in to whatever system it came from.</p>
<p>A side issue is that many of the problems come from defining what you are talking about, and bureaucracies tend to ruin this process, often by failing to provide a complete definition, sometimes deliberately. &#8220;What does it mean to drop out?&#8221; turns out to be tricky.</p>
<p>So we can predict who's going to drop out. Is that going to help any students this year? If it does, it will be because of systems-building work in larger measure than modeling work.</p>
<hr>
<p>&#1587;&#1602;&#1587;&#1602;&#1577;</p>
<hr>
<p>Google Translate tells me that this is Arabic for &#8220;tweet.&#8221;</p>
<p>This is an example from a hackathon I participated in. It gives me the opportunity to gently critique two more things: hackathons themselves, and what I'll call &#8220;data science mad-libs.&#8221;</p>
<p>Hackathons first. They're great for getting people excited and talking about something. They're hard to get results out of. So if you're clear about your goal, hackathons may be for you.</p>
<p>The hackathons that are successful tend to have goals that are either totally wide open, or very very well focused. In the first case, any interesting thing could be success. In the second, a lot of work has already gone into scoping a problem and making it attainable in the time available.</p>
<p>The deadly middle ground, which may be familiar if you've worked with clients, is a dream-like aspiration, sometimes a &#8220;data science mad-lib.&#8221;</p>
<p>Take a big problem and some big source of data, and you get a data science mad-lib:</p>
<ul>
<li>&#8220;We're going to stop human trafficking with Facebook!&#8221;</li>
<li>&#8220;We're going to end world hunger with Twitter!&#8221;</li>
<li>&#8220;We're going to monitor flu around the world with Google!&#8221;</li>
</ul>
<p>That last one has been <a href="http://www.google.org/flutrends/">done</a>, though there are <a href="http://bits.blogs.nytimes.com/2014/03/28/google-flu-trends-the-limits-of-big-data/">questions</a> about its effectiveness. Somebody may we working on these others. And wouldn't it be neat if these worked? And there may <em>be</em> a way to make them work&#8212;but these aspirations require methods. A starting point and an ending point do not imply a good path between them.</p>
<p>In the case of this particular hackathon we had quite a few Arabic tweets and the idea was to monitor socioeconomic conditions in towns in Egypt. So we put the tweets in Hadoop. Great! This gives us the illusion of making progress toward a goal. But what is our goal? How will we know if we've achieved it? We weren't sure, and while the exercise was interesting, we didn't have any usable results after our one day of work.</p>
<p>Now the <a href="http://www.unglobalpulse.org/">UN Global Pulse</a> is working on doing things like this Twitter exercise, and they may have some success. I don't want to nay-say all crazy-sounding ideas. Crazy ideas are some of my favorites. But crazy ideas that work are like magic tricks: they're only mysterious if you don't know what's really going on.</p>
<hr>
<p>data</p>
<hr>
<p>I am hopeful about data. I think we need more people working on data with more effective tools and practices. Dream big but write code. You can start writing Python tonight. Another book I recommend is called <a href="https://pragprog.com/the-pragmatic-programmer">The Pragmatic Programmer</a>.</p>
<hr>
<p>Thank you!</p>
<hr>
<p>And of course, you're under no obligation to agree with me.</p>    
    ]]></description>
<link>http://planspace.org/20141215-please_stop_saying_big_data/</link>
<guid>http://planspace.org/20141215-please_stop_saying_big_data/</guid>
<pubDate>Mon, 15 Dec 2014 12:00:00 -0500</pubDate>
</item>
<item>
<title>Make it Easy to Install Emacs</title>
<description><![CDATA[

<p>&#8220;GNU Emacs is an extensible, customizable text editor&#8212;and more.&#8221;</p>
<p>I wanted to invite novice hackers to check out <a href="http://www.gnu.org/software/emacs/">Emacs</a>. But installing Emacs for the first time isn't easy! Or rather: the installation can be easy, but finding the right package for your system may not be.</p>
<p>So I made <a href="http://emacs.link/">emacs.link</a>, a static site hosted with GitHub pages (<a href="https://github.com/ajschumacher/emacs.link">repo</a>). It does a rough system detection and then presents simple, direct installation directions.</p>
<p>The directions are opinionated in that they only show one method per system:</p>
<ul>
<li>On Linux you might want to build Emacs from source, but I don't mention that alternative.</li>
<li>On a Mac, you might prefer to install Emacs using <a href="http://brew.sh/">brew</a>, but if you don't have brew installed already it's easier to skip that step.</li>
<li>There's no Emacs installer for <a href="http://ftpmirror.gnu.org/emacs/windows/README">Windows</a>. The current directions may be too difficult for some Windows users.</li>
<li>There are ways to try to get Emacs running on <a href="http://gamma-level.com/iphoneos/ports/emacs">iOS</a> and <a href="https://play.google.com/store/apps/details?id=com.zielm.emacs">Android</a>, but I'm not sure that's even a good idea, so on mobile I recommend watching some fun <a href="https://www.youtube.com/watch?v=TMoPuv-xXMM&amp;index=6&amp;list=UUkRmQ_G_NbdbCQMpALg6UPg">Emacs videos</a>.</li>
</ul>
<p>I tried to make <a href="http://emacs.link/">emacs.link</a> attractive, after the fashion of hip startup sites. The nice background image is courtesy of <a href="https://www.flickr.com/photos/anschieber/">Andrea Schieber</a>, who released <a href="https://www.flickr.com/photos/anschieber/6303932505">the original</a> on Flickr with a Creative Commons license (<a href="https://creativecommons.org/licenses/by-nc-sa/2.0/">CC BY-NC-SA 2.0</a>). Thanks very much to her! I cropped and edited the original a little bit with <a href="http://www.gimp.org/">GIMP</a>. As required, the edited version has the same license.</p>    
    ]]></description>
<link>http://planspace.org/20141207-make_it_easy_to_install_emacs/</link>
<guid>http://planspace.org/20141207-make_it_easy_to_install_emacs/</guid>
<pubDate>Sun, 07 Dec 2014 12:00:00 -0500</pubDate>
</item>
  </channel>
</rss>
